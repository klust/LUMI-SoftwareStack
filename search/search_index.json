{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Docs for the LUMI software stack \u00b6 What's new or different? Installation instructions for the pilot phase Setup of the LUMI module system EasyBuild setup Cray PE integration Some procedures The directpry structure in the installation directory An overview of files in several of the repository directories and where they are used The contents of the SitePackage.lua file Toolchain documentation EasyBuild toolchains common options Common information to all cpe* toolchains cpeCray toolchain cpeGNU toolchain cpeAOCC toolchain Current configurations on LUMI Documentation on our EasyBlocks: The CrayPEToolchain EasyBlock A number of failed experiments during the development, to avoid making the same mistake twice","title":"Home"},{"location":"#docs-for-the-lumi-software-stack","text":"What's new or different? Installation instructions for the pilot phase Setup of the LUMI module system EasyBuild setup Cray PE integration Some procedures The directpry structure in the installation directory An overview of files in several of the repository directories and where they are used The contents of the SitePackage.lua file Toolchain documentation EasyBuild toolchains common options Common information to all cpe* toolchains cpeCray toolchain cpeGNU toolchain cpeAOCC toolchain Current configurations on LUMI Documentation on our EasyBlocks: The CrayPEToolchain EasyBlock A number of failed experiments during the development, to avoid making the same mistake twice","title":"Docs for the LUMI software stack"},{"location":"CrayPEToolchain/","text":"The CrayPEToolchain EasyBlock and cpeCray/cpeGNU/cpeAMD modules \u00b6 Introduction \u00b6 Our CrayPEToolchain EasyBlock allows for many different scenarios to generate the cpeCray/cpeGNU/cpeAMD modules: The cpe module can be loaded first, last or not at all. Note that if the module is not loaded at all, it may be wise to have a different way of setting the default versions for the Cray PE modules. On LUMI, in the LUMI software stacks, these default versions are already set by the LUMIstack_<yy.mm>_modulerc.lua files. In version 21.04 of the CPE, the cpe modules still have several problems, partly due to an LMOD restriction and partly due to bugs in those modules: The cpe modules set LMOD_MODULERCFILE through setenv rather than prepend_path or append_path so they overwrite any file that sets system-wide defaults and visibility from other sources, which is not desirable. A change to LMOD_MODULERCFILE has only effect the next time a module command is executed. This is a restriction not only of LMOD version 8.3.x used in the 21.04 CPE, but also of versions in the 8.4 and 8.5 series. Hence loading the cpe/yy.mm module before loading other versionless modules for the CPE components will not have the desired effect of loading the versions for that specific version of the CPE. The cpe modules do contain code to reload already loaded modules from the CPE in the correct version, but that code is also broken in the 21.04 version as the modules may be loaded in an order in which a module that has already been reloaded in the correct version, gets reloaded once more with a versionless load, which may reload the wrong version. This is because the LUA loop with pairs doesn't have a fixed order of going over the entries in the LUA table. The order should be such that no module reloads an other module that has already been reloaded in the correct version. The matching PrgEnv-* module can either be loaded, or its loading can just be emulated by only setting the environment variable that this module sets, but relying on the cray_targets variable and dependencies list to load all Cray PE components. The reason to avoid loading the PrgEnv-* module is reproducibility. That module depends on a file in /etc to define the components that will be loaded, and that file cannot distinguish between versions of the CPE. Hence if changes to that file would be made, it has an effect on the working of all cpe* modules that EasyBuild may already have generated. It is possible to specify target modules via cray_targets . This is a list just as the dependencies. They will be loaded after the PrgEnv-* module (if the latter is loaded) but before other dependencies specified by dependencies . They do not need to be defined in the EasyBuild external modules file. We chose to load them after the PrgEnv-* module (if the latter is loaded) to be able to overwrite Cray targeting modules loaded by the latter. Dependencies in this case will be external modules. It is possible to specify versions by using, e.g., ( 'gcc/9.3.0' , EXTERNAL_MODULE ) Versions should be specified if the cpe module is not loaded, even on LUMI, as if a user would execute module load LUMI/21.04 cpeGNU/21.04 the wrong versions of CPE components might be loaded because of the same LMOD restriction that causes the problems with the Cray cpe/yy.mm modules: The LUMI/yy.mm module will add a file that sets the default versions of CPE compoments for the requested LUMI software stack and matching CPE version, but those changes only take effect at the following module command, so the cpeGNU/21.04 module which is loaded in the above example will not yet see the correct default versions of the modules. Note also that if versions are specified but the cpe module is loaded at the end, modules might be reloaded in a different version. The default value for various parameters is chosen to generate module files that are as similar as possible to those used ast CSCS (or at least those used for their 20.04 environment), but are not the defaults initially used on LUMI. Supported extra parameters for the EasyConfig files \u00b6 The CrayPEToolchain EasyBlock supports the following parameters: PrgEnv : Sets the PrgEnv-* module to load or emulate. The default is to derive the value from the name of the module to generate: PrgEnv = 'cray' for cpeCray PrgEnv = 'gnu' for cpeGNU PrgEnv = 'aocc' for cpeAMD PrgEnv = 'intel' for cpeIntel PrgEnv = 'nvidia' for cpeNVIDIA (not tested as we have no access to a machine with a fully working version of this environment) It is also possible to specify any of these values, or even a different value for a PrgEnv-* module that is not yet recognized by the EasyBlock. PrgEnv_load : Boolean value, indicate if the PrgEnv module should be loaded explicitly (if True) or not (if False). Default is True . If you want to hard-code a version, you can do so by specifying the module with the version in the dependencies. It is important that all cpe* modules available in the system at the same time are also generated with the same setting for PrgEnv_load as otherwise the conflict resolution between those modules would not work correctly. PrgEnv_family : If cpeToolchain , the module will declare itself a member of the cpeToolchain family. If all cpe* modules are generated that way, this will ensure that no two different cpe* modules will be loaded simulataneously, which wouldn't work correctly anyway with the Cray compiler wrappers. If PrgEnv_load is false, it will also force unload all PrgEnv-* modules to ensure that none is loaded. Otherwise it relies on the family-mechanism used in the LMOD PrgEnv-* modules to do the job. This is the most robust option when explicitly loading a PrgEnv-* module and using LMOD as LMOD will then ensure that no two cpe* modules will be loaded simultaneously and the family mechanism used in the Cray PrgEnv-* modules will do the same for those modules. If PrgEnv , the module will declare itself a member of the PrgEnv- family. This will generate an error if PrgEnv_load is True as one cannot load two modules of the same family but is the most robust ootion when using LMOD and emulating the PrgEnv-* module. The LMOD family feature will take care of unloading all other PrgEnv-* or cpe* modules as they would conflict with the current module. If None (default), which is the only setting that works when TCL-based modules are used and is therefore the default, the module will start with unload commands for all known PrgEnv-* and all cpe* modules except itself and the PrgEnv-* module that it uses (if it uses one). It is important that all cpe* modules available in the system at the same time are also generated with the same setting for PrgEnv_family as otherwise the conflict resolution between those modules would not work correctly. CPE_compiler specifies the (versionless) compiler to load. Possible values are: None (default): Derive the name of the compiler module from the name of the module to generate. This may not yet work for cpeNVIDIA as it is not clear what the name of the compiler module will be. If will not add an additional load if that compiler module is already specified in the dependencies. Note that this will load the module without specifying the version, so it only makes sense to rely on the autodetect feature if the cpe module is loaded (and if the bugs with that one are fixed). Any other value will be considered the name of the compiler module to load. The module should be versionless. If you want to specify a version, you can do so via dependencies . No separate load will be generated if the compiler module is also found in the list of dependencies. CPE_version : Version of the cpe module to use (if it is used). Possible values: None (default): Determine the version from the version of the module to generate, i.e., the version parameter in the EasyConfig. Any other value is interpreted as the value to load. CPE_load : Possible values: first (default): Load as the very first module. This does not make sense until the LMOD problems with LMOD_MODULERCFILE are fixed. after : Load immediately after loading PrgEnv-* but before loading any other module. This does not make too much sense until the LMOD problems with LMOD_MODULERCFILE are fixed, but it could be a way to first load modules the Cray way and then correct by manually loading correct versions via the cray_targets and dependencies parameters. This value will produce an error message when PrgEnv_load is set to False . last : Load as the last module. Currently this does not make sense until the problems with the cpe module are fully fixed, and on LUMI, until the problem with overwriting LMOD_MODIULERCFILE is fixed. None : Do not load the cpe module but rely on explicit dependencies specified in the list of dependencies instead. cray_targets : A list of Cray targetting modules to load. dependencies : This is a standard EasyConfig parameter. The versions of the selected PrgEnv, compiler and craype module can be specified through dependencies but those modules will still be loaded according to the scheme below. Any redifinition of the cpe module is discarded. Order of loads generated by the EasyBlock \u00b6 The cpe/<CPE_version> module, if CPE_load is first . If LMOD would be modified to honour changes to LMOD_MODULERCFILE immediately as it does with changes to MODULEPATH , this would be the best moment to load the cpe module as it ensures that all other packages would be loaded with the correct version number immediately. The PrgEnv-<PrgEnv> module, if PrgEnv_load is True. The targeting modules specified by cray_targets . Hence they can overwrite the targets set by the PrgEnv-* module which may be usefull on a heterogeneous system should there only be a single configuration for the PrgEnv-* modules for all hardware partitions in the system, or to build a cpe* module for cross-compiling. Note that changes to the targeting modules may trigger reloads of other modules loaded by the PrgEnv-* module. The CPE_compiler module (or autodected one), unless both PrgEnv-* is loaded explicitly and the module is not in the list of dependencies (in which case we rely on the PrgEnv-* module to do the proper job). The craype module (compiler wrappers), unless both PrgEnv-* is loaded explicitly and the module is not in the list of dependencies (in which case we rely on the PrgEnv-* module to do the proper job). The specified dependencies, minus the cpe/* , PrgEnv-* and craype/* modules. The cpe/<CPE_version> module, if CPE_load is last . In principle this should reload any module loaded before in a version that does not match the selected Cray PE version, and hence will also overwrite versions set in the dependencies. However, in the Cray PE 21.04 release (which was used for testing) the module did not always do the reloads in the proper order to always ensure the right version, and one might even end up with a version that is neither the one specified in the dependencies nor the one specified by the cpe/* module. Some examples \u00b6 Non-working: Load cpe and PrgEnv-gnu \u00b6 This is the default configuration for this EasyBlock. A minimal EasyConfig (omitting some mandatory parts such the homepage and description parameters) is easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = \"21.04\" toolchain = SYSTEM moduleclass = 'toolchain' This generates a module file that activates the toolchain by only loading the cpe/21.04 and PrgEnv-gnu -modules (in that order). Unfortunately, this scheme does not work with LMOD 8.3.x as is part of the Cray PE stack when the 21.04-21.06 releases were made, nor with version from the 8.4 and 8.5 branches, as LMOD_MODULERCFILE is only honoured at the next module call. If the effect of LMOD_MODULERCFILE would be immediate, this would probably be the most efficient way of activating a particular release of a particular PrgEnv. The module does not belong to any family. Instead it explicitly unloads other cpe* modules. Non-working: Load PrgEnv-gnu and then cpe \u00b6 Now we first load a PrgEnv-* module and only subsequently the cpe/yy.mm module that fixes versions for the modules. easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = \"21.04\" toolchain = SYSTEM PrgEnv_family = 'cpeToolchain' CPE_load = 'after' moduleclass = 'toolchain' This generates a module file that activates the toolchain by first loading the PrgEnv-gnu module and then correcting the versions by loading cpe/21.04 . This doesn't work reliably either due to the current design of the module reloading process in the cpe/21.04 module combined with the delayed impact of changes to LMOD_MODULERCFILE . The module will belong to the cpeToolchain family. That family will take care of unloading any other cpe* module that would be loaded (provided the PrgEnv_family parameter was set the same way in their EasyConfigs), while the PrgEnv-gnu module will take care of unloading other PrgEnv-* modules through the PrgEnv family. A setup without PrgEnv- or cpe module \u00b6 On LUMI, due to the problems with LMOD and the cpe modules, we currently use a setup without PrgEnv-* or cpe module. One of the functions of the cpe module, setting the default versions of the Cray PE components, is already done by the LUMI module that loads the software stack. The other is replaced by hard-coding the necessary versions in the EasyConfig. One of the functions of the PrgEnv-* modules, setting and environment variable that tells the compiler wrappers which PE is selected, is taken over by the EasyBlock which sets the variable in the module file that it generates. The other, loading the correct targets and other PE modules, is taken over by the craype_targets parameter and the dependency list. This is the most reproducible setup as it only depends on versioned components (the partition module already ensures that a particular version of the Cray targeting modules is made available). easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = \"21.04\" toolchain = SYSTEM PrgEnv_load = False PrgEnv_family = 'PrgEnv' CPE_load = None cray_targets = [ 'craype-x86-rome' , 'craype-accel-host' , 'craype-network-ofi' ] dependencies = [ ( 'gcc/9.3.0' , EXTERNAL_MODULE ), ( 'craype/2.7.6' , EXTERNAL_MODULE ), ( 'cray-mpich/8.1.4' , EXTERNAL_MODULE ), ( 'cray-libsci/21.04.1.1' , EXTERNAL_MODULE ), ( 'cray-dsmml/0.1.4' , EXTERNAL_MODULE ), ( 'perftools-base/21.02.0' , EXTERNAL_MODULE ), ( 'xpmem' , EXTERNAL_MODULE ), ] moduleclass = 'toolchain' The cpeGNU module generated by this EasyConfig will be unloaded if the user would load a PrgEnv-* module as it is also a member of the PrgEnv family. As such it is a full replacement of the Cray PrgEnv-gnu module. Loading the cpe and PrgEnv modules first, then reloading packages just to be sure \u00b6 A compromise solution that will work around the problems with LMOD and the cpe modules yet retain much of the spirit of the Cray PE, and that also can correct the targeting modules should the PrgEnv-* module not take the ones that you want (or ensure that at least certain other modules are loaded, even if they would be removed from the list of modules loaded by PrgEnv-gnu in an update of the system), is the following setup: easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = '21.04' toolchain = SYSTEM CPE_load = 'first' PrgEnv_load = True PrgEnv_family = 'cpeToolchain' cray_targets = [ 'craype-x86-rome' , 'craype-accel-host' , 'craype-network-ofi' ] dependencies = [ ( 'PrgEnv-gnu/8.0.0' , EXTERNAL_MODULE ), ( 'gcc/9.3.0' , EXTERNAL_MODULE ), ( 'craype/2.7.6' , EXTERNAL_MODULE ), ( 'cray-mpich/8.1.4' , EXTERNAL_MODULE ), ( 'cray-libsci/21.04.1.1' , EXTERNAL_MODULE ), ( 'cray-dsmml/0.1.4' , EXTERNAL_MODULE ), ( 'perftools-base/21.02.0' , EXTERNAL_MODULE ), ( 'xpmem' , EXTERNAL_MODULE ), ] moduleclass = 'toolchain' This setup will first load the cpe/21.04 and PrgEnv-gnu/8.0.0 modules to stay in the Cray PE spirit. Next the indicated targeting modules will be loaded, one for the CPU, one for the accelerator architecture and one for the network. This may trigger reloads of some other modules and will overwrite targeting modules of the same type loaded by PrgEnv-gnu . Finally, the gcc compiler module, the craype module and all other modules from the dependency list are loaded with the versions specified. This setup is a compromise that on one hand stays close to the Cray PE spirit by using the cpe and PrgEnv-gnu modules, yet works around some problems, namely: * Setting LMOD_MODULERCFILE does not work immediately. * Any corrective action when loading cpe after PrgEnv-gnu does not work * On a heterogeneous cluster, the targeting modules loaded by PrgEnv-gnu may not be the ones you want when cross-compiling or when the system would use the same file defining the modules for the whole system. * The list of modules loaded by PrgEnv-gnu may change as it is determined by a single file on the system that does not depend on the version of the Cray PE. In this case, you can always be sure that at least the modules mentioned in the dependency list and cray_targets parameter will be loaded. A variant of this would set CPE_load = 'after' which would load the cpe/21.04 module immediately after loading PrgEnv-gnu rather than just before, but with the current flaws of the cpe/21.04 module this still does not solve all problems: easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = '21.04' toolchain = SYSTEM CPE_load = 'after' PrgEnv_load = True PrgEnv_family = 'cpeToolchain' cray_targets = [ 'craype-x86-rome' , 'craype-accel-host' , 'craype-network-ofi' ] dependencies = [ ( 'PrgEnv-gnu/8.0.0' , EXTERNAL_MODULE ), ( 'gcc/9.3.0' , EXTERNAL_MODULE ), ( 'craype/2.7.6' , EXTERNAL_MODULE ), ( 'cray-mpich/8.1.4' , EXTERNAL_MODULE ), ( 'cray-libsci/21.04.1.1' , EXTERNAL_MODULE ), ( 'cray-dsmml/0.1.4' , EXTERNAL_MODULE ), ( 'perftools-base/21.02.0' , EXTERNAL_MODULE ), ( 'xpmem' , EXTERNAL_MODULE ), ] moduleclass = 'toolchain' Mimic PrgEnv, load hardcoded versions but load cpe/yy.mm first \u00b6 This is yet another compromise scenario: * Loading cpe/yy.mm first ensures that further modules a user might load after loading the cpe* module will load in the proper versions if a user does a versionless load. * Mimicing PrgEnv-* and loading modules explicitly ensures reproducibility over time as the list of modules loaded does not depend on a single file elsewhere in the system configuration which is not specific to a particular release of the PE. * Hard-coding the versions ensures that we avoid the problems caused by the implementation of the cpe/yy.mm modules (certainly in releases up to and including 21.06) easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = '21.04' toolchain = SYSTEM PrgEnv_load = False PrgEnv_family = 'PrgEnv' CPE_load = 'first' cray_targets = [ 'craype-x86-rome' , 'craype-accel-host' , 'craype-network-ofi' ] dependencies = [ ( 'PrgEnv-gnu/8.0.0' , EXTERNAL_MODULE ), ( 'gcc/9.3.0' , EXTERNAL_MODULE ), ( 'craype/2.7.6' , EXTERNAL_MODULE ), ( 'cray-mpich/8.1.4' , EXTERNAL_MODULE ), ( 'cray-libsci/21.04.1.1' , EXTERNAL_MODULE ), ( 'cray-dsmml/0.1.4' , EXTERNAL_MODULE ), ( 'perftools-base/21.02.0' , EXTERNAL_MODULE ), ( 'xpmem' , EXTERNAL_MODULE ), ] moduleclass = 'toolchain' Mimic PrgEnv and load cpe/yy.mm at the end \u00b6 This would be a valid scenario once the cpe/yy.mm modules have been corrected and work as they should. In this scenario, We mimic PrgEnv-* by setting the necessary environment variables and then loading a list of versionless modules. This avoids a problem with the actual PrgEnv modules as the list of modules they load depends on a single system file which is the same for all releases of the PE and hence may change over time. At the end the relevant cpe/yy.mm module is loaded to fix the versions of all already loaded modules. The corresponding EasyConfig file (minus help etc.) is: easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = '21.04' toolchain = SYSTEM PrgEnv_load = False PrgEnv_family = 'PrgEnv' CPE_load = 'last' cray_targets = [ 'craype-x86-rome' , 'craype-accel-host' , 'craype-network-ofi' ] dependencies = [ ( 'gcc' , EXTERNAL_MODULE ), ( 'craype' , EXTERNAL_MODULE ), ( 'cray-mpich' , EXTERNAL_MODULE ), ( 'cray-libsci' , EXTERNAL_MODULE ), ( 'cray-dsmml' , EXTERNAL_MODULE ), ( 'perftools-base' , EXTERNAL_MODULE ), ( 'xpmem' , EXTERNAL_MODULE ), ] moduleclass = 'toolchain'","title":"The CrayPEToolchain EasyBlock"},{"location":"CrayPEToolchain/#the-craypetoolchain-easyblock-and-cpecraycpegnucpeamd-modules","text":"","title":"The CrayPEToolchain EasyBlock and cpeCray/cpeGNU/cpeAMD modules"},{"location":"CrayPEToolchain/#introduction","text":"Our CrayPEToolchain EasyBlock allows for many different scenarios to generate the cpeCray/cpeGNU/cpeAMD modules: The cpe module can be loaded first, last or not at all. Note that if the module is not loaded at all, it may be wise to have a different way of setting the default versions for the Cray PE modules. On LUMI, in the LUMI software stacks, these default versions are already set by the LUMIstack_<yy.mm>_modulerc.lua files. In version 21.04 of the CPE, the cpe modules still have several problems, partly due to an LMOD restriction and partly due to bugs in those modules: The cpe modules set LMOD_MODULERCFILE through setenv rather than prepend_path or append_path so they overwrite any file that sets system-wide defaults and visibility from other sources, which is not desirable. A change to LMOD_MODULERCFILE has only effect the next time a module command is executed. This is a restriction not only of LMOD version 8.3.x used in the 21.04 CPE, but also of versions in the 8.4 and 8.5 series. Hence loading the cpe/yy.mm module before loading other versionless modules for the CPE components will not have the desired effect of loading the versions for that specific version of the CPE. The cpe modules do contain code to reload already loaded modules from the CPE in the correct version, but that code is also broken in the 21.04 version as the modules may be loaded in an order in which a module that has already been reloaded in the correct version, gets reloaded once more with a versionless load, which may reload the wrong version. This is because the LUA loop with pairs doesn't have a fixed order of going over the entries in the LUA table. The order should be such that no module reloads an other module that has already been reloaded in the correct version. The matching PrgEnv-* module can either be loaded, or its loading can just be emulated by only setting the environment variable that this module sets, but relying on the cray_targets variable and dependencies list to load all Cray PE components. The reason to avoid loading the PrgEnv-* module is reproducibility. That module depends on a file in /etc to define the components that will be loaded, and that file cannot distinguish between versions of the CPE. Hence if changes to that file would be made, it has an effect on the working of all cpe* modules that EasyBuild may already have generated. It is possible to specify target modules via cray_targets . This is a list just as the dependencies. They will be loaded after the PrgEnv-* module (if the latter is loaded) but before other dependencies specified by dependencies . They do not need to be defined in the EasyBuild external modules file. We chose to load them after the PrgEnv-* module (if the latter is loaded) to be able to overwrite Cray targeting modules loaded by the latter. Dependencies in this case will be external modules. It is possible to specify versions by using, e.g., ( 'gcc/9.3.0' , EXTERNAL_MODULE ) Versions should be specified if the cpe module is not loaded, even on LUMI, as if a user would execute module load LUMI/21.04 cpeGNU/21.04 the wrong versions of CPE components might be loaded because of the same LMOD restriction that causes the problems with the Cray cpe/yy.mm modules: The LUMI/yy.mm module will add a file that sets the default versions of CPE compoments for the requested LUMI software stack and matching CPE version, but those changes only take effect at the following module command, so the cpeGNU/21.04 module which is loaded in the above example will not yet see the correct default versions of the modules. Note also that if versions are specified but the cpe module is loaded at the end, modules might be reloaded in a different version. The default value for various parameters is chosen to generate module files that are as similar as possible to those used ast CSCS (or at least those used for their 20.04 environment), but are not the defaults initially used on LUMI.","title":"Introduction"},{"location":"CrayPEToolchain/#supported-extra-parameters-for-the-easyconfig-files","text":"The CrayPEToolchain EasyBlock supports the following parameters: PrgEnv : Sets the PrgEnv-* module to load or emulate. The default is to derive the value from the name of the module to generate: PrgEnv = 'cray' for cpeCray PrgEnv = 'gnu' for cpeGNU PrgEnv = 'aocc' for cpeAMD PrgEnv = 'intel' for cpeIntel PrgEnv = 'nvidia' for cpeNVIDIA (not tested as we have no access to a machine with a fully working version of this environment) It is also possible to specify any of these values, or even a different value for a PrgEnv-* module that is not yet recognized by the EasyBlock. PrgEnv_load : Boolean value, indicate if the PrgEnv module should be loaded explicitly (if True) or not (if False). Default is True . If you want to hard-code a version, you can do so by specifying the module with the version in the dependencies. It is important that all cpe* modules available in the system at the same time are also generated with the same setting for PrgEnv_load as otherwise the conflict resolution between those modules would not work correctly. PrgEnv_family : If cpeToolchain , the module will declare itself a member of the cpeToolchain family. If all cpe* modules are generated that way, this will ensure that no two different cpe* modules will be loaded simulataneously, which wouldn't work correctly anyway with the Cray compiler wrappers. If PrgEnv_load is false, it will also force unload all PrgEnv-* modules to ensure that none is loaded. Otherwise it relies on the family-mechanism used in the LMOD PrgEnv-* modules to do the job. This is the most robust option when explicitly loading a PrgEnv-* module and using LMOD as LMOD will then ensure that no two cpe* modules will be loaded simultaneously and the family mechanism used in the Cray PrgEnv-* modules will do the same for those modules. If PrgEnv , the module will declare itself a member of the PrgEnv- family. This will generate an error if PrgEnv_load is True as one cannot load two modules of the same family but is the most robust ootion when using LMOD and emulating the PrgEnv-* module. The LMOD family feature will take care of unloading all other PrgEnv-* or cpe* modules as they would conflict with the current module. If None (default), which is the only setting that works when TCL-based modules are used and is therefore the default, the module will start with unload commands for all known PrgEnv-* and all cpe* modules except itself and the PrgEnv-* module that it uses (if it uses one). It is important that all cpe* modules available in the system at the same time are also generated with the same setting for PrgEnv_family as otherwise the conflict resolution between those modules would not work correctly. CPE_compiler specifies the (versionless) compiler to load. Possible values are: None (default): Derive the name of the compiler module from the name of the module to generate. This may not yet work for cpeNVIDIA as it is not clear what the name of the compiler module will be. If will not add an additional load if that compiler module is already specified in the dependencies. Note that this will load the module without specifying the version, so it only makes sense to rely on the autodetect feature if the cpe module is loaded (and if the bugs with that one are fixed). Any other value will be considered the name of the compiler module to load. The module should be versionless. If you want to specify a version, you can do so via dependencies . No separate load will be generated if the compiler module is also found in the list of dependencies. CPE_version : Version of the cpe module to use (if it is used). Possible values: None (default): Determine the version from the version of the module to generate, i.e., the version parameter in the EasyConfig. Any other value is interpreted as the value to load. CPE_load : Possible values: first (default): Load as the very first module. This does not make sense until the LMOD problems with LMOD_MODULERCFILE are fixed. after : Load immediately after loading PrgEnv-* but before loading any other module. This does not make too much sense until the LMOD problems with LMOD_MODULERCFILE are fixed, but it could be a way to first load modules the Cray way and then correct by manually loading correct versions via the cray_targets and dependencies parameters. This value will produce an error message when PrgEnv_load is set to False . last : Load as the last module. Currently this does not make sense until the problems with the cpe module are fully fixed, and on LUMI, until the problem with overwriting LMOD_MODIULERCFILE is fixed. None : Do not load the cpe module but rely on explicit dependencies specified in the list of dependencies instead. cray_targets : A list of Cray targetting modules to load. dependencies : This is a standard EasyConfig parameter. The versions of the selected PrgEnv, compiler and craype module can be specified through dependencies but those modules will still be loaded according to the scheme below. Any redifinition of the cpe module is discarded.","title":"Supported extra parameters for the EasyConfig files"},{"location":"CrayPEToolchain/#order-of-loads-generated-by-the-easyblock","text":"The cpe/<CPE_version> module, if CPE_load is first . If LMOD would be modified to honour changes to LMOD_MODULERCFILE immediately as it does with changes to MODULEPATH , this would be the best moment to load the cpe module as it ensures that all other packages would be loaded with the correct version number immediately. The PrgEnv-<PrgEnv> module, if PrgEnv_load is True. The targeting modules specified by cray_targets . Hence they can overwrite the targets set by the PrgEnv-* module which may be usefull on a heterogeneous system should there only be a single configuration for the PrgEnv-* modules for all hardware partitions in the system, or to build a cpe* module for cross-compiling. Note that changes to the targeting modules may trigger reloads of other modules loaded by the PrgEnv-* module. The CPE_compiler module (or autodected one), unless both PrgEnv-* is loaded explicitly and the module is not in the list of dependencies (in which case we rely on the PrgEnv-* module to do the proper job). The craype module (compiler wrappers), unless both PrgEnv-* is loaded explicitly and the module is not in the list of dependencies (in which case we rely on the PrgEnv-* module to do the proper job). The specified dependencies, minus the cpe/* , PrgEnv-* and craype/* modules. The cpe/<CPE_version> module, if CPE_load is last . In principle this should reload any module loaded before in a version that does not match the selected Cray PE version, and hence will also overwrite versions set in the dependencies. However, in the Cray PE 21.04 release (which was used for testing) the module did not always do the reloads in the proper order to always ensure the right version, and one might even end up with a version that is neither the one specified in the dependencies nor the one specified by the cpe/* module.","title":"Order of loads generated by the EasyBlock"},{"location":"CrayPEToolchain/#some-examples","text":"","title":"Some examples"},{"location":"CrayPEToolchain/#non-working-load-cpe-and-prgenv-gnu","text":"This is the default configuration for this EasyBlock. A minimal EasyConfig (omitting some mandatory parts such the homepage and description parameters) is easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = \"21.04\" toolchain = SYSTEM moduleclass = 'toolchain' This generates a module file that activates the toolchain by only loading the cpe/21.04 and PrgEnv-gnu -modules (in that order). Unfortunately, this scheme does not work with LMOD 8.3.x as is part of the Cray PE stack when the 21.04-21.06 releases were made, nor with version from the 8.4 and 8.5 branches, as LMOD_MODULERCFILE is only honoured at the next module call. If the effect of LMOD_MODULERCFILE would be immediate, this would probably be the most efficient way of activating a particular release of a particular PrgEnv. The module does not belong to any family. Instead it explicitly unloads other cpe* modules.","title":"Non-working: Load cpe and PrgEnv-gnu"},{"location":"CrayPEToolchain/#non-working-load-prgenv-gnu-and-then-cpe","text":"Now we first load a PrgEnv-* module and only subsequently the cpe/yy.mm module that fixes versions for the modules. easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = \"21.04\" toolchain = SYSTEM PrgEnv_family = 'cpeToolchain' CPE_load = 'after' moduleclass = 'toolchain' This generates a module file that activates the toolchain by first loading the PrgEnv-gnu module and then correcting the versions by loading cpe/21.04 . This doesn't work reliably either due to the current design of the module reloading process in the cpe/21.04 module combined with the delayed impact of changes to LMOD_MODULERCFILE . The module will belong to the cpeToolchain family. That family will take care of unloading any other cpe* module that would be loaded (provided the PrgEnv_family parameter was set the same way in their EasyConfigs), while the PrgEnv-gnu module will take care of unloading other PrgEnv-* modules through the PrgEnv family.","title":"Non-working: Load PrgEnv-gnu and then cpe"},{"location":"CrayPEToolchain/#a-setup-without-prgenv-or-cpe-module","text":"On LUMI, due to the problems with LMOD and the cpe modules, we currently use a setup without PrgEnv-* or cpe module. One of the functions of the cpe module, setting the default versions of the Cray PE components, is already done by the LUMI module that loads the software stack. The other is replaced by hard-coding the necessary versions in the EasyConfig. One of the functions of the PrgEnv-* modules, setting and environment variable that tells the compiler wrappers which PE is selected, is taken over by the EasyBlock which sets the variable in the module file that it generates. The other, loading the correct targets and other PE modules, is taken over by the craype_targets parameter and the dependency list. This is the most reproducible setup as it only depends on versioned components (the partition module already ensures that a particular version of the Cray targeting modules is made available). easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = \"21.04\" toolchain = SYSTEM PrgEnv_load = False PrgEnv_family = 'PrgEnv' CPE_load = None cray_targets = [ 'craype-x86-rome' , 'craype-accel-host' , 'craype-network-ofi' ] dependencies = [ ( 'gcc/9.3.0' , EXTERNAL_MODULE ), ( 'craype/2.7.6' , EXTERNAL_MODULE ), ( 'cray-mpich/8.1.4' , EXTERNAL_MODULE ), ( 'cray-libsci/21.04.1.1' , EXTERNAL_MODULE ), ( 'cray-dsmml/0.1.4' , EXTERNAL_MODULE ), ( 'perftools-base/21.02.0' , EXTERNAL_MODULE ), ( 'xpmem' , EXTERNAL_MODULE ), ] moduleclass = 'toolchain' The cpeGNU module generated by this EasyConfig will be unloaded if the user would load a PrgEnv-* module as it is also a member of the PrgEnv family. As such it is a full replacement of the Cray PrgEnv-gnu module.","title":"A setup without PrgEnv- or cpe module"},{"location":"CrayPEToolchain/#loading-the-cpe-and-prgenv-modules-first-then-reloading-packages-just-to-be-sure","text":"A compromise solution that will work around the problems with LMOD and the cpe modules yet retain much of the spirit of the Cray PE, and that also can correct the targeting modules should the PrgEnv-* module not take the ones that you want (or ensure that at least certain other modules are loaded, even if they would be removed from the list of modules loaded by PrgEnv-gnu in an update of the system), is the following setup: easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = '21.04' toolchain = SYSTEM CPE_load = 'first' PrgEnv_load = True PrgEnv_family = 'cpeToolchain' cray_targets = [ 'craype-x86-rome' , 'craype-accel-host' , 'craype-network-ofi' ] dependencies = [ ( 'PrgEnv-gnu/8.0.0' , EXTERNAL_MODULE ), ( 'gcc/9.3.0' , EXTERNAL_MODULE ), ( 'craype/2.7.6' , EXTERNAL_MODULE ), ( 'cray-mpich/8.1.4' , EXTERNAL_MODULE ), ( 'cray-libsci/21.04.1.1' , EXTERNAL_MODULE ), ( 'cray-dsmml/0.1.4' , EXTERNAL_MODULE ), ( 'perftools-base/21.02.0' , EXTERNAL_MODULE ), ( 'xpmem' , EXTERNAL_MODULE ), ] moduleclass = 'toolchain' This setup will first load the cpe/21.04 and PrgEnv-gnu/8.0.0 modules to stay in the Cray PE spirit. Next the indicated targeting modules will be loaded, one for the CPU, one for the accelerator architecture and one for the network. This may trigger reloads of some other modules and will overwrite targeting modules of the same type loaded by PrgEnv-gnu . Finally, the gcc compiler module, the craype module and all other modules from the dependency list are loaded with the versions specified. This setup is a compromise that on one hand stays close to the Cray PE spirit by using the cpe and PrgEnv-gnu modules, yet works around some problems, namely: * Setting LMOD_MODULERCFILE does not work immediately. * Any corrective action when loading cpe after PrgEnv-gnu does not work * On a heterogeneous cluster, the targeting modules loaded by PrgEnv-gnu may not be the ones you want when cross-compiling or when the system would use the same file defining the modules for the whole system. * The list of modules loaded by PrgEnv-gnu may change as it is determined by a single file on the system that does not depend on the version of the Cray PE. In this case, you can always be sure that at least the modules mentioned in the dependency list and cray_targets parameter will be loaded. A variant of this would set CPE_load = 'after' which would load the cpe/21.04 module immediately after loading PrgEnv-gnu rather than just before, but with the current flaws of the cpe/21.04 module this still does not solve all problems: easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = '21.04' toolchain = SYSTEM CPE_load = 'after' PrgEnv_load = True PrgEnv_family = 'cpeToolchain' cray_targets = [ 'craype-x86-rome' , 'craype-accel-host' , 'craype-network-ofi' ] dependencies = [ ( 'PrgEnv-gnu/8.0.0' , EXTERNAL_MODULE ), ( 'gcc/9.3.0' , EXTERNAL_MODULE ), ( 'craype/2.7.6' , EXTERNAL_MODULE ), ( 'cray-mpich/8.1.4' , EXTERNAL_MODULE ), ( 'cray-libsci/21.04.1.1' , EXTERNAL_MODULE ), ( 'cray-dsmml/0.1.4' , EXTERNAL_MODULE ), ( 'perftools-base/21.02.0' , EXTERNAL_MODULE ), ( 'xpmem' , EXTERNAL_MODULE ), ] moduleclass = 'toolchain'","title":"Loading the cpe and PrgEnv modules first, then reloading packages just to be sure"},{"location":"CrayPEToolchain/#mimic-prgenv-load-hardcoded-versions-but-load-cpeyymm-first","text":"This is yet another compromise scenario: * Loading cpe/yy.mm first ensures that further modules a user might load after loading the cpe* module will load in the proper versions if a user does a versionless load. * Mimicing PrgEnv-* and loading modules explicitly ensures reproducibility over time as the list of modules loaded does not depend on a single file elsewhere in the system configuration which is not specific to a particular release of the PE. * Hard-coding the versions ensures that we avoid the problems caused by the implementation of the cpe/yy.mm modules (certainly in releases up to and including 21.06) easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = '21.04' toolchain = SYSTEM PrgEnv_load = False PrgEnv_family = 'PrgEnv' CPE_load = 'first' cray_targets = [ 'craype-x86-rome' , 'craype-accel-host' , 'craype-network-ofi' ] dependencies = [ ( 'PrgEnv-gnu/8.0.0' , EXTERNAL_MODULE ), ( 'gcc/9.3.0' , EXTERNAL_MODULE ), ( 'craype/2.7.6' , EXTERNAL_MODULE ), ( 'cray-mpich/8.1.4' , EXTERNAL_MODULE ), ( 'cray-libsci/21.04.1.1' , EXTERNAL_MODULE ), ( 'cray-dsmml/0.1.4' , EXTERNAL_MODULE ), ( 'perftools-base/21.02.0' , EXTERNAL_MODULE ), ( 'xpmem' , EXTERNAL_MODULE ), ] moduleclass = 'toolchain'","title":"Mimic PrgEnv, load hardcoded versions but load cpe/yy.mm first"},{"location":"CrayPEToolchain/#mimic-prgenv-and-load-cpeyymm-at-the-end","text":"This would be a valid scenario once the cpe/yy.mm modules have been corrected and work as they should. In this scenario, We mimic PrgEnv-* by setting the necessary environment variables and then loading a list of versionless modules. This avoids a problem with the actual PrgEnv modules as the list of modules they load depends on a single system file which is the same for all releases of the PE and hence may change over time. At the end the relevant cpe/yy.mm module is loaded to fix the versions of all already loaded modules. The corresponding EasyConfig file (minus help etc.) is: easyblock = 'CrayPEToolchain' name = 'cpeGNU' version = '21.04' toolchain = SYSTEM PrgEnv_load = False PrgEnv_family = 'PrgEnv' CPE_load = 'last' cray_targets = [ 'craype-x86-rome' , 'craype-accel-host' , 'craype-network-ofi' ] dependencies = [ ( 'gcc' , EXTERNAL_MODULE ), ( 'craype' , EXTERNAL_MODULE ), ( 'cray-mpich' , EXTERNAL_MODULE ), ( 'cray-libsci' , EXTERNAL_MODULE ), ( 'cray-dsmml' , EXTERNAL_MODULE ), ( 'perftools-base' , EXTERNAL_MODULE ), ( 'xpmem' , EXTERNAL_MODULE ), ] moduleclass = 'toolchain'","title":"Mimic PrgEnv and load cpe/yy.mm at the end"},{"location":"CrayPE_integration/","text":"Cray PE integration \u00b6 Cray PE components and the configuration of the module system and EasyBuild \u00b6 At several places in the configuration of the module system and of EasyBuild, information is needed about version numbers of CPE components per CPE release. For now, this information (and in particular version numbers) of CPE components is provided through a a .csv-file which is edited by hand and stored in the CrayPE subdirectory. It would be possible to extract some of the information in that file from parsing \\ /opt/cray/pe/cpe/yy.mm/modulerc.lua . In the future, HPE-Cray may deliver that information in a more easily machiine-readable format. There are several points where we need information about versions of specific packages in releases of the Cray PE: We need to define external modules to EasyBuild. That file is currently generate by make_EB_external_modules.py which is a wrapper that calls lumitools/gen_EB_external_modules_from_CPEdef.py to generate the EasyBuild external modules file for a particular version of the CPE. The external module definition file is stored in the easybuild configuration directory and is called external_modules_metadata-CPE-<CPE version>.cfg . Hence in the current design it is named after the version of the CPE and not the name of the software stack, so the developer and release versions of a software stack would share the same file. For now, as the cpe module cannot work correctly due to restrictions in LMOD, we need to specify the exact versions of packages in the various cpe* toolchain easyconfigs. In a later phase this may not be needed anymore though we might still want to avoid relying on the PrgEnv-* modules as the only source for the toolchain components as the components included via that file might change over time and are determined by a single file on the system which has to be the same for all releases. For now we need to overwrite the Cray PE cpe module for the release of the CPE. This is needed because the cpe module sets the LMOD_MODULERCFILE environment variable, a problem that should be solved from release 21.08 onwards. Moreover, our version implements a better strategy to reload modules so that (re)loading a cpe module at the end of the configuration of the PE will always set the right versions. But HPE Cray will use that method also starting from 21.08 onwards so that our own cpe modules will no longer be needed. We use a generic implementation of the module file that simply reads the .csv file to find out component versions. A module avail hook to hide those modules that are irrelevant to a particular LUMI/yy.mm toolchain could also use that information. For efficiency reasons this hook actually uses a LUA file which is generated from the .csv file with the components. Note that such a module only unclutters the display for users. A hidden module can still be loaded, and if that module is marked as default it would actually be loaded instead of another module with the same name and version. The information is also used to generate a modulerc file for each particular version of the LUMI/yy.mm software stack to mark the specific versions of the Cray PE modules for that release as the default (effectively already doing part of the work of the cpe modules when loading the software stack module). Within the software stack module we ensure that we load the matching version of the Cray targeting modules. We may need it to define Cray PE components to Spack Not developed yet.","title":"Cray PE integration"},{"location":"CrayPE_integration/#cray-pe-integration","text":"","title":"Cray PE integration"},{"location":"CrayPE_integration/#cray-pe-components-and-the-configuration-of-the-module-system-and-easybuild","text":"At several places in the configuration of the module system and of EasyBuild, information is needed about version numbers of CPE components per CPE release. For now, this information (and in particular version numbers) of CPE components is provided through a a .csv-file which is edited by hand and stored in the CrayPE subdirectory. It would be possible to extract some of the information in that file from parsing \\ /opt/cray/pe/cpe/yy.mm/modulerc.lua . In the future, HPE-Cray may deliver that information in a more easily machiine-readable format. There are several points where we need information about versions of specific packages in releases of the Cray PE: We need to define external modules to EasyBuild. That file is currently generate by make_EB_external_modules.py which is a wrapper that calls lumitools/gen_EB_external_modules_from_CPEdef.py to generate the EasyBuild external modules file for a particular version of the CPE. The external module definition file is stored in the easybuild configuration directory and is called external_modules_metadata-CPE-<CPE version>.cfg . Hence in the current design it is named after the version of the CPE and not the name of the software stack, so the developer and release versions of a software stack would share the same file. For now, as the cpe module cannot work correctly due to restrictions in LMOD, we need to specify the exact versions of packages in the various cpe* toolchain easyconfigs. In a later phase this may not be needed anymore though we might still want to avoid relying on the PrgEnv-* modules as the only source for the toolchain components as the components included via that file might change over time and are determined by a single file on the system which has to be the same for all releases. For now we need to overwrite the Cray PE cpe module for the release of the CPE. This is needed because the cpe module sets the LMOD_MODULERCFILE environment variable, a problem that should be solved from release 21.08 onwards. Moreover, our version implements a better strategy to reload modules so that (re)loading a cpe module at the end of the configuration of the PE will always set the right versions. But HPE Cray will use that method also starting from 21.08 onwards so that our own cpe modules will no longer be needed. We use a generic implementation of the module file that simply reads the .csv file to find out component versions. A module avail hook to hide those modules that are irrelevant to a particular LUMI/yy.mm toolchain could also use that information. For efficiency reasons this hook actually uses a LUA file which is generated from the .csv file with the components. Note that such a module only unclutters the display for users. A hidden module can still be loaded, and if that module is marked as default it would actually be loaded instead of another module with the same name and version. The information is also used to generate a modulerc file for each particular version of the LUMI/yy.mm software stack to mark the specific versions of the Cray PE modules for that release as the default (effectively already doing part of the work of the cpe modules when loading the software stack module). Within the software stack module we ensure that we load the matching version of the Cray targeting modules. We may need it to define Cray PE components to Spack Not developed yet.","title":"Cray PE components and the configuration of the module system and EasyBuild"},{"location":"SitePackage/","text":"LUMI prototype SitePackage.lua \u00b6 SitePackage.lua is used to implement various customistations to Lmod. The file defines both LMOD hooks and a number of new functions for the sandbox to ease the implementation of modulefiles for LUMI. Make sure that the environment variable LMOD_PACKAGE_PATH points to the directory that contains this file to activate this file. Settings \u00b6 Target modules per partition \u00b6 The target modules per partition are set via the init_module_list table. This also includes other modules that are set at initialisation. Default programming environment \u00b6 The default programming environment is set via the init_PrgEnv variable. List of LTS software stacks \u00b6 In the initial implementation every software stack that was not a development stack was marked as \"LTS\" but it turns out it will be very difficult initially to live up to the promise of 2 years of support. We simply need to change the programming environment too often and HPE Cray also does not keep supporting them. The LUMI-stacks with long-term support are set in LTS_LUMI_stacks Hooks \u00b6 SiteName hook \u00b6 This hook is used to define the prefix for some of the environment variables that Lmod will generate internally. Rather then just setting it to LUMI we decided to set it to LUMI_LMOD to lower the chance of conflicts with environment variables that may be defined elsewhere. avail hook \u00b6 This hook is used to replace directories with labels in the output of module avail . To work for the prototypes, one needs to set: export LMOD_AVAIL_STYLE = <label>:PEhierarchy:system which will make the labeled view the default but will still allow to see the directory view using module -s system avail The ModuleLabel modules can also be used to switch between the three settings: label : User-friendly labels for the directories, some directories collapsed into a single category, and all Cray PE modules collapsed into a single category. PEhierarchyy : User-friendly labels for the directories, some directories collapsed into a single category, but with the original Cray PE module hierarchy. system : The view showing the full directory path for each module directory. msgHook \u00b6 This hook is used to adapt the following messages: * output of module avail : Add more information about how to search for software and to contact LUMI User Support. Visibility hook \u00b6 The visibility hook is currently used to hide the Cray modules of other CPE versions in a particular version of the CPE software stack. As the hook function is called many times during a single call to module avail some effort was done to make it efficient at the cost of readability. The data about the modules that should not be hidden is contained in a LUA script in mgmt/LMOD/VisibilityHookData . This script is read via require so that it is cached and really processed only once. Furthermore, to make locating the script easy, the LUMI stack module stores the path and name in two environment variables that are ready-to-use without further substitutions. The file is auto-generated during the initialisation of a software stack. The name and location of the file is set through two environment variables set in the LUMI modules. Note that the feature is turned of for power users. Custom LMOD functions \u00b6 detect_LUMI_partition \u00b6 detect_LUMI_partition is a function that can be used to request the current LUMI partition. This is used by the generic modules/LUMIstack modules files but put in SitePackage.lua to have a single point where this is implemented. The alternative would be to use a trick that is also used in some CPE module files to read in and execute code from an external file. LUMI_OVERWRITE_PARTITION is defined and if so, the value of that variable is used. It is assumed to be C, G, D or L depending on the node type (CPU compute, GPU compute, data and visualisation or login), or can be common to install software in the hidden common partition. Currently the following rules are implemented for LUMI: Node name starts with uan : It must be a login node, so in partition/L Nude number 16-23: LUMI-D with GPU Node number 101-108: LUMI-D without GPY, assing to partition/L Node number 1000-2535: Regular LUMI-C compute nodes, assign partition/C Other nodes are for now assigned to partition/L. The idea is to ensure that a module update would reload the loaded software stack for the partition on which the module update command is run. get_CPE_component \u00b6 get_CPE_component is a function that can be used in modulefiles to request the version of a CPE component. The data is read from the CPE definition files in the CrayPE subdirectory of the repository. The function is currently used in the modules/LUMIpartition generic implementation of the partition modules for the LUMI stacks to determine the version of the Cray targeting modules to add that directory to the MODULEPATH. get_CPE_versions \u00b6 get_CPE_versions is a function that can be used in module files to request a table with the version for each package in a CPE release. The data is read from the CPE definition files in the CrayPE subdirectory of the repository. The function is used in the prototype in the cpe modules for the Grenoble system as a proof-of-concept for a generic cpe module to reduce the number of places where the version info of the Cray packages in a CPE release is kept. get_versionedfile \u00b6 get_versionedfile is a function that can be used to find the suitable versioned file for a given version of the LUMI software stack, i.e., the file which according to the version number is the most recent one not newer than the software stack. get_hostname \u00b6 get_hostname gets the hostname from the output of the hostname command. It is meant to be used by detect_LUMI_partition but is also exported so that other module files can use it if needed. get_user_prefix_EasyBuild \u00b6 get_user_prefix_EasyBuild computes the root of the user EasyBuild installation from the environment variable EBU_USER_PREFIX and the default name in the home directory. It is used in the EasyBuild-config module, the LUMIpartition module (to include the user module directory in the MODULEPATH`) and in the avail_hook`` LMOD hook. get_init_module_list \u00b6 get_init_module_list is a function that returns the list of modules to load at initialisation. This includes target modules, other modules, and optionally the default programming environment. The function takes two arguments: The partition (L, C, G, D) A boolean: When true the default programming environment is added to the list of modules that is generated as a result of the function. get_motd \u00b6 get_motd returns the message-fo-the-day as stored in etc/motd in the repository root. The function takes no input arguments. get_fortune \u00b6 get_fortune works as the old UNIX fortune command. It returns a random tip read from etc/lumi_fortune.txt in the repository root. is_interactive \u00b6 is_interactive returns true if it is called in an interactive shell, otherwise false. The function takes no input arguments. It is used to ensure that the message-of-the-day is not printed in cases where Linux would not print it. is_LTS_LUMI_stack \u00b6 is_LTS_LUMI_stack takes one input argument: the version of the LUMI stack. It returns true if that version is a LTS stack and returns false otherwise.","title":"The contents of the SitePackage.lua file"},{"location":"SitePackage/#lumi-prototype-sitepackagelua","text":"SitePackage.lua is used to implement various customistations to Lmod. The file defines both LMOD hooks and a number of new functions for the sandbox to ease the implementation of modulefiles for LUMI. Make sure that the environment variable LMOD_PACKAGE_PATH points to the directory that contains this file to activate this file.","title":"LUMI prototype SitePackage.lua"},{"location":"SitePackage/#settings","text":"","title":"Settings"},{"location":"SitePackage/#target-modules-per-partition","text":"The target modules per partition are set via the init_module_list table. This also includes other modules that are set at initialisation.","title":"Target modules per partition"},{"location":"SitePackage/#default-programming-environment","text":"The default programming environment is set via the init_PrgEnv variable.","title":"Default programming environment"},{"location":"SitePackage/#list-of-lts-software-stacks","text":"In the initial implementation every software stack that was not a development stack was marked as \"LTS\" but it turns out it will be very difficult initially to live up to the promise of 2 years of support. We simply need to change the programming environment too often and HPE Cray also does not keep supporting them. The LUMI-stacks with long-term support are set in LTS_LUMI_stacks","title":"List of LTS software stacks"},{"location":"SitePackage/#hooks","text":"","title":"Hooks"},{"location":"SitePackage/#sitename-hook","text":"This hook is used to define the prefix for some of the environment variables that Lmod will generate internally. Rather then just setting it to LUMI we decided to set it to LUMI_LMOD to lower the chance of conflicts with environment variables that may be defined elsewhere.","title":"SiteName hook"},{"location":"SitePackage/#avail-hook","text":"This hook is used to replace directories with labels in the output of module avail . To work for the prototypes, one needs to set: export LMOD_AVAIL_STYLE = <label>:PEhierarchy:system which will make the labeled view the default but will still allow to see the directory view using module -s system avail The ModuleLabel modules can also be used to switch between the three settings: label : User-friendly labels for the directories, some directories collapsed into a single category, and all Cray PE modules collapsed into a single category. PEhierarchyy : User-friendly labels for the directories, some directories collapsed into a single category, but with the original Cray PE module hierarchy. system : The view showing the full directory path for each module directory.","title":"avail hook"},{"location":"SitePackage/#msghook","text":"This hook is used to adapt the following messages: * output of module avail : Add more information about how to search for software and to contact LUMI User Support.","title":"msgHook"},{"location":"SitePackage/#visibility-hook","text":"The visibility hook is currently used to hide the Cray modules of other CPE versions in a particular version of the CPE software stack. As the hook function is called many times during a single call to module avail some effort was done to make it efficient at the cost of readability. The data about the modules that should not be hidden is contained in a LUA script in mgmt/LMOD/VisibilityHookData . This script is read via require so that it is cached and really processed only once. Furthermore, to make locating the script easy, the LUMI stack module stores the path and name in two environment variables that are ready-to-use without further substitutions. The file is auto-generated during the initialisation of a software stack. The name and location of the file is set through two environment variables set in the LUMI modules. Note that the feature is turned of for power users.","title":"Visibility hook"},{"location":"SitePackage/#custom-lmod-functions","text":"","title":"Custom LMOD functions"},{"location":"SitePackage/#detect_lumi_partition","text":"detect_LUMI_partition is a function that can be used to request the current LUMI partition. This is used by the generic modules/LUMIstack modules files but put in SitePackage.lua to have a single point where this is implemented. The alternative would be to use a trick that is also used in some CPE module files to read in and execute code from an external file. LUMI_OVERWRITE_PARTITION is defined and if so, the value of that variable is used. It is assumed to be C, G, D or L depending on the node type (CPU compute, GPU compute, data and visualisation or login), or can be common to install software in the hidden common partition. Currently the following rules are implemented for LUMI: Node name starts with uan : It must be a login node, so in partition/L Nude number 16-23: LUMI-D with GPU Node number 101-108: LUMI-D without GPY, assing to partition/L Node number 1000-2535: Regular LUMI-C compute nodes, assign partition/C Other nodes are for now assigned to partition/L. The idea is to ensure that a module update would reload the loaded software stack for the partition on which the module update command is run.","title":"detect_LUMI_partition"},{"location":"SitePackage/#get_cpe_component","text":"get_CPE_component is a function that can be used in modulefiles to request the version of a CPE component. The data is read from the CPE definition files in the CrayPE subdirectory of the repository. The function is currently used in the modules/LUMIpartition generic implementation of the partition modules for the LUMI stacks to determine the version of the Cray targeting modules to add that directory to the MODULEPATH.","title":"get_CPE_component"},{"location":"SitePackage/#get_cpe_versions","text":"get_CPE_versions is a function that can be used in module files to request a table with the version for each package in a CPE release. The data is read from the CPE definition files in the CrayPE subdirectory of the repository. The function is used in the prototype in the cpe modules for the Grenoble system as a proof-of-concept for a generic cpe module to reduce the number of places where the version info of the Cray packages in a CPE release is kept.","title":"get_CPE_versions"},{"location":"SitePackage/#get_versionedfile","text":"get_versionedfile is a function that can be used to find the suitable versioned file for a given version of the LUMI software stack, i.e., the file which according to the version number is the most recent one not newer than the software stack.","title":"get_versionedfile"},{"location":"SitePackage/#get_hostname","text":"get_hostname gets the hostname from the output of the hostname command. It is meant to be used by detect_LUMI_partition but is also exported so that other module files can use it if needed.","title":"get_hostname"},{"location":"SitePackage/#get_user_prefix_easybuild","text":"get_user_prefix_EasyBuild computes the root of the user EasyBuild installation from the environment variable EBU_USER_PREFIX and the default name in the home directory. It is used in the EasyBuild-config module, the LUMIpartition module (to include the user module directory in the MODULEPATH`) and in the avail_hook`` LMOD hook.","title":"get_user_prefix_EasyBuild"},{"location":"SitePackage/#get_init_module_list","text":"get_init_module_list is a function that returns the list of modules to load at initialisation. This includes target modules, other modules, and optionally the default programming environment. The function takes two arguments: The partition (L, C, G, D) A boolean: When true the default programming environment is added to the list of modules that is generated as a result of the function.","title":"get_init_module_list"},{"location":"SitePackage/#get_motd","text":"get_motd returns the message-fo-the-day as stored in etc/motd in the repository root. The function takes no input arguments.","title":"get_motd"},{"location":"SitePackage/#get_fortune","text":"get_fortune works as the old UNIX fortune command. It returns a random tip read from etc/lumi_fortune.txt in the repository root.","title":"get_fortune"},{"location":"SitePackage/#is_interactive","text":"is_interactive returns true if it is called in an interactive shell, otherwise false. The function takes no input arguments. It is used to ensure that the message-of-the-day is not printed in cases where Linux would not print it.","title":"is_interactive"},{"location":"SitePackage/#is_lts_lumi_stack","text":"is_LTS_LUMI_stack takes one input argument: the version of the LUMI stack. It returns true if that version is a LTS stack and returns false otherwise.","title":"is_LTS_LUMI_stack"},{"location":"configurations/","text":"Configurations used on LUMI \u00b6 Cray PE EasyBuild Toolchains 21.08 4.4.2 cpeCray, cpeGNU 21.12 4.5.3 cpeCray, cpeGNU, cpeAOCC","title":"Configurations on LUMI"},{"location":"configurations/#configurations-used-on-lumi","text":"Cray PE EasyBuild Toolchains 21.08 4.4.2 cpeCray, cpeGNU 21.12 4.5.3 cpeCray, cpeGNU, cpeAOCC","title":"Configurations used on LUMI"},{"location":"directory_structure/","text":"The directory structure \u00b6 System directory \u00b6 Most of this hierarchy is created with the prepare_LUMI.sh script. The Cray PE modules are left in their own hierarchy. In the LUMI software stack installation directory, one can find the following subdirectories: modules : Key here was to follow Lmod guidelines on hierarchical structures for the software stack itself, though once at the level of the software we currently do not use a hierarchy. SoftwareStack : A module file that enables the default Cray environment, and one for each of our LUMI software stacks, of the form LUMI/version.lua. SystemPartition/LUMI/yy.mm : The next level in the hierarchy. It contains modules by LUMI SoftwareStack to enable the different partitions. Structure: The modules are called partition/C.lua etc. We could not use LUMIpartition as this lead to problems with the Lmod hierachyA function which produced wrong results for LUMIpartition/L.lua but not for LUMIpartition/C.lua even though both modules had identical code. Besides the four regular partitions, there is also a meta-partition common that is used to house software that is common to all regular partitions. The corresponding module is hidden from regular users. Infrastructure/LUMI/yy.mm/partition/part : Infrastructure modules. This structure is needed for those modules of which we need versions of each of the regular partition and for the common partition. This does include the modules that are used for EasyBuild settings. easybuild/LUMI/yy.mm/partition/part : Directory for the EasyBuild-generated modules for the LUMI/yy.mm software stack for the LUMI-part partition (part actually being a single letter, except for the software that is common to all partitions, where part is common) easybuild/CrayEnv : Directory for the EasyBuild-generated modules for the CrayEnv software stack. easybuild/system : Directory for the EasyBuild-generated modules outside any software stack spack/LUMI/yy.mm/partition/part/<archstring> : Similar as the above, but for Spack-installed software. manual/LUMI/yy.mm/partition/part : Similar as the above, but for manually installed software. CrayOverwrite : A directory that is currently used to implement modules that are missing on our test system in Grenoble and to work around some of the problems in the Cray cpe/yy.mm modules. Note that these modules seem to give other problems so though we still put them on the system they are currently disabled. StyleModifiers : Links to the corresponding module in the repository. It contains the modules that can be used to change the presentation of the modules in module avail . SW : This is for the actual binaries, lib directories etc. Names are deliberately kept short to avoid problems with too long shebang lines. As shebang lines do not undergoe variable expansion, we cannot use the EBROOT variables and so on in those lines to save space. LUMI-yy.mm C EB SP MNL G D L common CrayEnv system mgmt : Files that are not stored in our GitHub, but are generated on the fly and are only useful to those users who want to build upon our software stack or for those who install software in our stacks. ebrepo_files LUMI-yy.mm LUMI-C LUMI-G LUMI-D LUMI-L LUMI-common CrayEnv system LMOD : Additional files for LMOD VisibilityHookData : Auto-generated files used by the LMOD SitePackage.lua file to hide Cray modules that are irrelevant for a particular software stack. sources : Directory structure to store sources of installed programs so that they can be reinstalled even if sources would no longer be downloadable easybuild : Sources downloaded by EasyBuild. The internal structure of this directory is determined by EasyBuild. Further subdirectories are not fixed yet, but the suggestion is to also provide a manual subdirectory for the sources of those packages that are installed manually and a spack subdirectory for spack-installed software when we proceed with the Spack integration. SystemRepo: GitHub repository with all managed files. The name is not fixed, any name can be used and will be picked up if the scripts from the scripts subdirectory inside the repository are used to initialise a new software stack or to determine the values for some of the environment variables for LMOD. For the structure inside the repository, see the \"overview of files in the repository and where they are being used . LUMI-EasyBuild-contrib (optional and not created by the script): A clone of the LUMI-EasyBuild-contrib repository only used for search in EasyBuild. User EasyBuild setup \u00b6 This is a very simplified version of the system directory structure with levels in the directory structure omitted when they don't make sense as this structure is for EasyBuild installations only. modules/LUMI/yy.mm/partition/part : Directory for the EasyBuild-generated modules for the LUMI/yy.mm software stack for the LUMI-part partition (part actually being a single letter, except for the software that is common to all partitions, where part is common) SW/LUMI-yy.mm/part : This is for the actual binaries, lib directories etc. Names are deliberately kept short to avoid problems with too long shebang lines. As shebang lines do not undergoe variable expansion, we cannot use the EBROOT variables and so on in those lines to save space. As for the modules directory, part is C , G , D , L or common . ebrepo_files/LUMI-yy.mm/LUMI-part for the EasyBuild repository of installed EasyConfigs in the user directory. As for the modules directory, part is C , G , D , L or common . sources : Subdirectory where EasyBuild stores sources of isntalled packages. The internal structure is fully determined by EasyBuild. UserRepo : The user EasyBuild repo. Contrary to the repository in the system directories, the name UserRepo is mandatory here. Subdirectories are easybuild/config to add to the system config files (the easybuild-user.cfg and easybuild-user-LUMI-yy.mm.cfg files, see the Setup of a LUMI software stack and EasyBuild page) easybuild/easyblkocks for additional custom EasyBlocks. We currently assume an organisation in two levels (first letter and then the python file). easybuild/easyconfigs for the EasyConfigs.","title":"Installation directory structure"},{"location":"directory_structure/#the-directory-structure","text":"","title":"The directory structure"},{"location":"directory_structure/#system-directory","text":"Most of this hierarchy is created with the prepare_LUMI.sh script. The Cray PE modules are left in their own hierarchy. In the LUMI software stack installation directory, one can find the following subdirectories: modules : Key here was to follow Lmod guidelines on hierarchical structures for the software stack itself, though once at the level of the software we currently do not use a hierarchy. SoftwareStack : A module file that enables the default Cray environment, and one for each of our LUMI software stacks, of the form LUMI/version.lua. SystemPartition/LUMI/yy.mm : The next level in the hierarchy. It contains modules by LUMI SoftwareStack to enable the different partitions. Structure: The modules are called partition/C.lua etc. We could not use LUMIpartition as this lead to problems with the Lmod hierachyA function which produced wrong results for LUMIpartition/L.lua but not for LUMIpartition/C.lua even though both modules had identical code. Besides the four regular partitions, there is also a meta-partition common that is used to house software that is common to all regular partitions. The corresponding module is hidden from regular users. Infrastructure/LUMI/yy.mm/partition/part : Infrastructure modules. This structure is needed for those modules of which we need versions of each of the regular partition and for the common partition. This does include the modules that are used for EasyBuild settings. easybuild/LUMI/yy.mm/partition/part : Directory for the EasyBuild-generated modules for the LUMI/yy.mm software stack for the LUMI-part partition (part actually being a single letter, except for the software that is common to all partitions, where part is common) easybuild/CrayEnv : Directory for the EasyBuild-generated modules for the CrayEnv software stack. easybuild/system : Directory for the EasyBuild-generated modules outside any software stack spack/LUMI/yy.mm/partition/part/<archstring> : Similar as the above, but for Spack-installed software. manual/LUMI/yy.mm/partition/part : Similar as the above, but for manually installed software. CrayOverwrite : A directory that is currently used to implement modules that are missing on our test system in Grenoble and to work around some of the problems in the Cray cpe/yy.mm modules. Note that these modules seem to give other problems so though we still put them on the system they are currently disabled. StyleModifiers : Links to the corresponding module in the repository. It contains the modules that can be used to change the presentation of the modules in module avail . SW : This is for the actual binaries, lib directories etc. Names are deliberately kept short to avoid problems with too long shebang lines. As shebang lines do not undergoe variable expansion, we cannot use the EBROOT variables and so on in those lines to save space. LUMI-yy.mm C EB SP MNL G D L common CrayEnv system mgmt : Files that are not stored in our GitHub, but are generated on the fly and are only useful to those users who want to build upon our software stack or for those who install software in our stacks. ebrepo_files LUMI-yy.mm LUMI-C LUMI-G LUMI-D LUMI-L LUMI-common CrayEnv system LMOD : Additional files for LMOD VisibilityHookData : Auto-generated files used by the LMOD SitePackage.lua file to hide Cray modules that are irrelevant for a particular software stack. sources : Directory structure to store sources of installed programs so that they can be reinstalled even if sources would no longer be downloadable easybuild : Sources downloaded by EasyBuild. The internal structure of this directory is determined by EasyBuild. Further subdirectories are not fixed yet, but the suggestion is to also provide a manual subdirectory for the sources of those packages that are installed manually and a spack subdirectory for spack-installed software when we proceed with the Spack integration. SystemRepo: GitHub repository with all managed files. The name is not fixed, any name can be used and will be picked up if the scripts from the scripts subdirectory inside the repository are used to initialise a new software stack or to determine the values for some of the environment variables for LMOD. For the structure inside the repository, see the \"overview of files in the repository and where they are being used . LUMI-EasyBuild-contrib (optional and not created by the script): A clone of the LUMI-EasyBuild-contrib repository only used for search in EasyBuild.","title":"System directory"},{"location":"directory_structure/#user-easybuild-setup","text":"This is a very simplified version of the system directory structure with levels in the directory structure omitted when they don't make sense as this structure is for EasyBuild installations only. modules/LUMI/yy.mm/partition/part : Directory for the EasyBuild-generated modules for the LUMI/yy.mm software stack for the LUMI-part partition (part actually being a single letter, except for the software that is common to all partitions, where part is common) SW/LUMI-yy.mm/part : This is for the actual binaries, lib directories etc. Names are deliberately kept short to avoid problems with too long shebang lines. As shebang lines do not undergoe variable expansion, we cannot use the EBROOT variables and so on in those lines to save space. As for the modules directory, part is C , G , D , L or common . ebrepo_files/LUMI-yy.mm/LUMI-part for the EasyBuild repository of installed EasyConfigs in the user directory. As for the modules directory, part is C , G , D , L or common . sources : Subdirectory where EasyBuild stores sources of isntalled packages. The internal structure is fully determined by EasyBuild. UserRepo : The user EasyBuild repo. Contrary to the repository in the system directories, the name UserRepo is mandatory here. Subdirectories are easybuild/config to add to the system config files (the easybuild-user.cfg and easybuild-user-LUMI-yy.mm.cfg files, see the Setup of a LUMI software stack and EasyBuild page) easybuild/easyblkocks for additional custom EasyBlocks. We currently assume an organisation in two levels (first letter and then the python file). easybuild/easyconfigs for the EasyConfigs.","title":"User EasyBuild setup"},{"location":"easybuild_setup/","text":"EasyBuild setup \u00b6 Configuration decisions \u00b6 EasyBuild Module Naming Scheme \u00b6 Options A flat naming scheme, even without the module classes as they are of little use. May packages belong to more than one class, it is impossible to come up with a consistent categorization. Fully omitting the categorization requires a slightly customized naming scheme that can be copied from UAntwerpen. When combining with --suffix-modules-path='' one can also drop the 'all' subdirectory level which is completely unnecessary in that case. A hierarchical naming scheme as used at CSCS and CSC. Note that CSCS has an open bug report at the time of writing (May 12) on the standard implementation in EasyBuild ( Issue #3626 and the related issue 3575 ). The solution might be to develop our own naming scheme module. Choice implemented for the current software stacks: A flat naming scheme that is a slight customization of the default EasyBuildMNS without the categories, combined with an empty suffix-modules-path to avoid adding the unnecessary all subdirectory level in the module tree. As we need to point to our own module naming scheme implementation which is hard to do in a configuration file (as the path needs to be hardcoded), the settings for the module scheme are done via EASYBUILD_* environment variables, specifically: EASYBUILD_INCLUDE_MODULE_NAMING_SCHEMES to add out own module naming schemes EASYBUILD_MODULE_NAMING_SCHEME=LUMI_FlatMNS EASYBUILD_SUFFIX_MODULES_PATH='' Other configuration decisions \u00b6 TODO: rpath or not? TODO: Hiding many basic libraries External modules for integration with the Cray PE \u00b6 See the Cray PE integration page . Running EasyBuild \u00b6 EasyBuild for LUMI is configured through a set of EasyBuild configuration files and environment variables. The basic idea is to never load the EasyBuild module directly without using one of the EasyBuild configuration modules. There are three such modules EasyBuild-production to do the installations in the production directories. EasyBuild-infrastructure is similar to EasyBuild-production but places the module in the Infrastructure tree rather than the easybuild tree. EasyBuild-user is meant to do software installations in the home directory of a user or in their project directory. This module will configure EasyBuild such that it builds on top of the software already installed on the system, with a compatible directory structure. This module is not only useful to regular users, but also to LUST to develop EasyConfig files for installation on the system. We aim to develop the module in such a way that being able to install the module in a home or project subdirectory would also practically guarantee that the installation will also work in the system directories. These three modules are implemented as one generic module, EasyBuild-config , that is symlinked to those three modules. The module will derive what it should do from its name and also gets all information about the software stack and partition from its place in the module hierarchy to ensure maximum robustness. NOTE: We deliberately chose to use EasyBuild-production, EasyBuild-user etc and not EasyBuild-config/production, EasyBuild-config/user, etc. because of the way EasyBuild acts when loading a different partition module. For additional safety we want to avoid that LMOD would reload a different version of the EasyBuild-config module and would rather get an error message that the EasyBuild-* partition is not available in the new partition. Common settings that are made through environment variables in all modes \u00b6 The buildpath and path for temporary files. The current implementation creates subdirectories in the directory pointed to by XDG_RUNTIME_DIR as this is a RAM-based file system and gets cleaned when the user logs out. This value is based on the CSCS setup. Name and location of the EasyBuild external modules definition file. Settings for the module naming scheme: As we need to point to a custom implementation of the module naming schemes, this is done through an environment variable. For consistency we also set the module naming scheme itself via a variable and set EASYBUILD_SUFFIX_MODULES_PATH as that together with the module naming scheme determines the location of the modules with respect to the module install path. EASYBUILD_OPTARCH has been extended compared to the CSCS setup: We support multiple target modules so that it is possible to select both the CPU and accelerator via EASYBUILD_OPTARCH . See the EasyBuild CPE toolchains common options It is now also possible to specify arguments for multiple compilers. Use CPE: to mark the options for the CPE toolchains. See also EasyBuild CPE toolchains common options As the CPE toolchains are not included with the standard EasyBuild distribution and as we have also extended them (if those from CSCS would ever be included), we set EASYBUILD_INCLUDE_TOOLCHAINS to tell EasyBuild where to find the toolchains. The EasyBuild-production and EasyBuild-infrastructure mode \u00b6 Most of the settings for EasyBuild on LUMI are controlled through environment variables as this gives us more flexibility and a setup that we can easily redo in a different directory (e.g., to test on the test system). Some settings that are independent of the directory structure and independent of the software stack are still done in regular configuration files. There are two regular configuration files: easybuild-production.cfg is always read. In the current implementation it is assumed to be present. easybuild-production-LUMI-yy.mm.cfg is read after production.cfg , hence can be used to overwrite settings made in production.cfg for a specific toolchain. This allows us to evolve the configuration files while keeping the possibility to install in older versions of the LUMI software stack. Settings made in the configuration files: Modules tool and modules syntax. Modules that may be loaded when EasyBuild runs Modules that should be hidden. Starting point is currently the list of CSCS. TODO: Not yet enabled as this makes development more difficult. In fact, another (untested) option is to hide the modules via a modulerc file in the module system rather than via EasyBuild which would have the advantage that they maintain regular version numbers rather than version numbers that start with a dot (as it seems that that version number with the dot should then also be used consistently). Ignore EBROOT variables without matching module as we use this to implement Bundles that are detected by certain EasyBlocks as if each package included in the Bundle was installed as a separate package. The following settings are made through environment variables: Software and module install paths, according to the directory scheme given in the module system section. The directory where sources will be stored, as indicated in the directory structure overview. The repo directories where EasyBuild stores EasyConfig files for the modules that are build, as indicated in the directory structure overview. EasyBuild robot paths: we use EASYBUILD_ROBOT_PATHS and not EASYBUILD_ROBOT so searching the robot path is not enabled by default but can be controlled through the -r flag of the eb command. The search order is: The repository for the currently active partition build by EasyBuild for installed packages ( ebrepo_files ) The repository for the common partition build by EasyBuild for installed packages ( ebrepo_files ) The LUMI-specific EasyConfig directory. We deliberately put the ebrepo_files repositories first as this ensure that EasyBuild will always find the EasyConfig file for the installed module first as changes may have been made to the EasyConfig in the LUMI EasyConfig repository that are not yet reflected in the installed software. The default EasyConfig files that come with EasyBuild are not put in the robot search path for two reasons: They are not made for the Cray toolchains anyway (though one could of course use --try-toolchain etc.) We want to ensure that our EasyConfig repository is complete so that we can impose our own standards on, e.g., adding information to the help block or whatis lines in modules, and do not accidentally install dependencies without realising this. Names and locations of the EasyBuild configuration files and of the external modules definition file. Settings for the module naming scheme: As we need to point to a custom implementation of the module naming schemes, this is done through an environment variable. For consistency we also set the module naming scheme itself via a variable and set EASYBUILD_SUFFIX_MODULES_PATH as that together with the module naming scheme determines the location of the modules with respect to the module install path. Custom EasyBlocks Search path for EasyConfig files with eb -S and eb --search Every directory of the robot search path is automatically included and does not need to be added to EASYBUILD_SEARCH_PATHS The LUMI-EasyBuild-contrib repository , as that one is not in the robot path in non-user mode. Not yet done, but we could maintain a local copy of the CSCS repository and enable search in that also. Default EasyConfig files that come with EasyBuild (if we can find EasyBuild, which is if an EasyBuild-build EasyBuild module is loaded) Deliberately not included: Our ebrepo_files repositories. Everything in there should be in our own EasyConfig repository if the installations are managed properly. We also set containerpath and packagepath even though we don't plan to use those, but it ensures that files produced by this option will not end up in our GitHub repository. The EasyBuild-user mode \u00b6 The root of the user EasyBuild directory structure is pointed to by the environment variable EBU_USER_PREFIX . The default value if the variable is not defined is $HOME/EasyBuild . Note that this environment variable is also used in the LUMI/yy.mm modules as these modules try to include the user modules in the MODULEPATH. The directory structure in that directory largely reflects the system directory structure. This may be a bit more complicated than really needed for the user who does an occasional install, but is great for user communities who install a more elaborate software stack in their project directory. Changes: SystemRepo is named UserRepo instead and that name is fixed, contrary to the SytemRepo name. We do keep it as a separate level so that the user can also easily do version tracking via a versioning system such as GitHub. The mgmt level is missing as we do not take into account subdirectories that might be related to other software management tools. As there are only modules generated by EasyBuild in this module tree, modules/easybuild simply becomes modules . Similarly, the EB level in the directory for installed software is omitted. The robot search path: The user repository for the currently active partition The user repository for the common partition (if different from the previous one) The system repository for the currently active partition The system repository for the common partition (if different from the previous one) The user EasyConfig directory UserRepo (even if it is not there yet) The LUMI-EasyBuild-contrib repository, if present in the user directory use that one and otherwise use the one from the central installation. The LUMI-specific EasyConfig directory from the application directory The search path for EasyConfig files with eb -S and eb --search The directories above in the robot search path are automatically also used for search. Not yet done, but we could maintain a local copy of the CSCS repository and enable search in that also. Default EasyConfig files that come with EasyBuild are deliberately not included in user mode as it was decided this is confusing for the users. Deliberately not included: Our ebrepo_files repositories. Everything in there should be in our own EasyConfig repository if the installations are managed properly. So currently the additional search paths in user mode are empty. There are two regular configuration files: The system easybuild-production.cfg is always read. In the current implementation it is assumed to be present. The user easybuild-user.cfg (in UserRepo/easybuild/config in the user direcgtory) is read next and meant for user-specific settings that should be read for all LUMI software stacks. Then the system easybuild-production-LUMI-yy.mm.cfg is read after, hence can be used to overwrite settings made in production.cfg for a specific toolchain. This allows us to evolve the configuration files while keeping the possibility to install in older versions of the LUMI software stack. This will overwrite generic user options! Finally the user easybuild-user-LUMI-yy.mm.cfg is read for user customizations to a specific toolchain. Only the first of those 4 files has to be present. Presence of the others is detected when the module is loaded. Reload the module after creating one of these files to start using it. Comparison: robot path non-user robot path user / ebrepo user active partition / ebrepo user common partition ebrepo system active partition ebrepo system active partition ebrepo system common partition ebrepo system common partition EBU_USER_PREFIX/UserRepo LUMI-EasyBuild-contrib user or system LUMI-SoftwareStack system LUMI-SoftwareStack system search path non-user search path user LUMI-EasyBuild-contrib system / Default easybuild EasyConfigs /","title":"EasyBuild setup"},{"location":"easybuild_setup/#easybuild-setup","text":"","title":"EasyBuild setup"},{"location":"easybuild_setup/#configuration-decisions","text":"","title":"Configuration decisions"},{"location":"easybuild_setup/#easybuild-module-naming-scheme","text":"Options A flat naming scheme, even without the module classes as they are of little use. May packages belong to more than one class, it is impossible to come up with a consistent categorization. Fully omitting the categorization requires a slightly customized naming scheme that can be copied from UAntwerpen. When combining with --suffix-modules-path='' one can also drop the 'all' subdirectory level which is completely unnecessary in that case. A hierarchical naming scheme as used at CSCS and CSC. Note that CSCS has an open bug report at the time of writing (May 12) on the standard implementation in EasyBuild ( Issue #3626 and the related issue 3575 ). The solution might be to develop our own naming scheme module. Choice implemented for the current software stacks: A flat naming scheme that is a slight customization of the default EasyBuildMNS without the categories, combined with an empty suffix-modules-path to avoid adding the unnecessary all subdirectory level in the module tree. As we need to point to our own module naming scheme implementation which is hard to do in a configuration file (as the path needs to be hardcoded), the settings for the module scheme are done via EASYBUILD_* environment variables, specifically: EASYBUILD_INCLUDE_MODULE_NAMING_SCHEMES to add out own module naming schemes EASYBUILD_MODULE_NAMING_SCHEME=LUMI_FlatMNS EASYBUILD_SUFFIX_MODULES_PATH=''","title":"EasyBuild Module Naming Scheme"},{"location":"easybuild_setup/#other-configuration-decisions","text":"TODO: rpath or not? TODO: Hiding many basic libraries","title":"Other configuration decisions"},{"location":"easybuild_setup/#external-modules-for-integration-with-the-cray-pe","text":"See the Cray PE integration page .","title":"External modules for integration with the Cray PE"},{"location":"easybuild_setup/#running-easybuild","text":"EasyBuild for LUMI is configured through a set of EasyBuild configuration files and environment variables. The basic idea is to never load the EasyBuild module directly without using one of the EasyBuild configuration modules. There are three such modules EasyBuild-production to do the installations in the production directories. EasyBuild-infrastructure is similar to EasyBuild-production but places the module in the Infrastructure tree rather than the easybuild tree. EasyBuild-user is meant to do software installations in the home directory of a user or in their project directory. This module will configure EasyBuild such that it builds on top of the software already installed on the system, with a compatible directory structure. This module is not only useful to regular users, but also to LUST to develop EasyConfig files for installation on the system. We aim to develop the module in such a way that being able to install the module in a home or project subdirectory would also practically guarantee that the installation will also work in the system directories. These three modules are implemented as one generic module, EasyBuild-config , that is symlinked to those three modules. The module will derive what it should do from its name and also gets all information about the software stack and partition from its place in the module hierarchy to ensure maximum robustness. NOTE: We deliberately chose to use EasyBuild-production, EasyBuild-user etc and not EasyBuild-config/production, EasyBuild-config/user, etc. because of the way EasyBuild acts when loading a different partition module. For additional safety we want to avoid that LMOD would reload a different version of the EasyBuild-config module and would rather get an error message that the EasyBuild-* partition is not available in the new partition.","title":"Running EasyBuild"},{"location":"easybuild_setup/#common-settings-that-are-made-through-environment-variables-in-all-modes","text":"The buildpath and path for temporary files. The current implementation creates subdirectories in the directory pointed to by XDG_RUNTIME_DIR as this is a RAM-based file system and gets cleaned when the user logs out. This value is based on the CSCS setup. Name and location of the EasyBuild external modules definition file. Settings for the module naming scheme: As we need to point to a custom implementation of the module naming schemes, this is done through an environment variable. For consistency we also set the module naming scheme itself via a variable and set EASYBUILD_SUFFIX_MODULES_PATH as that together with the module naming scheme determines the location of the modules with respect to the module install path. EASYBUILD_OPTARCH has been extended compared to the CSCS setup: We support multiple target modules so that it is possible to select both the CPU and accelerator via EASYBUILD_OPTARCH . See the EasyBuild CPE toolchains common options It is now also possible to specify arguments for multiple compilers. Use CPE: to mark the options for the CPE toolchains. See also EasyBuild CPE toolchains common options As the CPE toolchains are not included with the standard EasyBuild distribution and as we have also extended them (if those from CSCS would ever be included), we set EASYBUILD_INCLUDE_TOOLCHAINS to tell EasyBuild where to find the toolchains.","title":"Common settings that are made through environment variables in all modes"},{"location":"easybuild_setup/#the-easybuild-production-and-easybuild-infrastructure-mode","text":"Most of the settings for EasyBuild on LUMI are controlled through environment variables as this gives us more flexibility and a setup that we can easily redo in a different directory (e.g., to test on the test system). Some settings that are independent of the directory structure and independent of the software stack are still done in regular configuration files. There are two regular configuration files: easybuild-production.cfg is always read. In the current implementation it is assumed to be present. easybuild-production-LUMI-yy.mm.cfg is read after production.cfg , hence can be used to overwrite settings made in production.cfg for a specific toolchain. This allows us to evolve the configuration files while keeping the possibility to install in older versions of the LUMI software stack. Settings made in the configuration files: Modules tool and modules syntax. Modules that may be loaded when EasyBuild runs Modules that should be hidden. Starting point is currently the list of CSCS. TODO: Not yet enabled as this makes development more difficult. In fact, another (untested) option is to hide the modules via a modulerc file in the module system rather than via EasyBuild which would have the advantage that they maintain regular version numbers rather than version numbers that start with a dot (as it seems that that version number with the dot should then also be used consistently). Ignore EBROOT variables without matching module as we use this to implement Bundles that are detected by certain EasyBlocks as if each package included in the Bundle was installed as a separate package. The following settings are made through environment variables: Software and module install paths, according to the directory scheme given in the module system section. The directory where sources will be stored, as indicated in the directory structure overview. The repo directories where EasyBuild stores EasyConfig files for the modules that are build, as indicated in the directory structure overview. EasyBuild robot paths: we use EASYBUILD_ROBOT_PATHS and not EASYBUILD_ROBOT so searching the robot path is not enabled by default but can be controlled through the -r flag of the eb command. The search order is: The repository for the currently active partition build by EasyBuild for installed packages ( ebrepo_files ) The repository for the common partition build by EasyBuild for installed packages ( ebrepo_files ) The LUMI-specific EasyConfig directory. We deliberately put the ebrepo_files repositories first as this ensure that EasyBuild will always find the EasyConfig file for the installed module first as changes may have been made to the EasyConfig in the LUMI EasyConfig repository that are not yet reflected in the installed software. The default EasyConfig files that come with EasyBuild are not put in the robot search path for two reasons: They are not made for the Cray toolchains anyway (though one could of course use --try-toolchain etc.) We want to ensure that our EasyConfig repository is complete so that we can impose our own standards on, e.g., adding information to the help block or whatis lines in modules, and do not accidentally install dependencies without realising this. Names and locations of the EasyBuild configuration files and of the external modules definition file. Settings for the module naming scheme: As we need to point to a custom implementation of the module naming schemes, this is done through an environment variable. For consistency we also set the module naming scheme itself via a variable and set EASYBUILD_SUFFIX_MODULES_PATH as that together with the module naming scheme determines the location of the modules with respect to the module install path. Custom EasyBlocks Search path for EasyConfig files with eb -S and eb --search Every directory of the robot search path is automatically included and does not need to be added to EASYBUILD_SEARCH_PATHS The LUMI-EasyBuild-contrib repository , as that one is not in the robot path in non-user mode. Not yet done, but we could maintain a local copy of the CSCS repository and enable search in that also. Default EasyConfig files that come with EasyBuild (if we can find EasyBuild, which is if an EasyBuild-build EasyBuild module is loaded) Deliberately not included: Our ebrepo_files repositories. Everything in there should be in our own EasyConfig repository if the installations are managed properly. We also set containerpath and packagepath even though we don't plan to use those, but it ensures that files produced by this option will not end up in our GitHub repository.","title":"The EasyBuild-production and EasyBuild-infrastructure mode"},{"location":"easybuild_setup/#the-easybuild-user-mode","text":"The root of the user EasyBuild directory structure is pointed to by the environment variable EBU_USER_PREFIX . The default value if the variable is not defined is $HOME/EasyBuild . Note that this environment variable is also used in the LUMI/yy.mm modules as these modules try to include the user modules in the MODULEPATH. The directory structure in that directory largely reflects the system directory structure. This may be a bit more complicated than really needed for the user who does an occasional install, but is great for user communities who install a more elaborate software stack in their project directory. Changes: SystemRepo is named UserRepo instead and that name is fixed, contrary to the SytemRepo name. We do keep it as a separate level so that the user can also easily do version tracking via a versioning system such as GitHub. The mgmt level is missing as we do not take into account subdirectories that might be related to other software management tools. As there are only modules generated by EasyBuild in this module tree, modules/easybuild simply becomes modules . Similarly, the EB level in the directory for installed software is omitted. The robot search path: The user repository for the currently active partition The user repository for the common partition (if different from the previous one) The system repository for the currently active partition The system repository for the common partition (if different from the previous one) The user EasyConfig directory UserRepo (even if it is not there yet) The LUMI-EasyBuild-contrib repository, if present in the user directory use that one and otherwise use the one from the central installation. The LUMI-specific EasyConfig directory from the application directory The search path for EasyConfig files with eb -S and eb --search The directories above in the robot search path are automatically also used for search. Not yet done, but we could maintain a local copy of the CSCS repository and enable search in that also. Default EasyConfig files that come with EasyBuild are deliberately not included in user mode as it was decided this is confusing for the users. Deliberately not included: Our ebrepo_files repositories. Everything in there should be in our own EasyConfig repository if the installations are managed properly. So currently the additional search paths in user mode are empty. There are two regular configuration files: The system easybuild-production.cfg is always read. In the current implementation it is assumed to be present. The user easybuild-user.cfg (in UserRepo/easybuild/config in the user direcgtory) is read next and meant for user-specific settings that should be read for all LUMI software stacks. Then the system easybuild-production-LUMI-yy.mm.cfg is read after, hence can be used to overwrite settings made in production.cfg for a specific toolchain. This allows us to evolve the configuration files while keeping the possibility to install in older versions of the LUMI software stack. This will overwrite generic user options! Finally the user easybuild-user-LUMI-yy.mm.cfg is read for user customizations to a specific toolchain. Only the first of those 4 files has to be present. Presence of the others is detected when the module is loaded. Reload the module after creating one of these files to start using it. Comparison: robot path non-user robot path user / ebrepo user active partition / ebrepo user common partition ebrepo system active partition ebrepo system active partition ebrepo system common partition ebrepo system common partition EBU_USER_PREFIX/UserRepo LUMI-EasyBuild-contrib user or system LUMI-SoftwareStack system LUMI-SoftwareStack system search path non-user search path user LUMI-EasyBuild-contrib system / Default easybuild EasyConfigs /","title":"The EasyBuild-user mode"},{"location":"failed_experiments/","text":"Some failed experiments \u00b6 These failed experiments are documented to avoid making the same mistake twice. Overwriting a module by putting one with the same name and version earlier in the path may not work \u00b6 During the development of the LUMI software stack prototype using the 21.04 CPE release on eiger it turned out that the cpe/21.04 module had one nasty habbit: By setting LMOD_MODULERCFILE with the setenv LMOD function rather than by prepending or appending to it using prependpath / append_path , it cleared the modulerc.lua file that the LUMI software setup relies upon. We tried to cure that by putting a modified copy of the file in a directory earlier in the MODULEPATH and hiding the original Cray module with a hide_modulefile ( '/opt/cray/pe/lmod/modulefiles/core/cpe/21.04.lua' ) line in the system modulerc.lua file. Though the module was indeed hidden as module avail confirmed, and though there was a cpe/21.04.lua file earlier in the MODULEPATH, module load cpe/21.04 still loaded the wrong version, even with LMOD_IGNORE_CACHE=1 . A clue to what is happening is that when the line that hides the module in modulerc.lua is removed, you will notice that LMOD marks the one in the Cray core/cpe subdirectory as the default and not the one with the same name earlier in the MODULEPATH. The problem is that When a full name is used, LMOD can still load a hidden module and does so when that module is marked as the default. The CPE does mark one of the cpe modules as the default using a .version file in the core/cpe subdirectory. Luckily HPE-Cray uses a .version file which has the lowest priority. The solution is to mark a module in the overwrite directory as the default using either the symlink method or a .modulerc.lua file in that directory. A change of visibility of modules is only detected when the module command exits \u00b6 The idea was to write a module cpe/restore-defaults to restore the defaults rather than calling a script. This didn't entirely work as expected, likely because LMOD only assesses visibility at the start of the execution. Whereas changing MODULEPATH triggers LMOD to reassess the module tree, making changes to LMOD_MODULERCFILE doesn't trigger reassessing the visibility. To recreate the experiment, take any standard cpe modulefile that doesn't only use those modules with the highest version number, e.g., one that uses an older version of gcc, ( and with the setenv replaced with append_path or prepend_path see the issues with the CPE), and the following cpe/restore-defaults.lua file: modules = { \"PrgEnv-aocc\" , \"PrgEnv-cray\" , \"PrgEnv-gnu\" , \"PrgEnv-intel\" , \"PrgEnv-nvidia\" , \"aocc\" , \"atp\" , \"cce\" , \"cray-R\" , \"cray-ccdb\" , \"cray-cti\" , \"cray-dsmml\" , \"cray-fftw\" , \"cray-hdf5\" , \"cray-hdf5-parallel\" , \"cray-jemalloc\" , \"cray-libsci\" , \"cray-mpich\" , \"cray-netcdf\" , \"cray-netcdf-hdf5parallel\" , \"cray-openshmemx\" , \"cray-parallel-netcdf\" , \"cray-pmi\" , \"cray-pmi-lib\" , \"cray-python\" , \"cray-stat\" , \"craype\" , \"craype-dl-plugin-py3\" , \"craypkg-gen\" , \"gcc\" , \"gdb4hpc\" , \"iobuf\" , \"modules\" , \"nvidia\" , \"papi\" , \"perftools-base\" , } if ( mode () == \"load\" or mode () == \"show\" ) then for _ , mod in pairs ( modules ) do if ( isloaded ( mod )) then unload ( mod ) load ( mod ) end end end Then observe that module load cpe/21.04 module load cpe/restore-defaults doesn't produce the same state as module load cpe/21.04 module unload cpe/21.04 module load cpe/restore-defaults even though loading cpe/restore-defaults triggers the unloading of cpe/21.04 . In the former, lower versions of modules will remain loaded. However, loading cpe/restore-defaults a second time to correct the situation, i.e., module load cpe/21.04 module load cpe/restore-defaults module load cpe/restore-defaults produces the expected result.","title":"Failed experiments"},{"location":"failed_experiments/#some-failed-experiments","text":"These failed experiments are documented to avoid making the same mistake twice.","title":"Some failed experiments"},{"location":"failed_experiments/#overwriting-a-module-by-putting-one-with-the-same-name-and-version-earlier-in-the-path-may-not-work","text":"During the development of the LUMI software stack prototype using the 21.04 CPE release on eiger it turned out that the cpe/21.04 module had one nasty habbit: By setting LMOD_MODULERCFILE with the setenv LMOD function rather than by prepending or appending to it using prependpath / append_path , it cleared the modulerc.lua file that the LUMI software setup relies upon. We tried to cure that by putting a modified copy of the file in a directory earlier in the MODULEPATH and hiding the original Cray module with a hide_modulefile ( '/opt/cray/pe/lmod/modulefiles/core/cpe/21.04.lua' ) line in the system modulerc.lua file. Though the module was indeed hidden as module avail confirmed, and though there was a cpe/21.04.lua file earlier in the MODULEPATH, module load cpe/21.04 still loaded the wrong version, even with LMOD_IGNORE_CACHE=1 . A clue to what is happening is that when the line that hides the module in modulerc.lua is removed, you will notice that LMOD marks the one in the Cray core/cpe subdirectory as the default and not the one with the same name earlier in the MODULEPATH. The problem is that When a full name is used, LMOD can still load a hidden module and does so when that module is marked as the default. The CPE does mark one of the cpe modules as the default using a .version file in the core/cpe subdirectory. Luckily HPE-Cray uses a .version file which has the lowest priority. The solution is to mark a module in the overwrite directory as the default using either the symlink method or a .modulerc.lua file in that directory.","title":"Overwriting a module by putting one with the same name and version earlier in the path may not work"},{"location":"failed_experiments/#a-change-of-visibility-of-modules-is-only-detected-when-the-module-command-exits","text":"The idea was to write a module cpe/restore-defaults to restore the defaults rather than calling a script. This didn't entirely work as expected, likely because LMOD only assesses visibility at the start of the execution. Whereas changing MODULEPATH triggers LMOD to reassess the module tree, making changes to LMOD_MODULERCFILE doesn't trigger reassessing the visibility. To recreate the experiment, take any standard cpe modulefile that doesn't only use those modules with the highest version number, e.g., one that uses an older version of gcc, ( and with the setenv replaced with append_path or prepend_path see the issues with the CPE), and the following cpe/restore-defaults.lua file: modules = { \"PrgEnv-aocc\" , \"PrgEnv-cray\" , \"PrgEnv-gnu\" , \"PrgEnv-intel\" , \"PrgEnv-nvidia\" , \"aocc\" , \"atp\" , \"cce\" , \"cray-R\" , \"cray-ccdb\" , \"cray-cti\" , \"cray-dsmml\" , \"cray-fftw\" , \"cray-hdf5\" , \"cray-hdf5-parallel\" , \"cray-jemalloc\" , \"cray-libsci\" , \"cray-mpich\" , \"cray-netcdf\" , \"cray-netcdf-hdf5parallel\" , \"cray-openshmemx\" , \"cray-parallel-netcdf\" , \"cray-pmi\" , \"cray-pmi-lib\" , \"cray-python\" , \"cray-stat\" , \"craype\" , \"craype-dl-plugin-py3\" , \"craypkg-gen\" , \"gcc\" , \"gdb4hpc\" , \"iobuf\" , \"modules\" , \"nvidia\" , \"papi\" , \"perftools-base\" , } if ( mode () == \"load\" or mode () == \"show\" ) then for _ , mod in pairs ( modules ) do if ( isloaded ( mod )) then unload ( mod ) load ( mod ) end end end Then observe that module load cpe/21.04 module load cpe/restore-defaults doesn't produce the same state as module load cpe/21.04 module unload cpe/21.04 module load cpe/restore-defaults even though loading cpe/restore-defaults triggers the unloading of cpe/21.04 . In the former, lower versions of modules will remain loaded. However, loading cpe/restore-defaults a second time to correct the situation, i.e., module load cpe/21.04 module load cpe/restore-defaults module load cpe/restore-defaults produces the expected result.","title":"A change of visibility of modules is only detected when the module command exits"},{"location":"files_used/","text":"An overview of files in the LUMI-SoftwareStack repository and where they are being used \u00b6 As GitHub markdown has pretty poor tables we currently use a list layout. CrayPE subdirectory \u00b6 CPEpackages_yy.mm.csv files: Two-column .csv files defining the components of the yy.mm CPE release. Much but not all of that data could be automatically extracted from the matching /opt/cray/pe/cpe/yy.mm/modulerc.lua file. See the procedure to install a new version of the Cray PE on the system . LMOD subdirectory \u00b6 The LMOD subdirectory itself is set as the value of the environment variable LMOD_PACKAGE_PATH in the system setup. admin.list : Set as the value of the environment variable LMOD_ADMIN_FILE in the system setup. lmodrc.lua : Set as the value of the environment variable LMOD_RC in the system setup. LUMIstack_yy.mm_modulerc.lua files: Added to the list of modulerc files in the LMOD_MODULERCFILE environment variable by the modules/LUMIstack generic module files for the LUMI/yy.mm modules. Sets the default versions for the CPE release yy.mm in a way that is independent of everything the cpe/yy.mm modules might do (unless they really overwrite LMOD_MODULERCFILE ). LUMIstack_modulerc.lua files: Added to the list of modulerc files in the LMOD_MODULERCFILE environment variable by the modules/LUMIstack generic module files for the LUMI/yy.mm modules. Used to hide a number of modules that are of no use to ordinary users when using the LUMI software stacks, and provide some user-friendly aliases for the partition modules. SitePackage.lua : Referred to indirectly by the LMOD_PACKAGE_PATH system environment variable. Additional information on this file modules subdirectory \u00b6 CraEnv.lua : Module providing the plain nearly unmodified Cray Programming Environment. We do overwrite a number of CPE files in the modules/CrayOverwrite subdirectory to work around problems in some of the Cray module files. EasyBuild-config : A generic EasyBuild configuration module that instantiates into the EasyBuild-production , EasyBuild-infrastructure and EasyBuild-user modules in the relevant partitions of the LUMI software stacks. EasyBuild-unlock : A module that has to be loaded before any of the EasyBuild-production or EasyBuild-infrastructure modules can be loaded as additional protection to not accidentally overwrite a system installation. init-lumi : The module called from the Cray PE intialisation process (in /etc/cray-pe.d/cray-pe-configuration.sh ) to do the final steps of enabling the LUST software stacks. LUMIstack : Generic implementation(s) of the LUMI software stack modules. The actual modules can simply link to the right version of the generic module file. LUMIpartition : Generic implementation(s) of the LUMI partition modules. The actual modules can simply link to the right version of the generic module file. CrayOverwrite : A directory with modulefiles that should overwrite standard CPE module files that have behaviour that conflicts with our setup. The process of overruling a Cray module may not be easy: If one of those modules is set as default in the original CPE subdirectory, one should make sure that that is overruled in the CrayOverwrite subdirectory for that module. StyleModifiers : A set of small modules that change the presentation of the module display by module avail . These modules simply set one or more environment variables, one LUMI-specific one and the other LMOD configuration variables. These modules include: ModuleColour : Switch between colour and black-and-white display of the modules ModuleExtensions : Show or hide the display of extensions in the output of module avail . ModuleLabel : Switch between three ways of displaying the module subdirectories: label : Give the module subdirectories meaningful names and collapses all directories in the CPE module hierarchy. PEhierarcy : Give the module subdirectories meaningful names but do not collapse the directories in the CPE module hierarchy system : Show the directory names of all modules subdirectories ModulePowerUser : Enables the power user view of the module system: Less modules are hidden, but using those modules that are otherwise hidden is not supported by the LUMI User Support Team. These modules may not work under a regular user account or are their use is not documented in the regular LUMI documentation as they are only meant for support staff. ModuleStyle : Used to return to the situation at login or the default for the system. For the EasyBuild-config , LUMIstack and LUMIpartition modules we adopted a special version numbering: They are numbered in the same way as CPE releases and a particular module for any LUMI stack we use the most recent version that is not younger than the correpsonding CPE/LUMI stack. easybuild/config subdirectory \u00b6 The files are referred to by the EasyBuild-config generic module file. easybuild/easyblocks subdirectory \u00b6 This is the subdirectory for the LUMI custom EasyBlocks. We use a scheme with subdirectories as in the main EasyBlock repository. easybuild/easyconfigs subdirectory \u00b6 EasyConfig repository for LUMI. easybuild/hooks subdirectoruy \u00b6 This directory contains the custom hooks for EasyBuild on LUMI. The file names end on the first version of the software stack that they support. As such we can drop old code from hooks while still being able to regenerate old configirations. easybuild/toolchains subdirectoruy \u00b6 This directory contains the custom Cray toolchain code for EasyBuild. For LUMI: Custom toolchain definition files: cpecray.py : Toolchain based on the Cray compiler cpegnu.py : Toolchain based on the GNU compilers provided by Cray cpeamd.py : Toolchain based on the AMD AOCC compiler provided by Cray. Compiler definitions to use compilers through the Cray wrappers: compiler/cpecce.py : Definitions for the Cray CCE compiler with the Cray wrappers. compiler/cpegcc.py : Definitions for the GNU compilers with the Cray wrappers. compiler/cpeaocc.py : Definitions for the AMD AOCC compilers with the Cray wrappers. easybuild/tools subdirectory \u00b6 module_naming_scheme subdirectory: LUMI uses a customised flat EasyBuild naming scheme. The links by moduleclass are omitted as they are not used in our module system. Note that the EasyBuild-config generic module also sets the environment variable EASYBUILD_SUFFIX_MODULES_PATH to the empty string to omit the all level in the EasyBuild modules directory structure. The etc subdirectory \u00b6 This directory contains the motd.txt and [ lumi_fortune.txt``](https://github.com/Lumi-supercomputer/LUMI-SoftwareStack/tree/main/etc/lumi_fortune.txt) files that are used by the init-lumi` module to augment the message of the day and to display a random tip about LUMI. The scripts and scripts/lumitools subdirectories \u00b6 The scripts subdirectory contains a number of shell scripts to initialise a new installation, new software stack in the installation, or various substeps of this. Several of those scripts are just wrapper scripts that call a Python routine to do the work. The Setup subdirectory \u00b6 A directory where we keep our preferred setup of the system configuration files for the HPE Cray PE, useful to communicate with the sysadmins who maintain those files. The tools subdirectory \u00b6 This directory contains scripts that are useful to any user of EasyBuild of LUMI, e.g., to bump EasyConfig files to a new release of the Cray PE. upgrade-tc.py : Bump the version of the Cray PE in an EasyConfig file, also adapting the name of the file. Note that the regular dependencies are not updated to a new version. The Testing subdirectory \u00b6 This directory contains various files to be used in component tests. install_lmod_newest.sh is a script to install a very recent version of LMOD to test compatibility of the module system agains more recent versions than the one that comes with a HPE-Cray system. Note: Files and directories referred to from outside the LUMI software structure \u00b6 LMOD subdirectory itself as the value of the environment variable LMOD_PACKAGE_PATH LMOD/admin.list as the value of the environment variable LMOD_ADMIN_FILE LMOD/lmodrc.lua as the value of the environment variable LMOD_RC Not a reference to a file, but the system should set LMOD_AVAIL_STYLE to <label>:PEhierarchy:system .","title":"Overview of files"},{"location":"files_used/#an-overview-of-files-in-the-lumi-softwarestack-repository-and-where-they-are-being-used","text":"As GitHub markdown has pretty poor tables we currently use a list layout.","title":"An overview of files in the LUMI-SoftwareStack repository and where they are being used"},{"location":"files_used/#craype-subdirectory","text":"CPEpackages_yy.mm.csv files: Two-column .csv files defining the components of the yy.mm CPE release. Much but not all of that data could be automatically extracted from the matching /opt/cray/pe/cpe/yy.mm/modulerc.lua file. See the procedure to install a new version of the Cray PE on the system .","title":"CrayPE subdirectory"},{"location":"files_used/#lmod-subdirectory","text":"The LMOD subdirectory itself is set as the value of the environment variable LMOD_PACKAGE_PATH in the system setup. admin.list : Set as the value of the environment variable LMOD_ADMIN_FILE in the system setup. lmodrc.lua : Set as the value of the environment variable LMOD_RC in the system setup. LUMIstack_yy.mm_modulerc.lua files: Added to the list of modulerc files in the LMOD_MODULERCFILE environment variable by the modules/LUMIstack generic module files for the LUMI/yy.mm modules. Sets the default versions for the CPE release yy.mm in a way that is independent of everything the cpe/yy.mm modules might do (unless they really overwrite LMOD_MODULERCFILE ). LUMIstack_modulerc.lua files: Added to the list of modulerc files in the LMOD_MODULERCFILE environment variable by the modules/LUMIstack generic module files for the LUMI/yy.mm modules. Used to hide a number of modules that are of no use to ordinary users when using the LUMI software stacks, and provide some user-friendly aliases for the partition modules. SitePackage.lua : Referred to indirectly by the LMOD_PACKAGE_PATH system environment variable. Additional information on this file","title":"LMOD subdirectory"},{"location":"files_used/#modules-subdirectory","text":"CraEnv.lua : Module providing the plain nearly unmodified Cray Programming Environment. We do overwrite a number of CPE files in the modules/CrayOverwrite subdirectory to work around problems in some of the Cray module files. EasyBuild-config : A generic EasyBuild configuration module that instantiates into the EasyBuild-production , EasyBuild-infrastructure and EasyBuild-user modules in the relevant partitions of the LUMI software stacks. EasyBuild-unlock : A module that has to be loaded before any of the EasyBuild-production or EasyBuild-infrastructure modules can be loaded as additional protection to not accidentally overwrite a system installation. init-lumi : The module called from the Cray PE intialisation process (in /etc/cray-pe.d/cray-pe-configuration.sh ) to do the final steps of enabling the LUST software stacks. LUMIstack : Generic implementation(s) of the LUMI software stack modules. The actual modules can simply link to the right version of the generic module file. LUMIpartition : Generic implementation(s) of the LUMI partition modules. The actual modules can simply link to the right version of the generic module file. CrayOverwrite : A directory with modulefiles that should overwrite standard CPE module files that have behaviour that conflicts with our setup. The process of overruling a Cray module may not be easy: If one of those modules is set as default in the original CPE subdirectory, one should make sure that that is overruled in the CrayOverwrite subdirectory for that module. StyleModifiers : A set of small modules that change the presentation of the module display by module avail . These modules simply set one or more environment variables, one LUMI-specific one and the other LMOD configuration variables. These modules include: ModuleColour : Switch between colour and black-and-white display of the modules ModuleExtensions : Show or hide the display of extensions in the output of module avail . ModuleLabel : Switch between three ways of displaying the module subdirectories: label : Give the module subdirectories meaningful names and collapses all directories in the CPE module hierarchy. PEhierarcy : Give the module subdirectories meaningful names but do not collapse the directories in the CPE module hierarchy system : Show the directory names of all modules subdirectories ModulePowerUser : Enables the power user view of the module system: Less modules are hidden, but using those modules that are otherwise hidden is not supported by the LUMI User Support Team. These modules may not work under a regular user account or are their use is not documented in the regular LUMI documentation as they are only meant for support staff. ModuleStyle : Used to return to the situation at login or the default for the system. For the EasyBuild-config , LUMIstack and LUMIpartition modules we adopted a special version numbering: They are numbered in the same way as CPE releases and a particular module for any LUMI stack we use the most recent version that is not younger than the correpsonding CPE/LUMI stack.","title":"modules subdirectory"},{"location":"files_used/#easybuildconfig-subdirectory","text":"The files are referred to by the EasyBuild-config generic module file.","title":"easybuild/config subdirectory"},{"location":"files_used/#easybuildeasyblocks-subdirectory","text":"This is the subdirectory for the LUMI custom EasyBlocks. We use a scheme with subdirectories as in the main EasyBlock repository.","title":"easybuild/easyblocks subdirectory"},{"location":"files_used/#easybuildeasyconfigs-subdirectory","text":"EasyConfig repository for LUMI.","title":"easybuild/easyconfigs subdirectory"},{"location":"files_used/#easybuildhooks-subdirectoruy","text":"This directory contains the custom hooks for EasyBuild on LUMI. The file names end on the first version of the software stack that they support. As such we can drop old code from hooks while still being able to regenerate old configirations.","title":"easybuild/hooks subdirectoruy"},{"location":"files_used/#easybuildtoolchains-subdirectoruy","text":"This directory contains the custom Cray toolchain code for EasyBuild. For LUMI: Custom toolchain definition files: cpecray.py : Toolchain based on the Cray compiler cpegnu.py : Toolchain based on the GNU compilers provided by Cray cpeamd.py : Toolchain based on the AMD AOCC compiler provided by Cray. Compiler definitions to use compilers through the Cray wrappers: compiler/cpecce.py : Definitions for the Cray CCE compiler with the Cray wrappers. compiler/cpegcc.py : Definitions for the GNU compilers with the Cray wrappers. compiler/cpeaocc.py : Definitions for the AMD AOCC compilers with the Cray wrappers.","title":"easybuild/toolchains subdirectoruy"},{"location":"files_used/#easybuildtools-subdirectory","text":"module_naming_scheme subdirectory: LUMI uses a customised flat EasyBuild naming scheme. The links by moduleclass are omitted as they are not used in our module system. Note that the EasyBuild-config generic module also sets the environment variable EASYBUILD_SUFFIX_MODULES_PATH to the empty string to omit the all level in the EasyBuild modules directory structure.","title":"easybuild/tools subdirectory"},{"location":"files_used/#the-etc-subdirectory","text":"This directory contains the motd.txt and [ lumi_fortune.txt``](https://github.com/Lumi-supercomputer/LUMI-SoftwareStack/tree/main/etc/lumi_fortune.txt) files that are used by the init-lumi` module to augment the message of the day and to display a random tip about LUMI.","title":"The etc subdirectory"},{"location":"files_used/#the-scripts-and-scriptslumitools-subdirectories","text":"The scripts subdirectory contains a number of shell scripts to initialise a new installation, new software stack in the installation, or various substeps of this. Several of those scripts are just wrapper scripts that call a Python routine to do the work.","title":"The scripts and scripts/lumitools subdirectories"},{"location":"files_used/#the-setup-subdirectory","text":"A directory where we keep our preferred setup of the system configuration files for the HPE Cray PE, useful to communicate with the sysadmins who maintain those files.","title":"The Setup subdirectory"},{"location":"files_used/#the-tools-subdirectory","text":"This directory contains scripts that are useful to any user of EasyBuild of LUMI, e.g., to bump EasyConfig files to a new release of the Cray PE. upgrade-tc.py : Bump the version of the Cray PE in an EasyConfig file, also adapting the name of the file. Note that the regular dependencies are not updated to a new version.","title":"The tools subdirectory"},{"location":"files_used/#the-testing-subdirectory","text":"This directory contains various files to be used in component tests. install_lmod_newest.sh is a script to install a very recent version of LMOD to test compatibility of the module system agains more recent versions than the one that comes with a HPE-Cray system.","title":"The Testing subdirectory"},{"location":"files_used/#note-files-and-directories-referred-to-from-outside-the-lumi-software-structure","text":"LMOD subdirectory itself as the value of the environment variable LMOD_PACKAGE_PATH LMOD/admin.list as the value of the environment variable LMOD_ADMIN_FILE LMOD/lmodrc.lua as the value of the environment variable LMOD_RC Not a reference to a file, but the system should set LMOD_AVAIL_STYLE to <label>:PEhierarchy:system .","title":"Note: Files and directories referred to from outside the LUMI software structure"},{"location":"module_setup/","text":"Setup of the LUMI module system \u00b6 Features of the module system on LUMI \u00b6 The module system is organised in software stacks enabled through loading a meta-module. CrayEnv will enable the native Cray environment for developers who prefer to do everything themselves, while the LUMI/yy.mm modules enable the LUMI software stacks, based on the corresponding yy.mm version of the Cray programming environment. To distinguish between versions of the software stack that are fairly complete and that we try to support for a longer period and those versions that are only meant to install a few applications that run into compiler bugs on the main versions, the latter versions of the software stack have a name of the form LUMI/yy.mm.dev . It is possible to at some point having both a LUMI/yy.mm.dev and LUMI/yy.mm software stack for the same release of the Cray PE so that if we figure out that a particular Cray PE release is good enough to build a new version of the software stack, we can do so without interupting users using the development version while still being able to update some components that are already installed in the .dev stack. However, they have to use the same components of the Cray PE and same cpeCray/cpeGNU/cpeAMD EasyBuild toolchain modules. When it matters, the module system will automatically select the software for the partition on which the software stack was loaded (and change to the correct partition in Slurm batch scripts if you reload the LUMI module). It is always possible to overwrite by loading a partition module after (re)loading the software stack meta-module. Moreover, we ensure as much as possible that after loading a different partition, already loaded modules are reloaded in as reasonable versions as possible. NOTE: LMOD 8.3.x, the default on Cray with the 21.06 software release: module update does not work correctly and the way to trigger a reload of the software stack in case of a possible partition change is module load LUMI/ $LUMI_STACK_VERSION In LMOD 8.4.x and 8.5.x, it is possible to instead use module update , and there seems to be an undocumented module reload that has exactly the same effect. User-friendly presentation with clear labels is the default, and there is a user-friendly way to change the presentation through modules rather than by setting LMOD environment variables by hand. We also do an effort to hide modules that a regular user does not need at some point, e.g., EasyBuild configuration modules to install in the system directories, or those Cray PE modules that are not relevant for a particular version of the LUMI software stack. Discoverability of software is a key feature. The whole module system is designed to make it easy to find installed software through module spider and module keyword even if that software may not be visible at some point through module avail . The modules that load the software stack and partition, and the modules that influence the presentation of the modules are sticky so that module purge will not remove them. Hence a user who wants to continue working in the same software stack can use module purge to unload all software stack-specific modules without having to start from scratch. Assumptions in the implementation \u00b6 The SoftwareStack module relies on the detect_LUMI_partition function in SitePackage.lua to determine on which partition it is running. The current implementation first checks if the environment variable LUMI_OVERWRITE_PARTITION is defined and if so, the value of that variable is used. It is assumed to be C, G, D or L depending on the node type (CPU compute, GPU compute, data and visualisation or login), or can be common to install software in the hidden common partition. If that environment variable is not defined, we currently emply the following algorithm as a demo of what can be done on the final LUMI installation: On eiger uan01 and uan02 the partition is set to L On eiger uan03 the partition is set to common On all other hosts we first check for the environment variable LUMI_PARTITION and use that one and otherwise we set the partition to L. This is used by the SoftwareStack module to then auto-load the module for the current partition when it is loaded. That function can be implemented differently also so that the environment variable is no longer needed. The current implementation assumes that LMOD_PACKAGE_PATH refers to the LMOD directory in the repository as that is used to determine the name of the repository (rather than use a hardcoded name, e./g/. SystemRepo ). Default behaviour when loading modules \u00b6 A user will always need to load a software stack module first. For those software stack modules that further split up according to partition (the LUMI/yy.mm modules), the relevant partition module will be loaded automatically when loading the software stack. This is based on the partition detected by the detect_LUMI_partition function defined in SitePackage.lua. We made the SoftwareStack and LUMIpartition modules sticky so that once loaded a user can use module purge if they want to continue working in the same software stack but load other packages. Module presentation \u00b6 The LUMI module system supports three ways of presenting the module to the users The default way users user-friendly labels rather than subdirectories to present the modules. Sometimes several subdirectories may map to the same label when our feeling is that those modules belong together for the regular user. There are two variants of this view: a. All Cray PE module directories collapsed into a single category. This style is called label . b. User-friendly names for the Cray PE module directories, but showing the full hierarchy. This is called the PEhierarchy style. But of course the view with subdirectories is also supported for the power user. This is called the system style. The implementation of the labeled view is done in the avail_hook hook in the SitePackage.lua file and is derived from an example in the Lmod manual. To not confront the user directly with all Lmod environment variables that influence the presentation of the modules, we also have a set of modules that can be used to change the presentation, including Switching between the label, PEhierarchy and system (subdirectory) view Showing extensions in module avail Turn on and off colour in the presentation of the modules as the colour selection can never be good for both white background and black background (though of course a power user can change the colour definitions in their terminal emulation software to ensure that both bright and dark colours are displayed properly). The presentation modules are sticky so that module purge doesn't change the presentation of the modules. There is a \"power user\" view that will reveal a number of modules that are otherwise hidden. This is at their own risk. Use of this module is not documented on the regular LUMI-documentation. There are also two ModuleStyle modules that try to reset the presentation to the system defaults or whatever the user may have set through environment variables. The common software subdirectories \u00b6 We have a set of subdirectories for each of the 4 LUMI partitions on which the modules are available. However, we also have a directory to install software that is common to all partitions and doesn't need specific processor optimisations hence is simply compiled for the minimal configuration that works everywhere, Rome without accelerator. We use a hidden partition module ( partition/common ) which can then be recognized by the EasyBuild configuration modules to install software in that partition. Hiding is done via the LMOD/LUMIstack_modulerc.lua file. The path to the common software modules is added in the other partition module files, always in such a way that the partition-specific directory comes before the equivalent common directory to ensure that partition-specific software has a higher priority. Yet because of the way that LMOD works, it is still better to either install a certain package only in the common partition or only in the other partitions as the order in the MODULEPATH is not always respected if a particular version is hard-marked as the default. Moreover, the labeling in SitePackage.lua is such that in the labeled view one sees the common and partition-specific software is put together. This is another reason to not have a package in both the common subdirectory and in one or more of the regular partition subdirectories as it would appear twice in the list for that partition. A hidden partition module did show up into the output of module spider as a way to reach certain other modules. This was the case even when it was hidden by using a name starting with a dot rather than a modulerc.lua file. This \"feature\" is also mentioned in Lmod issue #289 . Our solution was to modify the module file itself to not include the paths when the mode is \"spider\" so that Lmod cannot see that it is a partition module that makes other modules available. We currently do not show this for power users either which may be the best choice to avoid conflicts with caches. We needed an additional module tree, Infrastructure , which works a pure hierarchy (i.e., the common subdirectory is not loaded in the MODULEPATH of any other partition) to ensure that module update and changing partitions works well for modules that have to be present in all four regular partitions and the common meta-partition, e.g., cpe* modules needed by EasyBuild and EasyBuild configuration modules. We also use a visibility hook for LMOD to hide modules from the Cray PE that are not relevant for a particular version of the LUMI software stack. The data used by that hook is stored in the mgmt/LMOD/VisibilityHookData directory in a format that favours speed over readability and is auto-generated by a script based on the description of a version of the CPE. The CrayEnv partition \u00b6 The CrayEnv partition is a hidden partition used to trigger cross-installation of software in the CrayEnv stack. This is done via a separate hidden partition as we need a way to avoid loading modules that will not be visible in that stack. This partition had only modules in the corresponding Infrastructure tree in the module system. To make EasyBuild available, we symlink in that directory to the EasyBuild subdirectory in the common partition of the same LUMI/yy.mm software stack. The system partition \u00b6 The system partitions functions nearly identical to the CrayEnv partition but its function is to install software whose modules will be available system-wide, independent of any toolchain, and can be loaded as soon as the init-lumi module is loaded (which itself is loaded at login). Where do we set the default modules? \u00b6 Style modifiers: LMOD/modulerc.lua (central moduler.lua file) Software stack: Currently by a hidden .modulerc.lua file in the SoftwareStacks subdirectory since we use different defaults on different test systems. Partition: No assigned default, the software stack module determines the optimal partition based on the node where that module is loaded. Cray PE compoments: Depending on the version of the LUMI stack loaded, different versions of the modules in the Cray PE will be the default ones, corresponding to the release of the PE used for that stack. These defaults are set in LMOD/LUMIstack_<version>_modulerc.lua , which can be generated with the make_CPE_modulerc.sh script. Note that loading a cpe/yy.mm module may overwrite this depending on the implementation of that module. Helper functions in SitePackage.lua \u00b6 See the separate SitePackage information file .","title":"Setup of the LUMI module system"},{"location":"module_setup/#setup-of-the-lumi-module-system","text":"","title":"Setup of the LUMI module system"},{"location":"module_setup/#features-of-the-module-system-on-lumi","text":"The module system is organised in software stacks enabled through loading a meta-module. CrayEnv will enable the native Cray environment for developers who prefer to do everything themselves, while the LUMI/yy.mm modules enable the LUMI software stacks, based on the corresponding yy.mm version of the Cray programming environment. To distinguish between versions of the software stack that are fairly complete and that we try to support for a longer period and those versions that are only meant to install a few applications that run into compiler bugs on the main versions, the latter versions of the software stack have a name of the form LUMI/yy.mm.dev . It is possible to at some point having both a LUMI/yy.mm.dev and LUMI/yy.mm software stack for the same release of the Cray PE so that if we figure out that a particular Cray PE release is good enough to build a new version of the software stack, we can do so without interupting users using the development version while still being able to update some components that are already installed in the .dev stack. However, they have to use the same components of the Cray PE and same cpeCray/cpeGNU/cpeAMD EasyBuild toolchain modules. When it matters, the module system will automatically select the software for the partition on which the software stack was loaded (and change to the correct partition in Slurm batch scripts if you reload the LUMI module). It is always possible to overwrite by loading a partition module after (re)loading the software stack meta-module. Moreover, we ensure as much as possible that after loading a different partition, already loaded modules are reloaded in as reasonable versions as possible. NOTE: LMOD 8.3.x, the default on Cray with the 21.06 software release: module update does not work correctly and the way to trigger a reload of the software stack in case of a possible partition change is module load LUMI/ $LUMI_STACK_VERSION In LMOD 8.4.x and 8.5.x, it is possible to instead use module update , and there seems to be an undocumented module reload that has exactly the same effect. User-friendly presentation with clear labels is the default, and there is a user-friendly way to change the presentation through modules rather than by setting LMOD environment variables by hand. We also do an effort to hide modules that a regular user does not need at some point, e.g., EasyBuild configuration modules to install in the system directories, or those Cray PE modules that are not relevant for a particular version of the LUMI software stack. Discoverability of software is a key feature. The whole module system is designed to make it easy to find installed software through module spider and module keyword even if that software may not be visible at some point through module avail . The modules that load the software stack and partition, and the modules that influence the presentation of the modules are sticky so that module purge will not remove them. Hence a user who wants to continue working in the same software stack can use module purge to unload all software stack-specific modules without having to start from scratch.","title":"Features of the module system on LUMI"},{"location":"module_setup/#assumptions-in-the-implementation","text":"The SoftwareStack module relies on the detect_LUMI_partition function in SitePackage.lua to determine on which partition it is running. The current implementation first checks if the environment variable LUMI_OVERWRITE_PARTITION is defined and if so, the value of that variable is used. It is assumed to be C, G, D or L depending on the node type (CPU compute, GPU compute, data and visualisation or login), or can be common to install software in the hidden common partition. If that environment variable is not defined, we currently emply the following algorithm as a demo of what can be done on the final LUMI installation: On eiger uan01 and uan02 the partition is set to L On eiger uan03 the partition is set to common On all other hosts we first check for the environment variable LUMI_PARTITION and use that one and otherwise we set the partition to L. This is used by the SoftwareStack module to then auto-load the module for the current partition when it is loaded. That function can be implemented differently also so that the environment variable is no longer needed. The current implementation assumes that LMOD_PACKAGE_PATH refers to the LMOD directory in the repository as that is used to determine the name of the repository (rather than use a hardcoded name, e./g/. SystemRepo ).","title":"Assumptions in the implementation"},{"location":"module_setup/#default-behaviour-when-loading-modules","text":"A user will always need to load a software stack module first. For those software stack modules that further split up according to partition (the LUMI/yy.mm modules), the relevant partition module will be loaded automatically when loading the software stack. This is based on the partition detected by the detect_LUMI_partition function defined in SitePackage.lua. We made the SoftwareStack and LUMIpartition modules sticky so that once loaded a user can use module purge if they want to continue working in the same software stack but load other packages.","title":"Default behaviour when loading modules"},{"location":"module_setup/#module-presentation","text":"The LUMI module system supports three ways of presenting the module to the users The default way users user-friendly labels rather than subdirectories to present the modules. Sometimes several subdirectories may map to the same label when our feeling is that those modules belong together for the regular user. There are two variants of this view: a. All Cray PE module directories collapsed into a single category. This style is called label . b. User-friendly names for the Cray PE module directories, but showing the full hierarchy. This is called the PEhierarchy style. But of course the view with subdirectories is also supported for the power user. This is called the system style. The implementation of the labeled view is done in the avail_hook hook in the SitePackage.lua file and is derived from an example in the Lmod manual. To not confront the user directly with all Lmod environment variables that influence the presentation of the modules, we also have a set of modules that can be used to change the presentation, including Switching between the label, PEhierarchy and system (subdirectory) view Showing extensions in module avail Turn on and off colour in the presentation of the modules as the colour selection can never be good for both white background and black background (though of course a power user can change the colour definitions in their terminal emulation software to ensure that both bright and dark colours are displayed properly). The presentation modules are sticky so that module purge doesn't change the presentation of the modules. There is a \"power user\" view that will reveal a number of modules that are otherwise hidden. This is at their own risk. Use of this module is not documented on the regular LUMI-documentation. There are also two ModuleStyle modules that try to reset the presentation to the system defaults or whatever the user may have set through environment variables.","title":"Module presentation"},{"location":"module_setup/#the-common-software-subdirectories","text":"We have a set of subdirectories for each of the 4 LUMI partitions on which the modules are available. However, we also have a directory to install software that is common to all partitions and doesn't need specific processor optimisations hence is simply compiled for the minimal configuration that works everywhere, Rome without accelerator. We use a hidden partition module ( partition/common ) which can then be recognized by the EasyBuild configuration modules to install software in that partition. Hiding is done via the LMOD/LUMIstack_modulerc.lua file. The path to the common software modules is added in the other partition module files, always in such a way that the partition-specific directory comes before the equivalent common directory to ensure that partition-specific software has a higher priority. Yet because of the way that LMOD works, it is still better to either install a certain package only in the common partition or only in the other partitions as the order in the MODULEPATH is not always respected if a particular version is hard-marked as the default. Moreover, the labeling in SitePackage.lua is such that in the labeled view one sees the common and partition-specific software is put together. This is another reason to not have a package in both the common subdirectory and in one or more of the regular partition subdirectories as it would appear twice in the list for that partition. A hidden partition module did show up into the output of module spider as a way to reach certain other modules. This was the case even when it was hidden by using a name starting with a dot rather than a modulerc.lua file. This \"feature\" is also mentioned in Lmod issue #289 . Our solution was to modify the module file itself to not include the paths when the mode is \"spider\" so that Lmod cannot see that it is a partition module that makes other modules available. We currently do not show this for power users either which may be the best choice to avoid conflicts with caches. We needed an additional module tree, Infrastructure , which works a pure hierarchy (i.e., the common subdirectory is not loaded in the MODULEPATH of any other partition) to ensure that module update and changing partitions works well for modules that have to be present in all four regular partitions and the common meta-partition, e.g., cpe* modules needed by EasyBuild and EasyBuild configuration modules. We also use a visibility hook for LMOD to hide modules from the Cray PE that are not relevant for a particular version of the LUMI software stack. The data used by that hook is stored in the mgmt/LMOD/VisibilityHookData directory in a format that favours speed over readability and is auto-generated by a script based on the description of a version of the CPE.","title":"The common software subdirectories"},{"location":"module_setup/#the-crayenv-partition","text":"The CrayEnv partition is a hidden partition used to trigger cross-installation of software in the CrayEnv stack. This is done via a separate hidden partition as we need a way to avoid loading modules that will not be visible in that stack. This partition had only modules in the corresponding Infrastructure tree in the module system. To make EasyBuild available, we symlink in that directory to the EasyBuild subdirectory in the common partition of the same LUMI/yy.mm software stack.","title":"The CrayEnv partition"},{"location":"module_setup/#the-system-partition","text":"The system partitions functions nearly identical to the CrayEnv partition but its function is to install software whose modules will be available system-wide, independent of any toolchain, and can be loaded as soon as the init-lumi module is loaded (which itself is loaded at login).","title":"The system partition"},{"location":"module_setup/#where-do-we-set-the-default-modules","text":"Style modifiers: LMOD/modulerc.lua (central moduler.lua file) Software stack: Currently by a hidden .modulerc.lua file in the SoftwareStacks subdirectory since we use different defaults on different test systems. Partition: No assigned default, the software stack module determines the optimal partition based on the node where that module is loaded. Cray PE compoments: Depending on the version of the LUMI stack loaded, different versions of the modules in the Cray PE will be the default ones, corresponding to the release of the PE used for that stack. These defaults are set in LMOD/LUMIstack_<version>_modulerc.lua , which can be generated with the make_CPE_modulerc.sh script. Note that loading a cpe/yy.mm module may overwrite this depending on the implementation of that module.","title":"Where do we set the default modules?"},{"location":"module_setup/#helper-functions-in-sitepackagelua","text":"See the separate SitePackage information file .","title":"Helper functions in SitePackage.lua"},{"location":"procedures/","text":"Some procedures \u00b6 Installing a new version of the Cray PE on the system \u00b6 Part 1: Making available in CrayEnv \u00b6 These instructions are enough to make the new CPE available in the CrayEnv stack but do not yet include instructions for EasyBuild Install the CPE components using the instructions of HPE Create the components file in the CrayPE subdirectory of the repository. This is currently a simple .csv-file but may be replaced by a different one if HPE would provide that information in machine-readable format in a future version of the CPE. Much of that information can be extracted from the HPE-Cray provided file /opt/cray/pe/cpe/yy.mm/modulerc.lua , though we do currently keep some additional packages in that file that don't have their defaults set in that file. One of them which is actually used in the LUMIpartition module file is the version of the craype-targets packages (essentially the CPE targeting modules). This step is only needed if you don't do part 2 as the installation script used in part 2 of this procedure will also do this. Add a cpe/yy.mm.lua link for the just installed programming environment to the modules/CrayOverwrite/core subdirectory of the modules tree (NOT in the modules directory of the repository). This file does need the .csv file generated in the previous step to function properly. Note that this directory also contains a link named .modulerc to the .version file of `/opt/cray/pe/lmod/modulefiles/core/cpe''. It is essential that the default version of the cpe module is the same in the original module directory and in CrayOVerwrite as otherwise the original one may get loaded in some circumstances. By using the symbolic link and the new name with higher priority we assure that the modules from CrayOVerwrite are used. One still needs to ensure that the overwrite modules also are more to the front of the MODULEPATH or things still go wrong. This step is only needed if you don't do part 2 as the installation script used in part 2 of this procedure will also do this. If /opt/cray/pe/cpe/yy.mm/modulerc.lua is missing on the system, an alternative needs to be generated and stored in modules/CrayOverwrite/data-cpe/yy.mm as it is used by the generic cpe module installed in the previous step. This can be done by running ./make_CPE_modulerc yy.mm in the scripts subdirectory of SystemRepo . Hide the new cpe module installed in the Cray PE subdirectory by adding a line to LMOD/modulerc.lua in the repository. If there are new Cray targeting modules that are irrelevant for LUMI you may want to hide them in the respective code block in LMOD/LUMIstack_modulerc.lua . Part 2: Adding support as a LUMI/yy.mm(.dev) software stack \u00b6 From here on the setup is largely automated by the prepare_LUMI_stack.sh script in the scripts subdirectory of the repository. A few things need to be in place though before running this script: The component definition of the Cray PE software stack in the .csv file in the CrayPE subdirectory. An EasyConfig file for the right version of EasyBuild in the easybuild/easyconfigs/e/EasyBuild subdirectory of the repository. Note that it is always possible to run prepare_LUMI_stack.sh -script with EASYBUILD_IGNORE_CHECKSUMS=1 set if the checksums in the module file are not yet OK. Add a software stack-specific configuration file easybuild-production-LUMI-yy.mm.cfg for EasyBuild if needed in the easybuild/config subdirectory of the repository. We currently don't include the .dev extension, i.e., if there is a development and an LTS software stack for the same version of the CPE, they share the EasyBuild configuration file. This makes sense because the development stack is meant to prepare for a production stack. Furthermore you may want to make sure that the proper versions of the following files are available in the repository should you want to make changes compared to versions installed before: The generic EasyBuild-config module should you want to make changes to, e.g., the environment variables et by that module. The generic LUMIstack and `LUMIpartition module. The generic cpe module cpe-generic . The software stack initialisation script will take the most recent one (based on the yy.mm version number) that is not newer than the release of the CPE for the software stack. The prepare_LUMI_stack.sh script takes three arguments, and the order is important: The version of the software stack: the Cray PE version, with the extension .dev for a development stack (e.g., 21.05.dev or 21.06). The version of EasyBuild to install in the software stack A work directory for temporary files, used to install a bootstrapping copy of EasyBuild. Rather than trying to use an EasyBuild installation from an older software stack if present to bootstrap the new one, we simply chose to do a bootstrap every time the script is run as this is a procedure that simply always works, though it is more time consuming. The advantage however is that one can just clone the production repository anywhere, run an initialisation script to initialise the structure around the repository, then initialise a software stack and start working. The script then does the following steps: Generate our own cpe/yy.mm module in modules/CrayOverwrite/core by creating a symbolic link to the right version of the generic modules in SystemRepo . Check if the Cray PE comes with its own modulerc.lua file with the default components (if so, that file can be found in /opt/cray/pe/cpe/yy.mm ). If not an alternative file is generated from the data in the compoments .csv file and stored in modules/CrayOverwrite/data-cpe/yy.mm . The file is used by the generic cpe/yy.mm module in modules/CrayOVerwrite/core . Create the software stack module files by symlinking to the generic implementations in SystemRepo . Create the partition modules (second level in the hierarchy) by symlinking to the generic implementations in SystemRepo . Create the LUMIstack_yy.mm_modulerc.lua file in mgmt/LMOD/ModuleRC . Currently this file only contain references to Cray PE modules and as such correspond to the modulerc.lua file in /opt/cray/pe/cpe/yy.mm but his may change in a future version. Whereas the aforementioned modulerc.lua files are meant to be activated by the cpe.yy.mm modules and hence to be used by any software stack that uses the Cray PE, the LUMIstack_yy.mm_modulerc.lua are really meant exclusively for the LUMI software stack. This is done by running the make_LUMIstack_modulerc.sh script. Creates the CPE_modules_*.lua file in mgmt/LMOD/VisibilityHookData for module avail visibility hook in SitePackage.lua . This is done by running the make_CPE_VisibilityHookData.sh script. Creates the full directory structure for the software stack for the modules, the binary installations and the EasyBuild repo. Creates the EasyBuild external modules definition file from the data in the corresponding CPEpakcages_<CPE version>.csv file (if the file does not yet exist). This file is stored in the repo as it may be useful for other people who check out the repository to know what is going on. Creates the EasyBuild-production, EasyBuild-infrastructure and EasyBuild-user modules for each partition by softlinking to the matching generic file in the SystemRepo . Downloads the selected version of EasyBuild to the EasyBuild sources directory if the files are not yet present. Installs a bootstrapping version of EasyBuild in the work directory. As that version will only be used to install EasyBuild from our own EasyConfig file, there is no need to also install the EasyConfig files. Loads the software stack module for the common partition and the EasyBuild-production module to install EasyBuild in the software stack. Create EasyConfig files in the repository for the cpeCray , cpeGNU and cpeAMD toolchains (if those files do not yet exist) and use the just installed EasyBuild to install them in the Infrastructure module subdirectory for all 4 regular and the common partition. The toolchain definition EasyConfig files are generated by the make_CPE_EBfile.sh script. Things to do afterwards: If you want to change the default version of the LUMI software stack module, you can do this by editing .modulerc.lua in modules/SoftwareStack/LUMI . And now one should be ready to install other software... Ensure the software stack module is loaded and the partition module for the location where you want to install the software is loaded (so the hidden module partition/common to install software in the location common to all regular partitions) Load EasyBuild-production for an install in the production directories or EasyBuild-user for an install in the user or project directories (and you'll need to set some environment variables beforehand to point to that location). Making a master installation of the software stack \u00b6 A master version is an installation of the full system that can run independently from another installation. This is made easy so that it is possible to install a master version under a personal account or on a test system for testing and development independent from the central installation on the system. Create an empty directory which will be the root of the software installation. Clone the LUMI-SoftwareStack repository in that directory. You can change the name of the repository directory to anything as the whole installation is independent of that name provided that LMOD_PACKAGE_PATH refers to the LMOD installation in that repository. In the documentation, we call the directory SystemRepo (as opposed to UserRepo for a user installation that depends on a master installation). Run the prepare_LUMI.sh script from SystemRepo/scripts to initialise the directory structure. The script takes no arguments but it is important to run the version in the SytemRepo in the software installation that needs to be initialised. Running the enable_LUMI.sh script from SystemRepo/scripts prints the (bash) shell commands that need to be executed to initialise the shell to start using this software installation. A very good way is to use it with eval : eval $( <directory>/SystemRepo/scripts/enable_LUMI.sh ) Now use the procedure \"Installing a new version of the Cray PE on the system\" to start a new version of the LUMI software stack, skipping the creation of those files that may already be in the repository because someone else has done them already. Bumping the toolchain to a newer version without changing the other dependencies \u00b6 For now, we use the script upgrade-tc.py developed at CSCS and kept in the tools subdirectory. It is also available when any of our EasyBuild configuration modules is loaded.","title":"Some procedures"},{"location":"procedures/#some-procedures","text":"","title":"Some procedures"},{"location":"procedures/#installing-a-new-version-of-the-cray-pe-on-the-system","text":"","title":"Installing a new version of the Cray PE on the system"},{"location":"procedures/#part-1-making-available-in-crayenv","text":"These instructions are enough to make the new CPE available in the CrayEnv stack but do not yet include instructions for EasyBuild Install the CPE components using the instructions of HPE Create the components file in the CrayPE subdirectory of the repository. This is currently a simple .csv-file but may be replaced by a different one if HPE would provide that information in machine-readable format in a future version of the CPE. Much of that information can be extracted from the HPE-Cray provided file /opt/cray/pe/cpe/yy.mm/modulerc.lua , though we do currently keep some additional packages in that file that don't have their defaults set in that file. One of them which is actually used in the LUMIpartition module file is the version of the craype-targets packages (essentially the CPE targeting modules). This step is only needed if you don't do part 2 as the installation script used in part 2 of this procedure will also do this. Add a cpe/yy.mm.lua link for the just installed programming environment to the modules/CrayOverwrite/core subdirectory of the modules tree (NOT in the modules directory of the repository). This file does need the .csv file generated in the previous step to function properly. Note that this directory also contains a link named .modulerc to the .version file of `/opt/cray/pe/lmod/modulefiles/core/cpe''. It is essential that the default version of the cpe module is the same in the original module directory and in CrayOVerwrite as otherwise the original one may get loaded in some circumstances. By using the symbolic link and the new name with higher priority we assure that the modules from CrayOVerwrite are used. One still needs to ensure that the overwrite modules also are more to the front of the MODULEPATH or things still go wrong. This step is only needed if you don't do part 2 as the installation script used in part 2 of this procedure will also do this. If /opt/cray/pe/cpe/yy.mm/modulerc.lua is missing on the system, an alternative needs to be generated and stored in modules/CrayOverwrite/data-cpe/yy.mm as it is used by the generic cpe module installed in the previous step. This can be done by running ./make_CPE_modulerc yy.mm in the scripts subdirectory of SystemRepo . Hide the new cpe module installed in the Cray PE subdirectory by adding a line to LMOD/modulerc.lua in the repository. If there are new Cray targeting modules that are irrelevant for LUMI you may want to hide them in the respective code block in LMOD/LUMIstack_modulerc.lua .","title":"Part 1: Making available in CrayEnv"},{"location":"procedures/#part-2-adding-support-as-a-lumiyymmdev-software-stack","text":"From here on the setup is largely automated by the prepare_LUMI_stack.sh script in the scripts subdirectory of the repository. A few things need to be in place though before running this script: The component definition of the Cray PE software stack in the .csv file in the CrayPE subdirectory. An EasyConfig file for the right version of EasyBuild in the easybuild/easyconfigs/e/EasyBuild subdirectory of the repository. Note that it is always possible to run prepare_LUMI_stack.sh -script with EASYBUILD_IGNORE_CHECKSUMS=1 set if the checksums in the module file are not yet OK. Add a software stack-specific configuration file easybuild-production-LUMI-yy.mm.cfg for EasyBuild if needed in the easybuild/config subdirectory of the repository. We currently don't include the .dev extension, i.e., if there is a development and an LTS software stack for the same version of the CPE, they share the EasyBuild configuration file. This makes sense because the development stack is meant to prepare for a production stack. Furthermore you may want to make sure that the proper versions of the following files are available in the repository should you want to make changes compared to versions installed before: The generic EasyBuild-config module should you want to make changes to, e.g., the environment variables et by that module. The generic LUMIstack and `LUMIpartition module. The generic cpe module cpe-generic . The software stack initialisation script will take the most recent one (based on the yy.mm version number) that is not newer than the release of the CPE for the software stack. The prepare_LUMI_stack.sh script takes three arguments, and the order is important: The version of the software stack: the Cray PE version, with the extension .dev for a development stack (e.g., 21.05.dev or 21.06). The version of EasyBuild to install in the software stack A work directory for temporary files, used to install a bootstrapping copy of EasyBuild. Rather than trying to use an EasyBuild installation from an older software stack if present to bootstrap the new one, we simply chose to do a bootstrap every time the script is run as this is a procedure that simply always works, though it is more time consuming. The advantage however is that one can just clone the production repository anywhere, run an initialisation script to initialise the structure around the repository, then initialise a software stack and start working. The script then does the following steps: Generate our own cpe/yy.mm module in modules/CrayOverwrite/core by creating a symbolic link to the right version of the generic modules in SystemRepo . Check if the Cray PE comes with its own modulerc.lua file with the default components (if so, that file can be found in /opt/cray/pe/cpe/yy.mm ). If not an alternative file is generated from the data in the compoments .csv file and stored in modules/CrayOverwrite/data-cpe/yy.mm . The file is used by the generic cpe/yy.mm module in modules/CrayOVerwrite/core . Create the software stack module files by symlinking to the generic implementations in SystemRepo . Create the partition modules (second level in the hierarchy) by symlinking to the generic implementations in SystemRepo . Create the LUMIstack_yy.mm_modulerc.lua file in mgmt/LMOD/ModuleRC . Currently this file only contain references to Cray PE modules and as such correspond to the modulerc.lua file in /opt/cray/pe/cpe/yy.mm but his may change in a future version. Whereas the aforementioned modulerc.lua files are meant to be activated by the cpe.yy.mm modules and hence to be used by any software stack that uses the Cray PE, the LUMIstack_yy.mm_modulerc.lua are really meant exclusively for the LUMI software stack. This is done by running the make_LUMIstack_modulerc.sh script. Creates the CPE_modules_*.lua file in mgmt/LMOD/VisibilityHookData for module avail visibility hook in SitePackage.lua . This is done by running the make_CPE_VisibilityHookData.sh script. Creates the full directory structure for the software stack for the modules, the binary installations and the EasyBuild repo. Creates the EasyBuild external modules definition file from the data in the corresponding CPEpakcages_<CPE version>.csv file (if the file does not yet exist). This file is stored in the repo as it may be useful for other people who check out the repository to know what is going on. Creates the EasyBuild-production, EasyBuild-infrastructure and EasyBuild-user modules for each partition by softlinking to the matching generic file in the SystemRepo . Downloads the selected version of EasyBuild to the EasyBuild sources directory if the files are not yet present. Installs a bootstrapping version of EasyBuild in the work directory. As that version will only be used to install EasyBuild from our own EasyConfig file, there is no need to also install the EasyConfig files. Loads the software stack module for the common partition and the EasyBuild-production module to install EasyBuild in the software stack. Create EasyConfig files in the repository for the cpeCray , cpeGNU and cpeAMD toolchains (if those files do not yet exist) and use the just installed EasyBuild to install them in the Infrastructure module subdirectory for all 4 regular and the common partition. The toolchain definition EasyConfig files are generated by the make_CPE_EBfile.sh script. Things to do afterwards: If you want to change the default version of the LUMI software stack module, you can do this by editing .modulerc.lua in modules/SoftwareStack/LUMI . And now one should be ready to install other software... Ensure the software stack module is loaded and the partition module for the location where you want to install the software is loaded (so the hidden module partition/common to install software in the location common to all regular partitions) Load EasyBuild-production for an install in the production directories or EasyBuild-user for an install in the user or project directories (and you'll need to set some environment variables beforehand to point to that location).","title":"Part 2: Adding support as a LUMI/yy.mm(.dev) software stack"},{"location":"procedures/#making-a-master-installation-of-the-software-stack","text":"A master version is an installation of the full system that can run independently from another installation. This is made easy so that it is possible to install a master version under a personal account or on a test system for testing and development independent from the central installation on the system. Create an empty directory which will be the root of the software installation. Clone the LUMI-SoftwareStack repository in that directory. You can change the name of the repository directory to anything as the whole installation is independent of that name provided that LMOD_PACKAGE_PATH refers to the LMOD installation in that repository. In the documentation, we call the directory SystemRepo (as opposed to UserRepo for a user installation that depends on a master installation). Run the prepare_LUMI.sh script from SystemRepo/scripts to initialise the directory structure. The script takes no arguments but it is important to run the version in the SytemRepo in the software installation that needs to be initialised. Running the enable_LUMI.sh script from SystemRepo/scripts prints the (bash) shell commands that need to be executed to initialise the shell to start using this software installation. A very good way is to use it with eval : eval $( <directory>/SystemRepo/scripts/enable_LUMI.sh ) Now use the procedure \"Installing a new version of the Cray PE on the system\" to start a new version of the LUMI software stack, skipping the creation of those files that may already be in the repository because someone else has done them already.","title":"Making a master installation of the software stack"},{"location":"procedures/#bumping-the-toolchain-to-a-newer-version-without-changing-the-other-dependencies","text":"For now, we use the script upgrade-tc.py developed at CSCS and kept in the tools subdirectory. It is also available when any of our EasyBuild configuration modules is loaded.","title":"Bumping the toolchain to a newer version without changing the other dependencies"},{"location":"start_on_LUMI_pilot/","text":"Setting up the stack on LUMI \u00b6 There are currently two options to use the LUST LUMI software stacks There is already one installed in /appl/lumi but not yet activated (as of December 14). However, it requires very little to activate. You can still follow the procedure for the initial pilot projects, when nothing was installed already. Take a look at the \"Installing the stack from scratch\" section. Augmenting the stack with software installed in /projappl follows the same procedure in both cases. Using the pre-installed stack \u00b6 Simply run /appl/lumi/LUMI-SoftwareStack/activate_LMOD_LUMI.sh and it will tell you what to do. Note that those instructions change over time as the stack is still in beta and evolves, so if you run into problems it is good to check them again. Installing the stack from scratch \u00b6 Create the directory where you want to install the software stack and move into that directory: mkdir LUMI-pilot cd LUMI-pilot If you want to share the installation with everybody in your project, doing so in your /projappl directory may be a good choice. However, beware as a full installation will eat into your file quota. Parallel file systems are really meant to work with large files while a software installation unfortunately can contain many small files. You can also work in scratch but the problem there is that the installation will be removed automatically after 90 days. Clone the LUMI-SoftwareStack and LUMI-EasyBuild-contrib repositories: git clone git@github.com:Lumi-supercomputer/LUMI-SoftwareStack.git git clone git@github.com:Lumi-supercomputer/LUMI-EasyBuild-contrib.git or git clone https://github.com/Lumi-supercomputer/LUMI-SoftwareStack.git git clone https://github.com/Lumi-supercomputer/LUMI-EasyBuild-contrib.git depending on whether you have ssh access to GitHub or not. git clone git@github.com:klust/LUMI-SoftwareStack.git Move into the LUMI-SoftwareStack/scripts subdirectory cd LUMI-SoftwareStack/scripts Now we will first build the raw structure of the software stack: ./prepare_LUMI.sh Since currently CPE 21.05 is installed on LUMI, but we have no intention to use it in a LUMI software stack, we run a script to simply make CPE 21.05 available in the CrayEnv stack of our system. ./prepare_CrayEnv_stack.sh 21 .05 Now is time to prepare the LUMI/21.08 stack for which we used EasyBuild 4.4.1. Some work space is also needed but can be erased afterwards. Though not ideal, $HOME/work is used in the command as this variable is available to all users. ./prepare_LUMI_stack.sh 21 .08 4 .4.2 $HOME /work This step does take a while though. Now we first need to activate LMOD and point it to the right module subdirectories. This is done by a script which is in the LUMI-SoftwareStack subdirectory itself (as it is only a temporary solution). The instructions to do this are generated by running the activate_LMOD_LUMI.sh script in the LUMI-SoftwareStack directory: cd .. ./activate_LMOD_LUMI.sh You can call this script at any time to regenerate the instructions as long as you call it from within the repository so that it can detect the directory. Follow the instructions. We suggest that for now you simply copy the 5 lines of code as indicated in the text. Now we are ready to begin installing software. Assuming you are still in the LUMI-pilot/LUMI-SoftwareStack directory: We'll use some lists of software that are in a subdirectory: cd easybuild/easystacks/LUMI-21.08 If you want some elementary build tools etc to be available in the CrayEnv environment, which is really just the plain Cray Programming Environment with minimal interference from the LUMI software stacks, you can generate those using: module --force purge ml LUMI/21.08 partition/CrayEnv ml EasyBuild-CrayEnv eb --experimental --easystack production-CrayEnv-21.08.yaml The regular software stack consists of multiple parts: A number of building blocks that are not performance-critical and have been installed with the system compilers. Complete lines of software compiled with either PrgEnv-gnu , PrgEnv-cray or PrgEnv-aocc . However, in EasyBuild these toolchains are loaded by loading respectively the cpeGNU/21.08 , cpeCray/21.08 or cpeAMD/21.08 toolchains. At the moment of writing, cpeAMD/21.-08 does not work as the modules are installed on the system by Cray but the software is missing. So this is the moment to consider what you want to use. The common building blocks are always needed though, though some users may not need the Rust module which will also be installed with the procedure below. However, if you only intend to use, e.g., the GNU compilers, it makes no sense to install all cpeCray or cpeAMD modules. To install the common part of building blocks: module --force purge ml LUMI/21.08 partition/common ml EasyBuild-production eb --experimental --easystack production-common-21.08.yaml To install the cpeGNU modules: module --force purge ml LUMI/21.08 partition/L ml EasyBuild-production eb -r --experimental --easystack production-L-21.08-GNU.yaml Note the -r argument that was absent before. This is because due to problems with the so-called EasyStack files (those yaml files) it was not yet possible to specify all required EasyBuild recipes, and recursive installations are disabled by default. To install the cpeCraymodules: module --force purge ml LUMI/21.08 partition/L ml EasyBuild-production eb -r --experimental --easystack production-L-21.08-Cray.yaml Note again the -r argument for the very same reasons. mkdir LUMI-pilot cd LUMI-pilot git clone git@github.com:klust/LUMI-SoftwareStack.git git clone git@github.com:Lumi-supercomputer/LUMI-EasyBuild-contrib.git cd LUMI-SoftwareStack/scripts ./prepare_LUMI.sh ./prepare_CrayEnv_stack.sh 21 .05 ./prepare_LUMI_stack.sh 21 .08 4 .4.2 $HOME /work Installing additional software \u00b6 If you installed the full stack in your own project directory, you can actually have two locations to install software with EasyBuild In the main stack (which you will of course not be able to do once we have a central installation): Use EasyBuild-production. It is however also possible to have your own personal stack. By default this will be build in your home directory, but it might be better to share with your colleagues in your project. To use any other location then the default one, every user of the stack should ensure that the environment variable EBU_USER_PREFIX points to the directory. After that, the LUMI software stack will take care of everything and activate the right modules. However, you now have to use the module EasyBuild-user for the installation. We recommend to have your own repository in $EBU_USER_PREFIX where you have all EasyConfig files that you want to use. The mandatory name of that repository directory is UserRepo (as otherwise it is not found by EasyBuild-user), and EasyConfig files should be in UserRepo/easybuild/easyconfigs . For more complete and technical information on our setup, please read the other pages pointed to in the README document in this docs directory . Note that we cannot support the common EasyBuild toolchains on LUMI in this phase as for now we concentrate on the Cray Programming Environment for which we have support from HPE-Cray. This does mean that EasyBuild recipes have to be reworked. We're still working on reducing module clutter on the system. For now all basic libraries have their own module. Some of those may be hidden in the future as few users use them directly. However, even then they can still be found with module spider and be loaded. We have a few special modules on LUMI that bundle a lot of software that is otherwise in separate modules on a typical EasyBuild installation. buildtools/21.08 (and other respective versions for each LUMI/yy.mm stack) bundles a number of popular build tools. This is done in a single module to have them all available in a single command and to try to use a very consistent set of build tools throughout most of our software installation process. Try module help buildtools/21.08 to check what's in it. systools/15.1.0 contains some useful tools, such as tree and htop . syslibs/15.1.0 is a hidden module and is really just meant as a build dependency for some basic tools that we install.","title":"Installation instructions for the pilot phase"},{"location":"start_on_LUMI_pilot/#setting-up-the-stack-on-lumi","text":"There are currently two options to use the LUST LUMI software stacks There is already one installed in /appl/lumi but not yet activated (as of December 14). However, it requires very little to activate. You can still follow the procedure for the initial pilot projects, when nothing was installed already. Take a look at the \"Installing the stack from scratch\" section. Augmenting the stack with software installed in /projappl follows the same procedure in both cases.","title":"Setting up the stack on LUMI"},{"location":"start_on_LUMI_pilot/#using-the-pre-installed-stack","text":"Simply run /appl/lumi/LUMI-SoftwareStack/activate_LMOD_LUMI.sh and it will tell you what to do. Note that those instructions change over time as the stack is still in beta and evolves, so if you run into problems it is good to check them again.","title":"Using the pre-installed stack"},{"location":"start_on_LUMI_pilot/#installing-the-stack-from-scratch","text":"Create the directory where you want to install the software stack and move into that directory: mkdir LUMI-pilot cd LUMI-pilot If you want to share the installation with everybody in your project, doing so in your /projappl directory may be a good choice. However, beware as a full installation will eat into your file quota. Parallel file systems are really meant to work with large files while a software installation unfortunately can contain many small files. You can also work in scratch but the problem there is that the installation will be removed automatically after 90 days. Clone the LUMI-SoftwareStack and LUMI-EasyBuild-contrib repositories: git clone git@github.com:Lumi-supercomputer/LUMI-SoftwareStack.git git clone git@github.com:Lumi-supercomputer/LUMI-EasyBuild-contrib.git or git clone https://github.com/Lumi-supercomputer/LUMI-SoftwareStack.git git clone https://github.com/Lumi-supercomputer/LUMI-EasyBuild-contrib.git depending on whether you have ssh access to GitHub or not. git clone git@github.com:klust/LUMI-SoftwareStack.git Move into the LUMI-SoftwareStack/scripts subdirectory cd LUMI-SoftwareStack/scripts Now we will first build the raw structure of the software stack: ./prepare_LUMI.sh Since currently CPE 21.05 is installed on LUMI, but we have no intention to use it in a LUMI software stack, we run a script to simply make CPE 21.05 available in the CrayEnv stack of our system. ./prepare_CrayEnv_stack.sh 21 .05 Now is time to prepare the LUMI/21.08 stack for which we used EasyBuild 4.4.1. Some work space is also needed but can be erased afterwards. Though not ideal, $HOME/work is used in the command as this variable is available to all users. ./prepare_LUMI_stack.sh 21 .08 4 .4.2 $HOME /work This step does take a while though. Now we first need to activate LMOD and point it to the right module subdirectories. This is done by a script which is in the LUMI-SoftwareStack subdirectory itself (as it is only a temporary solution). The instructions to do this are generated by running the activate_LMOD_LUMI.sh script in the LUMI-SoftwareStack directory: cd .. ./activate_LMOD_LUMI.sh You can call this script at any time to regenerate the instructions as long as you call it from within the repository so that it can detect the directory. Follow the instructions. We suggest that for now you simply copy the 5 lines of code as indicated in the text. Now we are ready to begin installing software. Assuming you are still in the LUMI-pilot/LUMI-SoftwareStack directory: We'll use some lists of software that are in a subdirectory: cd easybuild/easystacks/LUMI-21.08 If you want some elementary build tools etc to be available in the CrayEnv environment, which is really just the plain Cray Programming Environment with minimal interference from the LUMI software stacks, you can generate those using: module --force purge ml LUMI/21.08 partition/CrayEnv ml EasyBuild-CrayEnv eb --experimental --easystack production-CrayEnv-21.08.yaml The regular software stack consists of multiple parts: A number of building blocks that are not performance-critical and have been installed with the system compilers. Complete lines of software compiled with either PrgEnv-gnu , PrgEnv-cray or PrgEnv-aocc . However, in EasyBuild these toolchains are loaded by loading respectively the cpeGNU/21.08 , cpeCray/21.08 or cpeAMD/21.08 toolchains. At the moment of writing, cpeAMD/21.-08 does not work as the modules are installed on the system by Cray but the software is missing. So this is the moment to consider what you want to use. The common building blocks are always needed though, though some users may not need the Rust module which will also be installed with the procedure below. However, if you only intend to use, e.g., the GNU compilers, it makes no sense to install all cpeCray or cpeAMD modules. To install the common part of building blocks: module --force purge ml LUMI/21.08 partition/common ml EasyBuild-production eb --experimental --easystack production-common-21.08.yaml To install the cpeGNU modules: module --force purge ml LUMI/21.08 partition/L ml EasyBuild-production eb -r --experimental --easystack production-L-21.08-GNU.yaml Note the -r argument that was absent before. This is because due to problems with the so-called EasyStack files (those yaml files) it was not yet possible to specify all required EasyBuild recipes, and recursive installations are disabled by default. To install the cpeCraymodules: module --force purge ml LUMI/21.08 partition/L ml EasyBuild-production eb -r --experimental --easystack production-L-21.08-Cray.yaml Note again the -r argument for the very same reasons. mkdir LUMI-pilot cd LUMI-pilot git clone git@github.com:klust/LUMI-SoftwareStack.git git clone git@github.com:Lumi-supercomputer/LUMI-EasyBuild-contrib.git cd LUMI-SoftwareStack/scripts ./prepare_LUMI.sh ./prepare_CrayEnv_stack.sh 21 .05 ./prepare_LUMI_stack.sh 21 .08 4 .4.2 $HOME /work","title":"Installing the stack from scratch"},{"location":"start_on_LUMI_pilot/#installing-additional-software","text":"If you installed the full stack in your own project directory, you can actually have two locations to install software with EasyBuild In the main stack (which you will of course not be able to do once we have a central installation): Use EasyBuild-production. It is however also possible to have your own personal stack. By default this will be build in your home directory, but it might be better to share with your colleagues in your project. To use any other location then the default one, every user of the stack should ensure that the environment variable EBU_USER_PREFIX points to the directory. After that, the LUMI software stack will take care of everything and activate the right modules. However, you now have to use the module EasyBuild-user for the installation. We recommend to have your own repository in $EBU_USER_PREFIX where you have all EasyConfig files that you want to use. The mandatory name of that repository directory is UserRepo (as otherwise it is not found by EasyBuild-user), and EasyConfig files should be in UserRepo/easybuild/easyconfigs . For more complete and technical information on our setup, please read the other pages pointed to in the README document in this docs directory . Note that we cannot support the common EasyBuild toolchains on LUMI in this phase as for now we concentrate on the Cray Programming Environment for which we have support from HPE-Cray. This does mean that EasyBuild recipes have to be reworked. We're still working on reducing module clutter on the system. For now all basic libraries have their own module. Some of those may be hidden in the future as few users use them directly. However, even then they can still be found with module spider and be loaded. We have a few special modules on LUMI that bundle a lot of software that is otherwise in separate modules on a typical EasyBuild installation. buildtools/21.08 (and other respective versions for each LUMI/yy.mm stack) bundles a number of popular build tools. This is done in a single module to have them all available in a single command and to try to use a very consistent set of build tools throughout most of our software installation process. Try module help buildtools/21.08 to check what's in it. systools/15.1.0 contains some useful tools, such as tree and htop . syslibs/15.1.0 is a hidden module and is really just meant as a build dependency for some basic tools that we install.","title":"Installing additional software"},{"location":"whats_new/","text":"What's new or different? \u00b6 2022-04-22: Corrections to the EasyBuild-config modules to avoid having twice the same directory in the output of searches. 2022-03-01: Not every non-development stack should be marked as a LTS stack as it turns out to be very difficult to guarantee longevity of the software stack. Hence there is now a table at the top of SitePackage.lua to mark which stacks are LTS. 2022-03-01: Option to turn off the tip in the message-of-the-day or even the whole message-of-the-day. 2022-02-24: Hiding cpe-cuda modules via LMOD/modulerc.lua as they don't make sense on LUMI. 2022-02-23: Added support for a system partition with software that is available everywhere, EasyBuild-CrayEnv has been replaced with EasyBuild-production. 2021-12-15: Moved the list of default modules to load out of CrayEnv to SitePackage.lua so and now also load target modules in the LUMIpartition module so that users who want to use PrgEnv instead of cpeGNU/cpeCray/cpeAMD get the right target modules. 2021-12-14: Added init-lumi, a module to finish the initialisation that is called from the Cray PE initialisation process. The module also adds to the message-of-the-day and adds a fortune-like tip about using LUMI. 2021-12-14: Removed adding the overwrite CPE modules to the MODULEPATH in CrayEnv and LUMI (actually the partition modules) as at system rollout it seemed they created as much problems as they solved. 2021-10-26: Implemented a dummy partition CrayEnv in the LUMI software stack to cross-install to CrayEnv to offer a simple way to install additional tools using the SYSTEM toolchain only. 2021-10-19: Removed the EB level in the directory hierarchy for binaries in the EasyBuild-user installation as the whole tree is only meant for EasyBuild-installed software. 2021-09-21: Added a new toolchain option to cpeGNU: gfortran9-compat that adds compatibility flags to FFLAGS , FCFLAGS and F90FLAGS to improve compatibility with gfortran 9. Currently this is only -fallow-argument-mismatch . 2021-09-14: Loading a LUMI software stack module now also sets the environment variable LUMI_STACK_CPE_VERSION which can be used to know which version of the CPE the software stack is for (useful if stack is a development stack with name ending on .dev) 2021-09-14: Added the tools subdirectory with scripts that we want to make available in the PATH and that are useful for any user, not just for a one-time setup of the repository or adding a new software stack to the repository (the latter remain in the scripts subdirectory and are not put in the PATH when loading one of our EasyBuild configuration modules). 2021-08-11: Added the LUMI-EasyBuild-contrib repository and made it part of the search path (but not of the robot path) of the EasyBuild-* modules. 2021-08-03: Changed the SitePackage.lua function that detects the LUMI partition, and this has influence on how the repository should be used for testing. The environment variable LUMI_OVERWRITE_PARTITION is now used to overwrite any automatic selection of the partition. A demo selection process based on the hostname was implemented: On eiger uan01 and uan02 the partition is set to L On eiger uan03 the partition is set to common On all other hosts we first check for the environment variable LUMI_PARTITION and use that one and otherwise we set the partition to L. The enable_LUMI.sh script now sets LUMI_OVERWRITE_PARTITION rather than LUMI_PARTITION so if you use that script to set the environment, you shouldn't note anything. Toolchain setup compared to the CSCS setup of June 2021 \u00b6 The CSCS definition for cpeAMD relied on a definition of the aocc compilers that used GNU compiler options that likely did not always work and couldn't be found in the AOCC manuals. They have been replaced with Clang-based options. The single implementation for all cpe* compilers in compilers/cpe.py has been replaced by a different implementation for each compiler. The reasons are that Not all data that should be copied from the non-Cray versions of the compiler to configure the compiler was actually copied. We found bugs in the implementation of the GNU compilers in EasyBuild and by using a separate file could work around them until we now for sure they are bugs and what the original intent was. Some compiler options may not map onto a flag but may need further processing, and having separate files makes that a bit easier until we have enough insight in all problems that may occur to bundle them again in a single implementation. Improvements to the Cray compiler file: The Cray compiler wrappers provide -openmp and -noopenmp flags to turn OpenMP on or off, and these should work with all supported compilers. These were implemented to solve the problem that for the CCE compilers with the Classic Cray Fortran compiler but new clang-based C compiler the options for enabling OpenMP are different which is something that is not appreciated by EasyBuild.","title":"What's new or different?"},{"location":"whats_new/#whats-new-or-different","text":"2022-04-22: Corrections to the EasyBuild-config modules to avoid having twice the same directory in the output of searches. 2022-03-01: Not every non-development stack should be marked as a LTS stack as it turns out to be very difficult to guarantee longevity of the software stack. Hence there is now a table at the top of SitePackage.lua to mark which stacks are LTS. 2022-03-01: Option to turn off the tip in the message-of-the-day or even the whole message-of-the-day. 2022-02-24: Hiding cpe-cuda modules via LMOD/modulerc.lua as they don't make sense on LUMI. 2022-02-23: Added support for a system partition with software that is available everywhere, EasyBuild-CrayEnv has been replaced with EasyBuild-production. 2021-12-15: Moved the list of default modules to load out of CrayEnv to SitePackage.lua so and now also load target modules in the LUMIpartition module so that users who want to use PrgEnv instead of cpeGNU/cpeCray/cpeAMD get the right target modules. 2021-12-14: Added init-lumi, a module to finish the initialisation that is called from the Cray PE initialisation process. The module also adds to the message-of-the-day and adds a fortune-like tip about using LUMI. 2021-12-14: Removed adding the overwrite CPE modules to the MODULEPATH in CrayEnv and LUMI (actually the partition modules) as at system rollout it seemed they created as much problems as they solved. 2021-10-26: Implemented a dummy partition CrayEnv in the LUMI software stack to cross-install to CrayEnv to offer a simple way to install additional tools using the SYSTEM toolchain only. 2021-10-19: Removed the EB level in the directory hierarchy for binaries in the EasyBuild-user installation as the whole tree is only meant for EasyBuild-installed software. 2021-09-21: Added a new toolchain option to cpeGNU: gfortran9-compat that adds compatibility flags to FFLAGS , FCFLAGS and F90FLAGS to improve compatibility with gfortran 9. Currently this is only -fallow-argument-mismatch . 2021-09-14: Loading a LUMI software stack module now also sets the environment variable LUMI_STACK_CPE_VERSION which can be used to know which version of the CPE the software stack is for (useful if stack is a development stack with name ending on .dev) 2021-09-14: Added the tools subdirectory with scripts that we want to make available in the PATH and that are useful for any user, not just for a one-time setup of the repository or adding a new software stack to the repository (the latter remain in the scripts subdirectory and are not put in the PATH when loading one of our EasyBuild configuration modules). 2021-08-11: Added the LUMI-EasyBuild-contrib repository and made it part of the search path (but not of the robot path) of the EasyBuild-* modules. 2021-08-03: Changed the SitePackage.lua function that detects the LUMI partition, and this has influence on how the repository should be used for testing. The environment variable LUMI_OVERWRITE_PARTITION is now used to overwrite any automatic selection of the partition. A demo selection process based on the hostname was implemented: On eiger uan01 and uan02 the partition is set to L On eiger uan03 the partition is set to common On all other hosts we first check for the environment variable LUMI_PARTITION and use that one and otherwise we set the partition to L. The enable_LUMI.sh script now sets LUMI_OVERWRITE_PARTITION rather than LUMI_PARTITION so if you use that script to set the environment, you shouldn't note anything.","title":"What's new or different?"},{"location":"whats_new/#toolchain-setup-compared-to-the-cscs-setup-of-june-2021","text":"The CSCS definition for cpeAMD relied on a definition of the aocc compilers that used GNU compiler options that likely did not always work and couldn't be found in the AOCC manuals. They have been replaced with Clang-based options. The single implementation for all cpe* compilers in compilers/cpe.py has been replaced by a different implementation for each compiler. The reasons are that Not all data that should be copied from the non-Cray versions of the compiler to configure the compiler was actually copied. We found bugs in the implementation of the GNU compilers in EasyBuild and by using a separate file could work around them until we now for sure they are bugs and what the original intent was. Some compiler options may not map onto a flag but may need further processing, and having separate files makes that a bit easier until we have enough insight in all problems that may occur to bundle them again in a single implementation. Improvements to the Cray compiler file: The Cray compiler wrappers provide -openmp and -noopenmp flags to turn OpenMP on or off, and these should work with all supported compilers. These were implemented to solve the problem that for the CCE compilers with the Classic Cray Fortran compiler but new clang-based C compiler the options for enabling OpenMP are different which is something that is not appreciated by EasyBuild.","title":"Toolchain setup compared to the CSCS setup of June 2021"},{"location":"Toolchains/cpeAOCC/","text":"cpeAOCC toolchain \u00b6 Note: The options are for the aocc.py file included in this repository and are not the same as those for the repository at CSCS. Note about the compilers \u00b6 AOCC 2.1 is based on LLVM 9.0 release (llvm.org, 19th Sep 2019) with improved Flang Fortran frond-end added with F2008 features and bug fixes. AOCC 2.2 is based on LLVM 10.0 release (llvm.org, 24th Mar 2020) with improved Flang Fortran front-end added with F2008 features and bug fixes. AOCC 3.0 is based on LLVM 12 trunk (llvm.org, 22nd Oct 2020) with Flang as a Fortran front-end added with F2008, Real 128 features. AOCC 3.0 also includes the support for OpenMP Debugging Interface (OMPD) APIs. Available options \u00b6 The cpeAMD toolchain supports the common toolchain options , the additional AOCC flags and some additional Cray-specific flags, two of which are really just redefinitions of standard compiler flags. AOCC-specific flags \u00b6 AOCC supports a number of extensions that are similar to those of the GNU compiler. Option Categorie What? lto code generation Enable Link Time Optimization loop-vectorize parallelism Explicitly enable/disable loop vectorization basic-block-vectorize parallelism Explicitly enable/disable basic block vectorization cpeAOCC-specific flags \u00b6 Option Categorie What? dynamic code generation Generate dynamically linked executable (default: True) mpich-mt parallelism Alternate Cray-MPICH library for MT support (default: False) mpich-mt: Directs the driver to link in an alternate version of the Cray-MPICH library which provides fine-grained multi-threading support to applications that perform MPI operations within threaded regions. (default: False) Two further options trigger different compiler flags than in the GCC toolchain: verbose and optarch but have otherwise the same meaning. Mapping of options onto compiler flags \u00b6 Compiler optimization level \u00b6 The common options translate into: Option Flag noopt -O0 lowopt -O1 defaultopt -O2 -fvectorize -fslp-vectorize opt -O3 Other optimization-related options (and see also parallelism below): Option Flag unroll -funroll-loops optarch TODO Floating point precision \u00b6 The decision of our mapping is based partly on information from Cray and partly on looking through various manuals on Clang. Level strict : We simply stick to -ffp-model=strict . This automatically disables all of the -ffast-math enablements so there is no need to add -fno-fast-math . Note that at this level, fused multiply adds are disabled. Level precise : We stick to -ffp-model=precise and add -ffp-contract=fast-honor-pragmas . Default level: Currently the same as strict . Level loose : Set to -ffp-model=fast but try to turn on again a few safeguards: -ffp-contract=fast-honor-pragmas , -fhonor-infinities , -fhonor-nans , -fsigned-zeros . Level veryloose : Set to -ffp-model=fast Note: Very recnet versions of clang add -ffp-contract=fast-honor-pragmas which may be interesting to add to precise , defaultprec and loose but is not yet supported by AOCC 2.2. Option Flag strict -ffp-model=strict precise -ffp-model=precise defaultprec -ffp-model=precise loose -ffp-model=fast -fhonor-infinities -fhonor-nans -fsigned-zeros veryloose -ffp-model=fast Other floating-point optimisation and accuracy-related flags: Option What? ieee Not supported in clang/flang Common parallelism-related options \u00b6 Option Flag vectorize False: -fno-vectorize -fno-slp-vectorize True: -fvectorize -fslp-vectorize loop -ftree-switch-conversion -floop-interchange -floop-strip-mine -floop-block openmp -fopenmp usempi No compiler flags mpich-mt -craympich-mt Code generation and linking options \u00b6 Option Flag dynamic No flag as this is currently the only mode supported lto -flto 32bit -m32 debug -g pic -fPIC packed-linker-options Pack the linker options as comma separated list (default: False) shared -shared static -static rpath Use RPATH wrappers when --rpath is enabled in EasyBuild configuration (default: True) Source-related options \u00b6 Option Flag cstd -std=%(value)s i8 -fdefault-integer-8 r8 -fdefault-real-8 f2c -ff2c Miscellaneous options \u00b6 Option Flag verbose -craype-verbose","title":"cpeAOCC toolchain"},{"location":"Toolchains/cpeAOCC/#cpeaocc-toolchain","text":"Note: The options are for the aocc.py file included in this repository and are not the same as those for the repository at CSCS.","title":"cpeAOCC toolchain"},{"location":"Toolchains/cpeAOCC/#note-about-the-compilers","text":"AOCC 2.1 is based on LLVM 9.0 release (llvm.org, 19th Sep 2019) with improved Flang Fortran frond-end added with F2008 features and bug fixes. AOCC 2.2 is based on LLVM 10.0 release (llvm.org, 24th Mar 2020) with improved Flang Fortran front-end added with F2008 features and bug fixes. AOCC 3.0 is based on LLVM 12 trunk (llvm.org, 22nd Oct 2020) with Flang as a Fortran front-end added with F2008, Real 128 features. AOCC 3.0 also includes the support for OpenMP Debugging Interface (OMPD) APIs.","title":"Note about the compilers"},{"location":"Toolchains/cpeAOCC/#available-options","text":"The cpeAMD toolchain supports the common toolchain options , the additional AOCC flags and some additional Cray-specific flags, two of which are really just redefinitions of standard compiler flags.","title":"Available options"},{"location":"Toolchains/cpeAOCC/#aocc-specific-flags","text":"AOCC supports a number of extensions that are similar to those of the GNU compiler. Option Categorie What? lto code generation Enable Link Time Optimization loop-vectorize parallelism Explicitly enable/disable loop vectorization basic-block-vectorize parallelism Explicitly enable/disable basic block vectorization","title":"AOCC-specific flags"},{"location":"Toolchains/cpeAOCC/#cpeaocc-specific-flags","text":"Option Categorie What? dynamic code generation Generate dynamically linked executable (default: True) mpich-mt parallelism Alternate Cray-MPICH library for MT support (default: False) mpich-mt: Directs the driver to link in an alternate version of the Cray-MPICH library which provides fine-grained multi-threading support to applications that perform MPI operations within threaded regions. (default: False) Two further options trigger different compiler flags than in the GCC toolchain: verbose and optarch but have otherwise the same meaning.","title":"cpeAOCC-specific flags"},{"location":"Toolchains/cpeAOCC/#mapping-of-options-onto-compiler-flags","text":"","title":"Mapping of options onto compiler flags"},{"location":"Toolchains/cpeAOCC/#compiler-optimization-level","text":"The common options translate into: Option Flag noopt -O0 lowopt -O1 defaultopt -O2 -fvectorize -fslp-vectorize opt -O3 Other optimization-related options (and see also parallelism below): Option Flag unroll -funroll-loops optarch TODO","title":"Compiler optimization level"},{"location":"Toolchains/cpeAOCC/#floating-point-precision","text":"The decision of our mapping is based partly on information from Cray and partly on looking through various manuals on Clang. Level strict : We simply stick to -ffp-model=strict . This automatically disables all of the -ffast-math enablements so there is no need to add -fno-fast-math . Note that at this level, fused multiply adds are disabled. Level precise : We stick to -ffp-model=precise and add -ffp-contract=fast-honor-pragmas . Default level: Currently the same as strict . Level loose : Set to -ffp-model=fast but try to turn on again a few safeguards: -ffp-contract=fast-honor-pragmas , -fhonor-infinities , -fhonor-nans , -fsigned-zeros . Level veryloose : Set to -ffp-model=fast Note: Very recnet versions of clang add -ffp-contract=fast-honor-pragmas which may be interesting to add to precise , defaultprec and loose but is not yet supported by AOCC 2.2. Option Flag strict -ffp-model=strict precise -ffp-model=precise defaultprec -ffp-model=precise loose -ffp-model=fast -fhonor-infinities -fhonor-nans -fsigned-zeros veryloose -ffp-model=fast Other floating-point optimisation and accuracy-related flags: Option What? ieee Not supported in clang/flang","title":"Floating point precision"},{"location":"Toolchains/cpeAOCC/#common-parallelism-related-options","text":"Option Flag vectorize False: -fno-vectorize -fno-slp-vectorize True: -fvectorize -fslp-vectorize loop -ftree-switch-conversion -floop-interchange -floop-strip-mine -floop-block openmp -fopenmp usempi No compiler flags mpich-mt -craympich-mt","title":"Common parallelism-related options"},{"location":"Toolchains/cpeAOCC/#code-generation-and-linking-options","text":"Option Flag dynamic No flag as this is currently the only mode supported lto -flto 32bit -m32 debug -g pic -fPIC packed-linker-options Pack the linker options as comma separated list (default: False) shared -shared static -static rpath Use RPATH wrappers when --rpath is enabled in EasyBuild configuration (default: True)","title":"Code generation and linking options"},{"location":"Toolchains/cpeAOCC/#source-related-options","text":"Option Flag cstd -std=%(value)s i8 -fdefault-integer-8 r8 -fdefault-real-8 f2c -ff2c","title":"Source-related options"},{"location":"Toolchains/cpeAOCC/#miscellaneous-options","text":"Option Flag verbose -craype-verbose","title":"Miscellaneous options"},{"location":"Toolchains/cpeCray/","text":"cpeGNU toolchain \u00b6 NOTE: Stuff in italics has to be checked: Do these options really work? Available options \u00b6 The cpeCray toolchain supports the common toolchain options , and some additional Cray-specific flags, two of which are really just redefinitions of standard compiler flags. cpeCray-specific flags \u00b6 Option Categorie What? dynamic code generation Generate dynamically linked executable (default: True) mpich-mt parallelism Alternate Cray-MPICH library for MT support (default: False) mpich-mt: Directs the driver to link in an alternate version of the Cray-MPICH library which provides fine-grained multi-threading support to applications that perform MPI operations within threaded regions. (default: False) Note that 'i8' and 'r8' do not work for the cpeCray toolchain as the compiler only has an option to promote both types. Two further options trigger different compiler flags than in the GCC toolchain: verbose and optarch but have otherwise the same meaning. Mapping of options onto compiler flags \u00b6 Some Cray-specific remarks \u00b6 A number of options are different for the clang-based C/C++ compilers and Fortran compiler, something that EasyBuild cannot easily deal with. This is, e.g., the case for the options that set the floating point computation model, so for now these are left blank. Fortran floating point optimisation and accuracy options: -hfp0 to -hfp4 , with -hfp2 the default according to the manual. The manual only claims -hfp0 to -hfp3 . For OpenMP the manual also claims a different option for the Fortran compiler, but it turns out that recent versions also support -fopenmp so no changes are needed with respect to the standard EasyBuild behaviour. The manual claims that there is also a -fnoopenmp but that turns out to be wrong in cce 12. The situation about promoting integer and real in Fortran to 8 bytes is very confusing. Those options are not properly documented in the manuals. The Cray driver manual only mentions -default64 which according to the documentation is then converted to -sdefault64 . In the documentation, there is mention of -sreal64 (or -s real64`), but this option is nowhere to be found in the list of compiler options. However, it turns out that -sinteger64 (or -s integer64 ) and -sreal64 (or -s real64``) both exist and do what is expected from them. TODO: Check if -flto exist in Fortran and C and if so, add to the options. Compiler optimization level \u00b6 The common options translate into: Option Flag noopt -O0 lowopt -O1 defaultopt -O2 opt -O3 Other optimization-related options (and see also parallelism below): Option Flag unroll -funroll optarch TODO Floating point precision \u00b6 These flags are currently not correctly honoured. Option Flag strict / precise / defaultprec / loose / veryloose / Other floating-point optimisation and accuracy-related flags: Option What? ieee / Common parallelism-related options \u00b6 Option Flag vectorize / openmp -fopenmp usempi No compiler flags mpich-mt -craympich-mt Code generation and linking options \u00b6 Option Flag dynamic No flag as this is currently the only mode supported 32bit -m32 debug -g pic -fPIC packed-linker-options Pack the linker options as comma separated list (default: False) shared -shared static -static rpath Use RPATH wrappers when --rpath is enabled in EasyBuild configuration (default: True) Source-related options \u00b6 Option Flag cstd -std=%(value)s i8 -sinteger64 r8 -sreal64 Miscellaneous options \u00b6 Option Flag verbose -craype-verbose","title":"cpeCray toolchain"},{"location":"Toolchains/cpeCray/#cpegnu-toolchain","text":"NOTE: Stuff in italics has to be checked: Do these options really work?","title":"cpeGNU toolchain"},{"location":"Toolchains/cpeCray/#available-options","text":"The cpeCray toolchain supports the common toolchain options , and some additional Cray-specific flags, two of which are really just redefinitions of standard compiler flags.","title":"Available options"},{"location":"Toolchains/cpeCray/#cpecray-specific-flags","text":"Option Categorie What? dynamic code generation Generate dynamically linked executable (default: True) mpich-mt parallelism Alternate Cray-MPICH library for MT support (default: False) mpich-mt: Directs the driver to link in an alternate version of the Cray-MPICH library which provides fine-grained multi-threading support to applications that perform MPI operations within threaded regions. (default: False) Note that 'i8' and 'r8' do not work for the cpeCray toolchain as the compiler only has an option to promote both types. Two further options trigger different compiler flags than in the GCC toolchain: verbose and optarch but have otherwise the same meaning.","title":"cpeCray-specific flags"},{"location":"Toolchains/cpeCray/#mapping-of-options-onto-compiler-flags","text":"","title":"Mapping of options onto compiler flags"},{"location":"Toolchains/cpeCray/#some-cray-specific-remarks","text":"A number of options are different for the clang-based C/C++ compilers and Fortran compiler, something that EasyBuild cannot easily deal with. This is, e.g., the case for the options that set the floating point computation model, so for now these are left blank. Fortran floating point optimisation and accuracy options: -hfp0 to -hfp4 , with -hfp2 the default according to the manual. The manual only claims -hfp0 to -hfp3 . For OpenMP the manual also claims a different option for the Fortran compiler, but it turns out that recent versions also support -fopenmp so no changes are needed with respect to the standard EasyBuild behaviour. The manual claims that there is also a -fnoopenmp but that turns out to be wrong in cce 12. The situation about promoting integer and real in Fortran to 8 bytes is very confusing. Those options are not properly documented in the manuals. The Cray driver manual only mentions -default64 which according to the documentation is then converted to -sdefault64 . In the documentation, there is mention of -sreal64 (or -s real64`), but this option is nowhere to be found in the list of compiler options. However, it turns out that -sinteger64 (or -s integer64 ) and -sreal64 (or -s real64``) both exist and do what is expected from them. TODO: Check if -flto exist in Fortran and C and if so, add to the options.","title":"Some Cray-specific remarks"},{"location":"Toolchains/cpeCray/#compiler-optimization-level","text":"The common options translate into: Option Flag noopt -O0 lowopt -O1 defaultopt -O2 opt -O3 Other optimization-related options (and see also parallelism below): Option Flag unroll -funroll optarch TODO","title":"Compiler optimization level"},{"location":"Toolchains/cpeCray/#floating-point-precision","text":"These flags are currently not correctly honoured. Option Flag strict / precise / defaultprec / loose / veryloose / Other floating-point optimisation and accuracy-related flags: Option What? ieee /","title":"Floating point precision"},{"location":"Toolchains/cpeCray/#common-parallelism-related-options","text":"Option Flag vectorize / openmp -fopenmp usempi No compiler flags mpich-mt -craympich-mt","title":"Common parallelism-related options"},{"location":"Toolchains/cpeCray/#code-generation-and-linking-options","text":"Option Flag dynamic No flag as this is currently the only mode supported 32bit -m32 debug -g pic -fPIC packed-linker-options Pack the linker options as comma separated list (default: False) shared -shared static -static rpath Use RPATH wrappers when --rpath is enabled in EasyBuild configuration (default: True)","title":"Code generation and linking options"},{"location":"Toolchains/cpeCray/#source-related-options","text":"Option Flag cstd -std=%(value)s i8 -sinteger64 r8 -sreal64","title":"Source-related options"},{"location":"Toolchains/cpeCray/#miscellaneous-options","text":"Option Flag verbose -craype-verbose","title":"Miscellaneous options"},{"location":"Toolchains/cpeGNU/","text":"cpeGNU toolchain \u00b6 Available options \u00b6 The cpeGNU toolchain supports the common toolchain options , the additional GCC flags and some additional Cray-specific flags, two of which are really just redefinitions of standard compiler flags. GCC-specific flags \u00b6 Option Categorie What? loop parallelism Automatic loop parallellisation f2c source Generate code compatible with f2c and f77 lto code generation Enable Link Time Optimization cpeGNU-specific flags \u00b6 Option Categorie What? dynamic code generation Generate dynamically linked executable (default: True) mpich-mt parallelism Alternate Cray-MPICH library for MT support (default: False) gfortran9-compat source Add flags that improve compatibility with gfortran 9 for 10 and higher (default: False) mpich-mt: Directs the driver to link in an alternate version of the Cray-MPICH library which provides fine-grained multi-threading support to applications that perform MPI operations within threaded regions. (default: False) Two further options trigger different compiler flags than in the GCC toolchain: verbose and optarch but have otherwise the same meaning. Mapping of options onto compiler flags \u00b6 Compiler optimization level \u00b6 The common options translate into: Option Flag noopt -O0 lowopt -O1 defaultopt -O2 -ftree-vectorize opt -O3 Other optimization-related options (and see also parallelism below): Option Flag unroll -funroll-loops optarch TODO Floating point precision \u00b6 Option Flag strict -mieee-fp -mno-recip precise -mno-recip defaultprec -fno-math-errno loose -fno-math-errno -mrecip -mno-ieee-fp veryloose -fno-math-errno -mrecip=all -mno-ieee-fp Other floating-point optimisation and accuracy-related flags: Option What? ieee -mieee-fp -fno-trapping-math Common parallelism-related options \u00b6 Option Flag vectorize False: -fno-tree-vectorize True: -ftree-vectorize loop -ftree-switch-conversion -floop-interchange -floop-strip-mine -floop-block openmp -fopenmp usempi No compiler flags mpich-mt -craympich-mt Code generation and linking options \u00b6 Option Flag dynamic No flag as this is currently the only mode supported 32bit -m32 debug -g pic -fPIC packed-linker-options Pack the linker options as comma separated list (default: False) shared -shared static -static rpath Use RPATH wrappers when --rpath is enabled in EasyBuild configuration (default: True) Source-related options \u00b6 Option Flag cstd -std=%(value)s i8 -fdefault-integer-8 r8 -fdefault-real-8 f2c -ff2c Miscellaneous options \u00b6 Option Flag verbose -craype-verbose","title":"cpeGNU toolchain"},{"location":"Toolchains/cpeGNU/#cpegnu-toolchain","text":"","title":"cpeGNU toolchain"},{"location":"Toolchains/cpeGNU/#available-options","text":"The cpeGNU toolchain supports the common toolchain options , the additional GCC flags and some additional Cray-specific flags, two of which are really just redefinitions of standard compiler flags.","title":"Available options"},{"location":"Toolchains/cpeGNU/#gcc-specific-flags","text":"Option Categorie What? loop parallelism Automatic loop parallellisation f2c source Generate code compatible with f2c and f77 lto code generation Enable Link Time Optimization","title":"GCC-specific flags"},{"location":"Toolchains/cpeGNU/#cpegnu-specific-flags","text":"Option Categorie What? dynamic code generation Generate dynamically linked executable (default: True) mpich-mt parallelism Alternate Cray-MPICH library for MT support (default: False) gfortran9-compat source Add flags that improve compatibility with gfortran 9 for 10 and higher (default: False) mpich-mt: Directs the driver to link in an alternate version of the Cray-MPICH library which provides fine-grained multi-threading support to applications that perform MPI operations within threaded regions. (default: False) Two further options trigger different compiler flags than in the GCC toolchain: verbose and optarch but have otherwise the same meaning.","title":"cpeGNU-specific flags"},{"location":"Toolchains/cpeGNU/#mapping-of-options-onto-compiler-flags","text":"","title":"Mapping of options onto compiler flags"},{"location":"Toolchains/cpeGNU/#compiler-optimization-level","text":"The common options translate into: Option Flag noopt -O0 lowopt -O1 defaultopt -O2 -ftree-vectorize opt -O3 Other optimization-related options (and see also parallelism below): Option Flag unroll -funroll-loops optarch TODO","title":"Compiler optimization level"},{"location":"Toolchains/cpeGNU/#floating-point-precision","text":"Option Flag strict -mieee-fp -mno-recip precise -mno-recip defaultprec -fno-math-errno loose -fno-math-errno -mrecip -mno-ieee-fp veryloose -fno-math-errno -mrecip=all -mno-ieee-fp Other floating-point optimisation and accuracy-related flags: Option What? ieee -mieee-fp -fno-trapping-math","title":"Floating point precision"},{"location":"Toolchains/cpeGNU/#common-parallelism-related-options","text":"Option Flag vectorize False: -fno-tree-vectorize True: -ftree-vectorize loop -ftree-switch-conversion -floop-interchange -floop-strip-mine -floop-block openmp -fopenmp usempi No compiler flags mpich-mt -craympich-mt","title":"Common parallelism-related options"},{"location":"Toolchains/cpeGNU/#code-generation-and-linking-options","text":"Option Flag dynamic No flag as this is currently the only mode supported 32bit -m32 debug -g pic -fPIC packed-linker-options Pack the linker options as comma separated list (default: False) shared -shared static -static rpath Use RPATH wrappers when --rpath is enabled in EasyBuild configuration (default: True)","title":"Code generation and linking options"},{"location":"Toolchains/cpeGNU/#source-related-options","text":"Option Flag cstd -std=%(value)s i8 -fdefault-integer-8 r8 -fdefault-real-8 f2c -ff2c","title":"Source-related options"},{"location":"Toolchains/cpeGNU/#miscellaneous-options","text":"Option Flag verbose -craype-verbose","title":"Miscellaneous options"},{"location":"Toolchains/toolchain_common/","text":"Common toolchain options \u00b6 Optimization level \u00b6 EasyBuild distinguishes between four optimization levels. Rather than having a single toolchain option that takes the level as a number, toolchainopts uses four boolean parameters where one takes precedence over others. Lower optimization takes precedence over higher optimization. All have as default value False yet defaultopt is the one that will be used if nothing is specified. Option What? noopt Disable compiler optimizations lowopt Low compiler optimizations defaultopt Default compiler optimizations opt High compiler optimizations Other optimization-related options (and see also parallelism below): Option What? unroll Unroll loops (default: False) optarch Enable architecture optimizations (default: False) Floating point accuracy \u00b6 There are five flags that set the floating point precision. All are False by default but defaultprec is taken if none of the options is set to True . Again, the first one that is set to True in the table below is used: Option What? strict Strict (highest) precision precise High precision defaultprec Default precision loose Loose precision veryloose Very loose precision Other floating-point optimisation and accuracy-related flags: Option What? ieee Adhere to IEEE-754 rules (default: False) Common parallelism-related options \u00b6 Option What? vectorize Enable compiler auto-vectorization, default except for noopt and lowopt openmp Enable OpenMP (default: False) usempi Use MPI compiler as default compiler (default: False) The usempi option is only supported by toolchains that also include an MPI component. Code generation and linking options \u00b6 Option What? 32bit Compile 32bit target (default: False) debug Enable debug (default: False) pic Use PIC (default: False) packed-linker-options No flag, internal to EasyBuild shared Build shared library (default: False) static Build static library (default: False) rpath No flag, internal to EasyBuild Source-related options \u00b6 Option What? cstd Specify C standard (C/C++ only - default: None) i8 Integers are 8 byte integers (Fortran only - default: False) r8 Real is 8 byte real (Fortran only - default: False) Miscellaneous options \u00b6 Option What? verbose Verbose output (default: False) cciscxx Use CC as CXX (default: False) extra_cflags Specify extra CFLAGS options. (default: None) extra_cxxflags Specify extra CXXFLAGS options. (default: None) extra_f90flags Specify extra F90FLAGS options. (default: None) extra_fcflags Specify extra FCFLAGS options. (default: None) extra_fflags Specify extra FFLAGS options. (default: None) Most of these options do not directly transform into compiler flags. Instead, they influence the way EasyBuild sets variables ( cciscxx ) or directly add additional flags to the indicated environment variables.","title":"EasyBuild toolchains common options"},{"location":"Toolchains/toolchain_common/#common-toolchain-options","text":"","title":"Common toolchain options"},{"location":"Toolchains/toolchain_common/#optimization-level","text":"EasyBuild distinguishes between four optimization levels. Rather than having a single toolchain option that takes the level as a number, toolchainopts uses four boolean parameters where one takes precedence over others. Lower optimization takes precedence over higher optimization. All have as default value False yet defaultopt is the one that will be used if nothing is specified. Option What? noopt Disable compiler optimizations lowopt Low compiler optimizations defaultopt Default compiler optimizations opt High compiler optimizations Other optimization-related options (and see also parallelism below): Option What? unroll Unroll loops (default: False) optarch Enable architecture optimizations (default: False)","title":"Optimization level"},{"location":"Toolchains/toolchain_common/#floating-point-accuracy","text":"There are five flags that set the floating point precision. All are False by default but defaultprec is taken if none of the options is set to True . Again, the first one that is set to True in the table below is used: Option What? strict Strict (highest) precision precise High precision defaultprec Default precision loose Loose precision veryloose Very loose precision Other floating-point optimisation and accuracy-related flags: Option What? ieee Adhere to IEEE-754 rules (default: False)","title":"Floating point accuracy"},{"location":"Toolchains/toolchain_common/#common-parallelism-related-options","text":"Option What? vectorize Enable compiler auto-vectorization, default except for noopt and lowopt openmp Enable OpenMP (default: False) usempi Use MPI compiler as default compiler (default: False) The usempi option is only supported by toolchains that also include an MPI component.","title":"Common parallelism-related options"},{"location":"Toolchains/toolchain_common/#code-generation-and-linking-options","text":"Option What? 32bit Compile 32bit target (default: False) debug Enable debug (default: False) pic Use PIC (default: False) packed-linker-options No flag, internal to EasyBuild shared Build shared library (default: False) static Build static library (default: False) rpath No flag, internal to EasyBuild","title":"Code generation and linking options"},{"location":"Toolchains/toolchain_common/#source-related-options","text":"Option What? cstd Specify C standard (C/C++ only - default: None) i8 Integers are 8 byte integers (Fortran only - default: False) r8 Real is 8 byte real (Fortran only - default: False)","title":"Source-related options"},{"location":"Toolchains/toolchain_common/#miscellaneous-options","text":"Option What? verbose Verbose output (default: False) cciscxx Use CC as CXX (default: False) extra_cflags Specify extra CFLAGS options. (default: None) extra_cxxflags Specify extra CXXFLAGS options. (default: None) extra_f90flags Specify extra F90FLAGS options. (default: None) extra_fcflags Specify extra FCFLAGS options. (default: None) extra_fflags Specify extra FFLAGS options. (default: None) Most of these options do not directly transform into compiler flags. Instead, they influence the way EasyBuild sets variables ( cciscxx ) or directly add additional flags to the indicated environment variables.","title":"Miscellaneous options"},{"location":"Toolchains/toolchain_cpe_common/","text":"EasyBuild CPE toolchains common options \u00b6 General principles \u00b6 The toolchains for the CPE in this repository have been developed from those of CSCS. Several changes have been made though. The AMD AOCC toolchain was very incomplete: It relied on a definition for the AOCC compiler that was derived from GCC rather than from the Clang one, so most of the options didn't really work. We also changed the floating point accuracy options compared to those used in the Clang compiler definition as they produced warnings about combinations that don't make sense. Some corrections were made to the Cray CCE compiler definition. In particular we added the correct options to support the -i8 and -r8 toolchain options for Fortran data types. We also had a look at better options for the floating point accuracy but there we ran into the problem that the options for the Cray compiler are different for the Fortran and C/C++ compilers, something that the option mapping mechanism in EasyBuild cannot deal with. For OpenMP there was an easy solution as it turns out that the Fortran compiler now does support -fopenmp as an alternative for -homp so there a single option that works for both the Fortran and new Clang-based C/C++ compilers is available. The code for processing --optarch / EASYBUILD_OPTARCH was extended to be more in line with how other toolchain definitions process that code, and to prepare for the GPUs where we may want to load multiple targeting modules (one for the CPU and one for the GPU, or even one for the network). It is now possible to specify the argument to optarch for multiple compilers. For the Cray toolchains, the name CPE is used. This makes it possible to also support more traditional EasyBuild toolchains simultaneously from the same setup should the need arrise. We did stick to the approach of loading the targeting modules rather than using the compiler flags -target-cpu , -target-accel and -target-network as now the implementation doesn't need to figure out which module is of which type (though that is very easy to do). The CSCS implementation used a common setup to all CPE compilers. For LUMI, this path is left for now as it turned out the code was incomplete and it was difficult to complete the code and to get it working in all circumstances. Not all default options of the regular GNU and AOCC compilers were picked up by their corresponding CPE compiler definitions. Moreover, the generic approach does not work with toolchain options that do not simply map onto compiler options. This may change in a future edition again when it becomes clearer which code is really common to all Cray compiler definitions and which code is specialised for a particular compiler. In the process we also ran into several bugs in the EasyBuild toolchain definitions that define toolchain options that are never picked up and translated into compiler flags because the compiler definition fails to add them to COMPILER_C_UNIQUE_FLAGS / COMPILER_F_UNIQUE_FLAGS (likely the recommended way) or to push them onto the list COMPILER_FLAGS defined in the compiler.py generic compiler definition from which all other compiler definitions inherit. Specifying the targets through --optarch/EASYBULD_OPTARCH \u00b6 The actual architecture arguments are specified through the names of the corresponding targeting modules but dropping the craype- prefix from their name. Multiple options can be specified by using a + -sign between two options. However, only one CPT target, one accelerator target and one network target should be specified as otherwise the second will overwrite the first one anyway. If you want to specify options for multiple compilers, you can use the regular EasyBuild syntax for that, labeling the options for the Cray toolchains with CPE: . The GENERIC target is currently not supported for the Cray compilers because it is not clear how to do that in a system-independent way on Cray systems. There may be modules that define a generic architecture, but if they exist they are certainly not installed on all Cray systems. Some examples are EASYBUILD_OPTARCH=\"x86-rome\" : Defines only an architecture for the Cray compilers, in this case it tells EasyBuild to optimize for the AMD Rome processor. EASYBUILD_OPTARCH=\"x86-milan+accel-AMD-gfx90A+network-ofi\" : Tells EasyBuild to target for the AMD Milan CPU, AMD gfx90A GPU (which is the likely module name for the MI 200) and OFI network stack. EASYBULD_OPTARCH=\"CPE:x86-rome;Intel:march=core-avx2 -mtune=core-avx2;GCC:march=znver2 -mtune=znver2\" would set a compiler target for the Cray toolchains, the Intel compilers used through the regular EasyBuild common toolchains and the GNU compilers, again used through the regular EasyBuild common toolchains, in each case specifying options suitable for the AMD Rome CPU.","title":"Common information to all cpe* toolchains"},{"location":"Toolchains/toolchain_cpe_common/#easybuild-cpe-toolchains-common-options","text":"","title":"EasyBuild CPE toolchains common options"},{"location":"Toolchains/toolchain_cpe_common/#general-principles","text":"The toolchains for the CPE in this repository have been developed from those of CSCS. Several changes have been made though. The AMD AOCC toolchain was very incomplete: It relied on a definition for the AOCC compiler that was derived from GCC rather than from the Clang one, so most of the options didn't really work. We also changed the floating point accuracy options compared to those used in the Clang compiler definition as they produced warnings about combinations that don't make sense. Some corrections were made to the Cray CCE compiler definition. In particular we added the correct options to support the -i8 and -r8 toolchain options for Fortran data types. We also had a look at better options for the floating point accuracy but there we ran into the problem that the options for the Cray compiler are different for the Fortran and C/C++ compilers, something that the option mapping mechanism in EasyBuild cannot deal with. For OpenMP there was an easy solution as it turns out that the Fortran compiler now does support -fopenmp as an alternative for -homp so there a single option that works for both the Fortran and new Clang-based C/C++ compilers is available. The code for processing --optarch / EASYBUILD_OPTARCH was extended to be more in line with how other toolchain definitions process that code, and to prepare for the GPUs where we may want to load multiple targeting modules (one for the CPU and one for the GPU, or even one for the network). It is now possible to specify the argument to optarch for multiple compilers. For the Cray toolchains, the name CPE is used. This makes it possible to also support more traditional EasyBuild toolchains simultaneously from the same setup should the need arrise. We did stick to the approach of loading the targeting modules rather than using the compiler flags -target-cpu , -target-accel and -target-network as now the implementation doesn't need to figure out which module is of which type (though that is very easy to do). The CSCS implementation used a common setup to all CPE compilers. For LUMI, this path is left for now as it turned out the code was incomplete and it was difficult to complete the code and to get it working in all circumstances. Not all default options of the regular GNU and AOCC compilers were picked up by their corresponding CPE compiler definitions. Moreover, the generic approach does not work with toolchain options that do not simply map onto compiler options. This may change in a future edition again when it becomes clearer which code is really common to all Cray compiler definitions and which code is specialised for a particular compiler. In the process we also ran into several bugs in the EasyBuild toolchain definitions that define toolchain options that are never picked up and translated into compiler flags because the compiler definition fails to add them to COMPILER_C_UNIQUE_FLAGS / COMPILER_F_UNIQUE_FLAGS (likely the recommended way) or to push them onto the list COMPILER_FLAGS defined in the compiler.py generic compiler definition from which all other compiler definitions inherit.","title":"General principles"},{"location":"Toolchains/toolchain_cpe_common/#specifying-the-targets-through-optarcheasybuld_optarch","text":"The actual architecture arguments are specified through the names of the corresponding targeting modules but dropping the craype- prefix from their name. Multiple options can be specified by using a + -sign between two options. However, only one CPT target, one accelerator target and one network target should be specified as otherwise the second will overwrite the first one anyway. If you want to specify options for multiple compilers, you can use the regular EasyBuild syntax for that, labeling the options for the Cray toolchains with CPE: . The GENERIC target is currently not supported for the Cray compilers because it is not clear how to do that in a system-independent way on Cray systems. There may be modules that define a generic architecture, but if they exist they are certainly not installed on all Cray systems. Some examples are EASYBUILD_OPTARCH=\"x86-rome\" : Defines only an architecture for the Cray compilers, in this case it tells EasyBuild to optimize for the AMD Rome processor. EASYBUILD_OPTARCH=\"x86-milan+accel-AMD-gfx90A+network-ofi\" : Tells EasyBuild to target for the AMD Milan CPU, AMD gfx90A GPU (which is the likely module name for the MI 200) and OFI network stack. EASYBULD_OPTARCH=\"CPE:x86-rome;Intel:march=core-avx2 -mtune=core-avx2;GCC:march=znver2 -mtune=znver2\" would set a compiler target for the Cray toolchains, the Intel compilers used through the regular EasyBuild common toolchains and the GNU compilers, again used through the regular EasyBuild common toolchains, in each case specifying options suitable for the AMD Rome CPU.","title":"Specifying the targets through --optarch/EASYBULD_OPTARCH"}]}